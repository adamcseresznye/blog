{
  "hash": "02ad6c54086c8cfd0b4782d9833896c2",
  "result": {
    "markdown": "---\ntitle: 'Predicting Belgian Real Estate Prices: Part 7: Fine-tuning our model'\nauthor: Adam Cseresznye\ndate: '2023-11-09'\ncategories:\n  - Predicting Belgian Real Estate Prices\ntoc: true\nformat:\n  html:\n    code-fold: true\n    code-tools: true\n---\n\n![Photo by Stephen Phillips - Hostreviews.co.uk on UnSplash](https://cf.bstatic.com/xdata/images/hotel/max1024x768/408003083.jpg?k=c49b5c4a2346b3ab002b9d1b22dbfb596cee523b53abef2550d0c92d0faf2d8b&o=&hp=1){fig-align=\"center\" width=50%}\n\nIn Part 6, we established a robust cross-validation strategy to consistently assess our model's performance across multiple data folds. We also identified and managed potential outliers in our dataset. Additionally, we explored diverse feature engineering methods, creating and evaluating informative features to enhance our model's predictive capabilities. \n\nIn this final segment, we'll optimize our hyperparameters using Optuna and end by evaluating the final model's performance based on the test portion. Let's dive in!\n\n::: {.callout-note}\nYou can access the project's app through its [Streamlit website](https://belgian-house-price-predictor.streamlit.app/).\n\n:::\n\n# Import data\n\n::: {.cell editable='true' slideshow='{\"slide_type\":\"\"}' tags='[]' execution_count=1}\n``` {.python .cell-code}\nimport gc\nimport os\nfrom pathlib import Path\nfrom typing import Any, List, Optional, Tuple\n\nimport catboost\nimport numpy as np\nimport pandas as pd\nfrom data import pre_process, update_database, utils\nfrom features import feature_engineering\nfrom lets_plot import *\nfrom lets_plot.mapping import as_discrete\nfrom models import predict_model, train_model\nfrom sklearn import metrics, model_selection\nfrom tqdm.notebook import tqdm\n\nLetsPlot.setup_html()\nimport pickle\n\nimport optuna\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n            <div id=\"aICBGt\"></div>\n            <script type=\"text/javascript\" data-lets-plot-script=\"library\">\n                if(!window.letsPlotCallQueue) {\n                    window.letsPlotCallQueue = [];\n                }; \n                window.letsPlotCall = function(f) {\n                    window.letsPlotCallQueue.push(f);\n                };\n                (function() {\n                    var script = document.createElement(\"script\");\n                    script.type = \"text/javascript\";\n                    script.src = \"https://cdn.jsdelivr.net/gh/JetBrains/lets-plot@v4.0.1/js-package/distr/lets-plot.min.js\";\n                    script.onload = function() {\n                        window.letsPlotCall = function(f) {f();};\n                        window.letsPlotCallQueue.forEach(function(f) {f();});\n                        window.letsPlotCallQueue = [];\n                        \n                    };\n                    script.onerror = function(event) {\n                        window.letsPlotCall = function(f) {};    // noop\n                        window.letsPlotCallQueue = [];\n                        var div = document.createElement(\"div\");\n                        div.style.color = 'darkred';\n                        div.textContent = 'Error loading Lets-Plot JS';\n                        document.getElementById(\"aICBGt\").appendChild(div);\n                    };\n                    var e = document.getElementById(\"aICBGt\");\n                    e.appendChild(script);\n                })()\n            </script>\n            \n```\n:::\n:::\n\n\nUp to this point, we've successfully obtained all the house advertisements from ImmoWeb. We've conducted data description, preselected features, established an effective data pre-processing pipeline, and identified the most suitable machine learning algorithm for this task. Additionally, we've engaged in feature engineering and carried out further feature selection to streamline our machine learning model. The final pha, that's yet to be done,se involves fine-tuning the hyperparameters of our machine learning model, enhancing predictive accuracy while mitigating overfittin\n\n\n# Prepare dataframe before modelling\n\nLet's get our data ready for modeling by applying the \"prepare_data_for_modelling\" function as detailed in Part 6. A quick recap: this function carries out the subsequent actions to prepare a DataFrame for machine learning:\n1. It randomly shuffles the DataFrame's rows.\n2. The 'price' column is transformed into the base 10 logarithm.\n3. Categorical variable missing values are replaced with 'missing value.'\n4. It divides the data into features (X) and the target (y).\n5. Using LocalOutlierFactor, it identifies and removes outlier values.rFactor.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndf = pd.read_parquet(\n    utils.Configuration.INTERIM_DATA_PATH.joinpath(\n        \"2023-10-01_Processed_dataset_for_NB_use.parquet.gzip\"\n    )\n)\n\nX, y = pre_process.prepare_data_for_modelling(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nShape of X and y with outliers: (3660, 14), (3660,)\nShape of X and y without outliers: (3427, 14), (3427,)\n```\n:::\n:::\n\n\nWe'll divide the data into training and testing sets. The training portion will be dedicated to hyperparameter tuning. It's worth noting that, to guard against overfitting during hyperparameter tuning, we'll implement cross-validation. This involves splitting the training set into subgroups for training and validation. The validation portion helps prevent overfitting, and the training continues until we achieve the desired performance. The test set will come into play later for evaluating our final model.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nX_train, X_test, y_train, y_test = model_selection.train_test_split(\n    X,\n    y,\n    test_size=0.2,\n    random_state=utils.Configuration.seed,\n)\n\nprint(f\"Shape of X_train: {X_train.shape}, Shape of X_test: {X_test.shape}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nShape of X_train: (2741, 14), Shape of X_test: (686, 14)\n```\n:::\n:::\n\n\nWe'll also create a handy helper function called `dumper`. This function enables us to save the best parameters discovered during tuning as a `.pickle` file, allowing us to load and utilize these parameters from the saved file at a later time.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ndef dumper(file: Any, name: str) -> None:\n    \"\"\"\n    Pickle and save an object to a file.\n\n    This function takes an object and a name, then uses the Pickle library to serialize\n    and save the object to a file with the given name. The file is saved in binary mode ('wb').\n\n    Args:\n        file (Any): The object to be pickled and saved.\n        name (str): The name of the file, including the file extension, where the object will be saved.\n\n    Returns:\n        None: This function does not return a value.\n\n    Example:\n        To save an object to a file:\n        >>> my_data = [1, 2, 3]\n        >>> dumper(my_data, \"my_data.pickle\")\n\n    Note:\n        The file is saved in binary mode ('wb') to ensure compatibility and proper\n        handling of binary data.\n    \"\"\"\n    pickle.dump(file, open(f\"{name}.pickle\", \"wb\"))\n```\n:::\n\n\n## Hyperparameter tuning using Optuna\n\nTo identify the most optimal settings, we'll leverage the Optuna library. The key hyperparameters under consideration are iterations, depth, learning_rate, random_strength, bagging_temperature, and others.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ndef objective(trial: optuna.Trial) -> float:\n    \"\"\"\n    Optuna objective function for tuning CatBoost hyperparameters.\n\n    This function takes an Optuna trial and explores hyperparameters for a CatBoost\n    model to minimize the Root Mean Squared Error (RMSE) using K-Fold cross-validation.\n\n    Parameters:\n    - trial (optuna.Trial): Optuna trial object for hyperparameter optimization.\n\n    Returns:\n    - float: Mean RMSE across K-Fold cross-validation iterations.\n\n    Example use case:\n    # Create an Optuna study and optimize hyperparameters\n    study = optuna.create_study(direction=\"minimize\")\n    study.optimize(objective, n_trials=100)\n\n    # Get the best hyperparameters\n    best_params = study.best_params\n    \"\"\"\n    catboost_params = {\n        \"iterations\": trial.suggest_int(\"iterations\", 10, 1000),\n        \"depth\": trial.suggest_int(\"depth\", 1, 8),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 1),\n        \"random_strength\": trial.suggest_float(\"random_strength\", 1e-9, 10),\n        \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 0, 1),\n        \"l2_leaf_reg\": trial.suggest_int(\"l2_leaf_reg\", 2, 30),\n        \"border_count\": trial.suggest_int(\"border_count\", 1, 255),\n        \"thread_count\": os.cpu_count(),\n    }\n\n    results = []\n    optuna.logging.set_verbosity(optuna.logging.WARNING)\n\n    # Extract feature names and data types\n    # features = X.columns[~X.columns.str.contains(\"price\")]\n    # numerical_features = X.select_dtypes(\"number\").columns.to_list()\n    categorical_features = X.select_dtypes(\"object\").columns.to_list()\n\n    # Create a K-Fold cross-validator\n    CV = model_selection.RepeatedKFold(\n        n_splits=10, n_repeats=1, random_state=utils.Configuration.seed\n    )\n\n    for train_fold_index, val_fold_index in CV.split(X):\n        X_train_fold, X_val_fold = X.loc[train_fold_index], X.loc[val_fold_index]\n        y_train_fold, y_val_fold = y.loc[train_fold_index], y.loc[val_fold_index]\n\n        # Create CatBoost datasets\n        catboost_train = catboost.Pool(\n            X_train_fold,\n            y_train_fold,\n            cat_features=categorical_features,\n        )\n        catboost_valid = catboost.Pool(\n            X_val_fold,\n            y_val_fold,\n            cat_features=categorical_features,\n        )\n\n        # Initialize and train the CatBoost model\n        model = catboost.CatBoostRegressor(**catboost_params)\n        model.fit(\n            catboost_train,\n            eval_set=[catboost_valid],\n            early_stopping_rounds=utils.Configuration.early_stopping_round,\n            verbose=utils.Configuration.verbose,\n            use_best_model=True,\n        )\n\n        # Calculate OOF validation predictions\n        valid_pred = model.predict(X_val_fold)\n\n        RMSE_score = metrics.mean_squared_error(y_val_fold, valid_pred, squared=False)\n\n        del (\n            X_train_fold,\n            y_train_fold,\n            X_val_fold,\n            y_val_fold,\n            catboost_train,\n            catboost_valid,\n            model,\n            valid_pred,\n        )\n        gc.collect()\n\n        results.append(RMSE_score)\n    return np.mean(results)\n```\n:::\n\n\n::: {.callout-note}\nSimilar to Part 6, the hyperparameter optimization step was pre-computed due to the significant computational time needed. The results were saved rather than executed during notebook rendering to save time. However, note that the outcomes should remain unchanged.\n:::\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n%%script echo skipping\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(train_model.Optuna_Objective(X_train, y_train), n_trials=100, show_progress_bar=True)\n\ndumper(study.best_params, \"CatBoost_params\")\ndumper(study.best_value, \"CatBoost_value\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCouldn't find program: 'echo'\n```\n:::\n:::\n\n\nAs shown below, Optuna found the best Out-Of-Fold (OOF) score using the selected parameters, which is 0.1060. Recall that in Part 6, our best OOF score was 0.1070, so this represents a modest improvement, albeit a slight one.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ncatboost_params_optuna = pd.read_pickle(\"CatBoost_params.pickle\")\n\nprint(\n    f'The best OOF RMSE score of the hyperparameter tuning is {pd.read_pickle(\"CatBoost_value.pickle\"):.4f}.'\n)\nprint(f\"The corresponding values: {catboost_params_optuna}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe best OOF RMSE score of the hyperparameter tuning is 0.1060.\nThe corresponding values: {'iterations': 956, 'depth': 7, 'learning_rate': 0.050050595110243595, 'random_strength': 7.110744896133362, 'bagging_temperature': 0.024119607385698107, 'l2_leaf_reg': 3, 'border_count': 205}\n```\n:::\n:::\n\n\n# Retrain using the best parameters and predict\n\nAfter obtaining the most optimal parameters, we can proceed to retrain our model using the entire dataset, excluding the test portion, of course. For this we can use the `train_catboost` as seen below.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\ndef train_catboost(\n    X: pd.DataFrame, y: pd.Series, catboost_params: dict\n) -> catboost.CatBoostRegressor:\n    \"\"\"\n    Train a CatBoostRegressor model using the provided data and parameters.\n\n    Parameters:\n        X (pd.DataFrame): The feature dataset.\n        y (pd.Series): The target variable.\n        catboost_params (dict): CatBoost hyperparameters.\n\n    Returns:\n        CatBoostRegressor: The trained CatBoost model.\n\n    This function takes the feature dataset `X`, the target variable `y`, and a dictionary of CatBoost\n    hyperparameters. It automatically detects categorical features in the dataset and trains a CatBoostRegressor\n    model with the specified parameters.\n\n    Example:\n        X, y = load_data()\n        catboost_params = {\n            'iterations': 1000,\n            'learning_rate': 0.1,\n            'depth': 6,\n            # ... other hyperparameters ...\n        }\n        model = train_catboost(X, y, catboost_params)\n    \"\"\"\n    categorical_features = X.select_dtypes(\"object\").columns.to_list()\n\n    catboost_train = catboost.Pool(\n        X,\n        y,\n        cat_features=categorical_features,\n    )\n\n    model = catboost.CatBoostRegressor(**catboost_params)\n    model.fit(\n        catboost_train,\n        verbose=utils.Configuration.verbose,\n    )\n\n    return model\n```\n:::\n\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nmodel = train_model.train_catboost(X_train, y_train, catboost_params_optuna)\n```\n:::\n\n\nExcellent! We've made good progress. Now, it's time for the final evaluation of our dataset using the test set. We can use the `predict_catboost` function for this.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\ndef predict_catboost(\n    model: catboost.CatBoostRegressor,\n    X: pd.DataFrame,\n    thread_count: int = -1,\n    verbose: int = None,\n) -> np.ndarray:\n    \"\"\"\n    Make predictions using a CatBoost model on the provided dataset.\n\n    Parameters:\n        model (catboost.CatBoostRegressor): The trained CatBoost model.\n        X (pd.DataFrame): The dataset for which predictions are to be made.\n        thread_count (int, optional): The number of threads for prediction. Default is -1 (auto).\n        verbose (int, optional): Verbosity level. Default is None.\n\n    Returns:\n        np.ndarray: Predicted values.\n\n    This function takes a trained CatBoost model, a dataset `X`, and optional parameters for\n    specifying the number of threads (`thread_count`) and verbosity (`verbose`) during prediction.\n    It returns an array of predicted values.\n\n    Example:\n        model = load_catboost_model()\n        X_new = load_new_data()\n        predictions = predict_catboost(model, X_new, thread_count=4, verbose=2)\n    \"\"\"\n    prediction = model.predict(data=X, thread_count=thread_count, verbose=verbose)\n    return prediction\n```\n:::\n\n\nTo assess the predictions, we'll obtain both RMSE and R2 values.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nprediction = predict_model.predict_catboost(model=model, X=X_test)\n```\n:::\n\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\ndef score_prediction(y_true, y_pred):\n    \"\"\"\n    Calculate regression evaluation metrics based on\n    true and predicted values.\n\n    Parameters:\n        y_true (array-like): True target values.\n        y_pred (array-like): Predicted values.\n\n    Returns:\n        tuple: A tuple containing Root Mean Squared Error (RMSE)\n        and R-squared (R2).\n\n    This function calculates RMSE and R2 to evaluate the goodness\n    of fit between the true target values and predicted values.\n\n    Example:\n        y_true = [3, 5, 7, 9]\n        y_pred = [2.8, 5.2, 7.1, 9.3]\n        rmse, r2 = score_prediction(y_true, y_pred)\n    \"\"\"\n    RMSE = np.sqrt(metrics.mean_squared_error(y_true, np.log10(y_pred)))\n    R2 = metrics.r2_score(y_true, y_pred)\n\n    return RMSE, R2\n```\n:::\n\n\nSuperb! As you can see, the test set has an RMSE of 0.1101 and an R2 of 0.877. It's worth noting that the test set's RMSE is slightly higher than that of the training set, which is expected and suggests overfitting. Despite our efforts to prevent overfitting, it can be challenging to eliminate entirely. Nevertheless, it appears that we've done well.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\npredict_model.score_prediction(y_pred=prediction, y_true=y_test)\n```\n\n::: {.cell-output .cell-output-display execution_count=32}\n```\n(0.1101569172024175, 0.8771378054646048)\n```\n:::\n:::\n\n\nLet's put the original values and prediction in a DataFrame so that we can evaluate them visually as well.\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nresults = (\n    pd.concat(\n        [y_test.reset_index(drop=True), pd.Series(prediction)], axis=\"columns\"\n    ).rename(columns={\"price\": \"original_values\", 0: \"predicted_values\"})\n    # .apply(lambda x: 10**x)\n    .assign(residuals=lambda df: df.original_values - df.predicted_values)\n)\nresults\n```\n\n::: {.cell-output .cell-output-display execution_count=33}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>original_values</th>\n      <th>predicted_values</th>\n      <th>residuals</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.201397</td>\n      <td>5.198018</td>\n      <td>0.003380</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5.445604</td>\n      <td>5.352777</td>\n      <td>0.092827</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>6.040998</td>\n      <td>6.023609</td>\n      <td>0.017389</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6.021189</td>\n      <td>6.137714</td>\n      <td>-0.116525</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.911158</td>\n      <td>5.862038</td>\n      <td>0.049119</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>681</th>\n      <td>5.812913</td>\n      <td>5.776833</td>\n      <td>0.036081</td>\n    </tr>\n    <tr>\n      <th>682</th>\n      <td>5.755875</td>\n      <td>5.678560</td>\n      <td>0.077315</td>\n    </tr>\n    <tr>\n      <th>683</th>\n      <td>5.290035</td>\n      <td>5.193649</td>\n      <td>0.096385</td>\n    </tr>\n    <tr>\n      <th>684</th>\n      <td>5.542825</td>\n      <td>5.545253</td>\n      <td>-0.002428</td>\n    </tr>\n    <tr>\n      <th>685</th>\n      <td>5.475671</td>\n      <td>5.543814</td>\n      <td>-0.068143</td>\n    </tr>\n  </tbody>\n</table>\n<p>686 rows Ã— 3 columns</p>\n</div>\n```\n:::\n:::\n\n\nAs depicted below, our model demonstrates the ability to generalize effectively for unseen data, showcasing high R2 values and low RMSE. Additionally, examining the residuals reveals an even distribution, symbolizing robust model performance.\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\n(\n    results.pipe(\n        lambda df: ggplot(df, aes(\"original_values\", \"predicted_values\"))\n        + geom_point()\n        + geom_smooth()\n        + geom_text(\n            x=5,\n            y=6.6,\n            label=f\"RMSE = {predict_model.score_prediction(y_pred=prediction, y_true=y_test)[0]:.4f}\",\n            fontface=\"bold\",\n        )\n        + geom_text(\n            x=4.965,\n            y=6.5,\n            label=f\"R2 = {predict_model.score_prediction(y_pred=prediction, y_true=y_test)[1]:.4f}\",\n            fontface=\"bold\",\n        )\n        + labs(\n            title=\"Contrasting Predicted House Prices with Actual House Prices\",\n            subtitle=\"\"\" The plot suggests that the model makes accurate predictions on the test data. This is evident from the low RMSE values, \n            signifying a high level of accuracy. Additionally, the high R2 value indicates that the model effectively accounts for a \n            substantial portion of the data's variance, demonstrating a strong alignment between the model's predictions and the actual data.\n            \"\"\",\n            x=\"log10 True Prices (EUR)\",\n            y=\"log10 Predicted Prices (EUR)\",\n            caption=\"https://www.immoweb.be/\",\n        )\n        + theme(\n            plot_subtitle=element_text(\n                size=12, face=\"italic\"\n            ),  # Customize subtitle appearance\n            plot_title=element_text(size=15, face=\"bold\"),  # Customize title appearance\n        )\n        + ggsize(800, 600)\n    )\n)\n```\n\n::: {#fig-fig1 .cell-output .cell-output-display execution_count=34}\n```{=html}\n   <div id=\"lzH5ce\"></div>\n   <script type=\"text/javascript\" data-lets-plot-script=\"plot\">\n       (function() {\n           var plotSpec={\n\"data\":{\n\"original_values\":[5.201397124320452,5.445604203273597,6.040997692423491,6.021189299069938,5.911157608739977,5.841984804590114,5.47567118832443,6.276461804173244,5.929418925714293,5.077367905284157,5.9164539485499255,6.105510184769974,5.396199347095736,5.413299764081252,5.173186268412274,5.439332693830263,5.826074802700826,5.681241237375588,5.252853030979893,5.252853030979893,5.021189299069938,6.095169351431755,5.230448921378274,5.217483944213907,5.619615005742807,5.264817823009537,5.77451696572855,5.977723605288848,5.77451696572855,5.617000341120899,5.298853076409706,5.698970004336019,6.089905111439398,5.9003671286564705,6.254064452914338,5.7774268223893115,5.423245873936808,5.694605198933568,5.47567118832443,5.997823080745725,5.6875289612146345,5.227886704613674,6.276461804173244,5.469822015978163,5.54282542695918,5.110589710299249,5.7596678446896306,5.568201724066995,5.332438459915605,6.36078268987328,5.056904851336473,5.108903127667313,5.380211241711606,5.431363764158987,5.585460729508501,6.102090525511836,5.69810054562339,5.298853076409706,5.350248018334163,6.161068385471174,5.661812685537261,5.628388930050312,6.146128035678238,5.447158031342219,5.7596678446896306,5.525044807036846,5.977723605288848,5.590507462008583,5.997823080745725,5.276461804173244,6.077367905284157,5.423245873936808,6.071882007306125,5.600972895686748,5.173186268412274,6.112269768417271,5.350248018334163,5.653212513775344,5.671172842715083,5.537819095073274,5.770852011642144,5.190331698170292,5.69810054562339,5.278753600952829,5.423245873936808,5.676693609624866,5.578639209968072,5.378397900948138,5.361727836017593,6.174641192660449,5.724275869600789,5.252853030979893,5.6127838567197355,5.498310553789601,6.319896858814888,5.525044807036846,5.4623979978989565,5.361727836017593,6.1303337684950066,5.942008053022313,5.690196080028514,5.981818607170664,5.555094448578319,5.585460729508501,5.423245873936808,5.680335513414564,5.6950436588212945,4.929418925714293,5.652246341003323,5.505149978319906,5.977723605288848,5.389166084364533,5.537819095073274,5.525044807036846,5.5532760461371,5.8095597146352675,5.812913356642856,5.523746466811565,5.694605198933568,5.638489256954637,5.517195897949974,5.217483944213907,5.6512780139981444,5.8061799739838875,5.661812685537261,5.47567118832443,5.423245873936808,5.993436230497612,5.204119982655925,5.511883360978874,5.6674529528899535,5.517195897949974,5.079181246047625,5.54282542695918,5.740361899867195,5.77451696572855,5.175801632848279,5.217483944213907,5.45484486000851,5.694605198933568,5.491361693834273,5.812913356642856,5.652246341003323,5.298853076409706,5.298853076409706,5.380211241711606,5.146128035678238,5.77451696572855,5.544068044350276,5.041392685158225,5.378397900948138,5.290034611362518,5.517195897949974,5.600972895686748,5.037426497940624,5.350248018334163,5.173186268412274,5.739572344450092,6.220108088040055,5.903089986991944,5.049218022670182,5.243038048686294,5.47567118832443,5.9614210940664485,5.252853030979893,5.724275869600789,5.812913356642856,5.926856708949693,5.5910646070264995,6.6429588794097905,5.4623979978989565,5.628388930050312,5.469822015978163,5.951823035315912,5.6180480967120925,4.977723605288848,5.204119982655925,5.434568904034199,5.060697840353612,5.845098040014257,5.574031267727719,5.685293781386784,5.648360010980932,5.276461804173244,5.544068044350276,5.600972895686748,5.5434471800817,5.954242509439325,5.812244696800369,5.77451696572855,5.633468455579586,5.622214022966295,5.56643749219507,5.653212513775344,5.352182518111363,5.445604203273597,5.204119982655925,5.578639209968072,5.460897842756548,5.2405492482825995,5.175801632848279,5.623249290397901,5.997823080745725,5.2552725051033065,5.503790683057181,5.079181246047625,6.079181246047625,5.217483944213907,5.648360010980932,5.489958479424835,5.748188027006201,5.628388930050312,5.837588438235511,5.812913356642856,4.99563519459755,5.252853030979893,5.778151250383644,5.795184589682424,5.681241237375588,5.1303337684950066,5.423245873936808,5.802773725291976,5.574031267727719,5.53135116458306,5.5301996982030825,5.474216264076255,5.371067862271736,5.643452676486188,5.203848463746235,6.371067862271736,5.217483944213907,5.568201724066995,5.574031267727719,5.380211241711606,5.298853076409706,5.491361693834273,5.748188027006201,5.977723605288848,5.755112266395071,5.8095597146352675,5.352182518111363,6.060697840353612,5.596597095626461,5.096910013008056,5.423245873936808,5.989004615698537,5.429752280002408,5.371067862271736,5.45484486000851,6.342422680822207,5.7626785637274365,5.477121254719663,6.143014800254095,5.585460729508501,5.389166084364533,5.565257343420214,5.469822015978163,5.7558748556724915,5.638489256954637,5.021189299069938,5.7619278384205295,5.267171728403014,5.860338006570994,5.041392685158225,5.694605198933568,5.894869656745253,5.361727836017593,5.378397900948138,5.600972895686748,5.361727836017593,5.47567118832443,5.437750562820388,6.201397124320452,5.143014800254095,5.57978359661681,5.371067862271736,5.243038048686294,4.949390006644912,5.574031267727719,5.518513939877887,5.311753861055754,5.112269768417271,5.544068044350276,5.740362689494244,5.429752280002408,5.9164539485499255,6.112269768417271,5.176091259055681,5.937016107464814,5.5301996982030825,5.658011396657113,5.975431808509263,5.724275869600789,5.2552725051033065,5.6523430550627145,5.567026366159061,5.4065401804339555,5.838849090737256,5.190331698170292,5.812913356642856,5.267171728403014,5.714329759745233,5.252853030979893,5.469822015978163,4.99563519459755,5.562292864456475,5.676693609624866,5.719289844693328,5.73996769675951,5.537819095073274,5.396199347095736,5.173186268412274,5.252853030979893,5.146128035678238,5.075546961392531,5.439332693830263,5.54282542695918,5.653212513775344,6.039414119176137,5.57978359661681,5.596597095626461,5.60151678365001,6.041392685158225,5.243038048686294,5.652246341003323,5.565696733446075,5.9003671286564705,5.230448921378274,5.505149978319906,5.841984804590114,6.216165902285993,5.380211241711606,5.342422680822207,5.54282542695918,5.267171728403014,5.544068044350276,5.8444771757456815,5.517195897949974,5.929418925714293,5.544068044350276,5.628388930050312,5.230448921378274,5.143014800254095,5.77451696572855,5.544068044350276,5.829303772831025,5.371067862271736,5.902546779313991,5.176091259055681,6.249198357391113,5.9003671286564705,5.413299764081252,6.173186268412274,5.173186268412274,4.954242509439325,6.2552725051033065,5.396199347095736,5.694605198933568,5.662757831681574,5.5301996982030825,5.445604203273597,5.342422680822207,5.7774268223893115,5.429752280002408,5.977723605288848,5.190331698170292,5.505149978319906,5.562292864456475,5.600972895686748,5.460897842756548,5.841984804590114,5.204119982655925,5.243038048686294,5.28443073384452,4.949390006644912,5.5910646070264995,5.77451696572855,5.396199347095736,5.552668216112194,5.515873843711679,5.445604203273597,5.970811610872518,5.99563519459755,5.384711742938283,5.47567118832443,5.596597095626461,5.510545010206612,5.359835482339888,5.77451696572855,5.173186268412274,5.460897842756548,5.447158031342219,5.110589710299249,5.3404441148401185,5.627365856592733,5.652246341003323,6.146128035678238,5.439332693830263,5.628388930050312,6.544068044350276,5.997823080745725,5.600972895686748,6.371067862271736,5.77451696572855,5.541579243946581,5.9003671286564705,5.525044807036846,5.201397124320452,5.676693609624866,5.596597095626461,5.648360010980932,5.841984804590114,5.628388930050312,5.385606273598312,6.371067862271736,5.77451696572855,5.431363764158987,5.060697840353612,5.854306041801081,5.951823035315912,5.926856708949693,5.276461804173244,5.600972895686748,5.342422680822207,5.841984804590114,5.875032309461098,5.176091259055681,5.176091259055681,5.954242509439325,5.426511261364575,5.298853076409706,5.544068044350276,5.326335860928752,5.110589710299249,5.139879086401237,5.942008053022313,5.986771734266245,5.544068044350276,5.446381812222442,5.829303772831025,5.652246341003323,5.653212513775344,5.379305517750582,5.173186268412274,5.096910013008056,5.491361693834273,5.739572344450092,6.054995861529141,5.54282542695918,5.9164539485499255,5.685741738602264,6.174641192660449,5.929418925714293,5.498310553789601,4.8750612633917,5.57978359661681,5.937016107464814,6.079181246047625,5.378397900948138,5.76715586608218,5.694605198933568,5.359835482339888,6.318063334962762,5.359835482339888,6.295567099962479,5.173186268412274,5.5301996982030825,5.661812685537261,5.6674529528899535,5.397070549959409,5.514547752660286,5.298853076409706,5.176091259055681,5.359835482339888,5.217483944213907,5.439332693830263,5.298853076409706,5.376576957056512,5.76715586608218,5.9003671286564705,5.598790506763115,5.217483944213907,5.537819095073274,5.136720567156407,5.76715586608218,5.841984804590114,5.145817714491828,5.600972895686748,5.217483944213907,5.491361693834273,5.911157608739977,5.812244696800369,5.439332693830263,5.505149978319906,5.628388930050312,5.361727836017593,6.112269768417271,5.201397124320452,5.578639209968072,5.602005701124516,5.503790683057181,5.7363965022766426,5.113943352306837,6.278753600952829,5.445604203273597,5.267171728403014,5.841984804590114,5.1303337684950066,5.252853030979893,5.161368002234975,4.903089986991944,5.332438459915605,5.230448921378274,5.832508912706237,5.993436230497612,5.7363965022766426,5.5301996982030825,5.110589710299249,5.342422680822207,5.889301702506311,5.6674529528899535,5.77451696572855,6.276461804173244,5.429752280002408,5.060697840353612,5.45484486000851,5.2027606873932,5.429752280002408,5.977723605288848,5.096910013008056,4.99563519459755,5.585460729508501,5.474216264076255,5.144574207609616,5.45484486000851,5.389166084364533,5.677515704798758,5.574031267727719,6.168792020314182,5.623249290397901,5.596597095626461,4.99563519459755,5.648360010980932,5.628388930050312,5.396199347095736,5.596597095626461,5.829303772831025,5.173186268412274,5.53135116458306,5.54282542695918,5.041392685158225,5.45484486000851,5.517195897949974,5.623249290397901,5.770852011642144,6.201397124320452,5.77451696572855,6.021189299069938,5.77451696572855,6.458637849025649,6.0603200286882855,6.190331698170292,6.212187604403958,5.601951404133522,5.45484486000851,5.378397900948138,5.54282542695918,5.653212513775344,5.567026366159061,5.9003671286564705,5.69810054562339,6.096910013008056,5.525044807036846,5.4048337166199385,5.6006135561423145,7.041392685158225,5.243038048686294,5.860338006570994,5.176091259055681,5.676693609624866,5.841984804590114,5.6127838567197355,5.711807229041191,5.740362689494244,5.511883360978874,5.537819095073274,5.252853030979893,6.254064452914338,5.531478917042255,5.511883360978874,5.290034611362518,5.812913356642856,5.600972895686748,5.060697840353612,5.672005445022952,5.037426497940624,5.585460729508501,5.795184589682424,5.694605198933568,5.503790683057181,5.574031267727719,5.928907690243952,5.561485397401995,5.69810054562339,5.361727836017593,5.173186268412274,5.439332693830263,5.6875289612146345,5.342422680822207,5.39776625612645,5.929418925714293,5.770115294787102,5.6464037262230695,5.276461804173244,5.76715586608218,5.872156272748293,5.229169702539101,5.477121254719663,5.648360010980932,5.555094448578319,5.9164539485499255,5.8750612633917,5.113609151073028,5.740362689494244,5.929418925714293,5.445604203273597,6.077367905284157,5.739572344450092,5.096910013008056,5.1303337684950066,6.229169702539101,5.359835482339888,5.469822015978163,5.661812685537261,4.99563519459755,5.643452676486188,5.9003671286564705,5.929418925714293,5.096910013008056,5.902546779313991,5.617157665694944,5.812913356642856,5.45484486000851,5.860338006570994,5.361727836017593,5.489958479424835,5.90200289135073,5.3404441148401185,6.144574207609616,5.6180480967120925,5.623249290397901,5.298853076409706,5.238046103128795,5.204119982655925,5.77451696572855,5.298853076409706,5.460897842756548,5.671172842715083,5.676693609624866,5.2552725051033065,5.439332693830263,5.252853030979893,5.444044795918076,5.342422680822207,5.413299764081252,5.271841606536499,5.600972895686748,5.460897842756548,5.812913356642856,5.201397124320452,5.429752280002408,5.556302500767287,5.544068044350276,5.845098040014257,5.928907690243952,5.913813852383717,6.439332693830263,5.653212513775344,5.1003705451175625,5.3979400086720375,5.505149978319906,5.812913356642856,5.7558748556724915,5.290034611362518,5.54282542695918,5.47567118832443],\n\"predicted_values\":[5.198017583346747,5.352777437598908,6.023609158048497,6.137713882547114,5.8620383632430135,5.956277921734019,5.5535350741511875,6.146087411574098,5.88921026506747,5.10008717681639,5.822651520719825,5.982719504504639,5.336268161819687,5.403421690323308,5.08510651664321,5.3622272627529854,5.928859632734965,5.768289994159694,5.165511163053322,5.280435719136627,5.090187405502687,5.965403397787371,5.3233387212229015,5.336277972362316,5.721611338067426,5.704692601757591,6.07742979667367,5.889405852387379,5.628007917384083,5.554647130592206,5.440557514760086,5.547315346284626,6.004759741721686,5.8680508885078915,6.199486371438465,5.546089300633953,5.36347612176344,5.760900114566021,5.36162149541067,6.016044955262136,5.716223731956669,5.541758664561429,6.279694485507539,5.413325076002355,5.550008422807423,5.165959061286612,5.71245511715615,5.529961486885903,5.371293662766824,5.9440904595998365,5.146916087112651,5.206955216274487,5.384094632895634,5.441147990554912,5.565052118164606,5.991714401323557,5.597227579158822,5.310606147369211,5.2958121164793965,5.971181946332835,5.635706430318455,5.696031097521085,5.979707502657198,5.4320055157521745,5.783644001304042,5.466784136239073,5.9575266021108595,5.638799347350912,5.968575280630661,5.171921236456575,5.970765011671315,5.705476052192901,5.767761762972211,5.650196278371377,5.120724462431466,6.188062873810361,5.346930248983752,5.840205230989805,5.605567216427271,5.587496434737329,5.781912708317513,5.188101095395462,5.575980890792882,5.2280094630742235,5.475645169425742,5.64362698018977,5.645038059690255,5.417206976993192,5.084133716920452,6.369391902712625,5.72413810395866,5.203342597414071,5.709336217296337,5.3213063962684215,5.998291063025936,5.565409413213112,5.626060183755987,5.273608128166394,5.840016688312368,5.771053960694083,5.69348739318209,5.709622967365702,5.781568385174735,5.448965709525484,5.5598244129750185,5.46451551818865,5.7417342451523385,5.080227726213606,5.649745174446148,5.59762330392374,5.951244935476236,5.51911083028027,5.530708868305543,5.601713585694922,5.567521404320344,5.769104684872131,5.941614611216826,5.5668823790631174,5.522082648118855,5.620332700884624,5.501573514832586,5.165180654992431,5.600022245669818,5.9242416901347,5.605001558625751,5.639824221566237,5.391949760187745,6.163130432899817,5.176806975469519,5.454615496949427,5.645183796824524,5.5787064739288565,5.16523260123306,5.5620324205806515,5.668646300603935,5.695237547646805,5.234169696730729,5.253216529448627,5.382020395883666,5.675286143793546,5.397511139797798,5.6724354731415465,5.731793832440589,5.265589441142735,5.252591236286762,5.523952034163472,5.284953349710064,6.214088509208359,5.57457308362894,5.132896525270499,5.373001172650547,5.4631587091779155,5.572462319063994,5.507335029890791,5.024809845339182,5.428300820707567,5.332961921280063,5.732277512425799,6.164275669977843,5.9078833406867854,5.186989508675565,5.314312907986055,5.43893829905711,5.925751325238853,5.261711471035012,5.78417536953123,5.829065156307034,5.963635427639431,5.493078304125806,6.70681579535296,5.478656255043698,5.621854193713348,5.510674784684393,5.880551644583828,5.588211504833062,5.153694680282873,5.254718295325511,5.394195890652872,5.215341680430553,5.757788949651094,5.558195793459656,5.701773452148211,5.6113064129136365,5.330314968515697,5.560046894731406,5.575666023272752,5.603447596556163,6.107082859898615,5.825134325770681,5.76027503290314,5.583098143751556,5.671224928012176,5.584720027956888,5.715589546613673,5.269518556284456,5.483631470175581,5.324789586070667,5.620343217486969,5.4134114307309416,5.28631182053033,5.274822065955216,5.611039549754596,5.838194180444299,5.18621960059434,5.500178598405289,5.173685555022058,6.146230516627,5.20003430914506,5.517049320755662,5.4983462781977845,5.711578667433243,5.481909546884301,5.907569296090188,5.799119862440784,4.928808854011247,5.451221358854105,5.735738335488402,5.947428396340188,5.605267631326931,5.171458802306371,5.339004084590545,5.907504599266661,5.705235142028197,5.517046247923625,5.508976162934281,5.362747263812486,5.311910841237611,5.639579675596188,5.185693907879128,6.459265373910344,5.106870703703752,5.570443357652001,5.669184907490548,5.425438337741688,5.273133153371047,5.698651448550514,5.7618976332438,6.156519333571467,5.691533130971305,5.814563859157965,5.316108229579724,6.0496151913617116,5.470216133671353,5.154237942140793,5.550948813584717,6.003454359131398,5.369436264177484,5.397816420741005,5.56177887188529,6.279428475273905,5.857293329022074,5.4636075240043365,6.197010802159038,5.629215713102649,5.453598090856618,5.572188998679407,5.448162355880633,5.658242619752865,5.698847760084977,4.997453874768905,5.7446130689227655,5.257352802047809,5.817070153817085,5.2308886043988245,5.7981633373221095,5.91323766531743,5.34162113356189,5.275845610956187,5.630894629356396,5.423985424502534,5.602071631021028,5.417262519138885,6.1298289277668845,5.112233631785711,5.715482960901448,5.313469892924141,5.430860904339057,5.003216334604454,5.572138743002416,5.508516004882625,5.325955610461827,5.207993192666924,5.489792882299614,5.850864048886915,5.58824978666596,5.858199648888489,6.124444256591561,5.188418501505234,5.9060579901642525,5.527839333765918,5.679840436996017,5.928052111645805,5.735496953354869,5.188097761056061,5.652934095840561,5.5397311230918564,5.342737752841274,5.947748879108513,5.1410746076365275,5.908244258078245,5.362362017804158,5.717787511861751,5.247788137480146,5.460925553353683,5.301705453889825,5.51944723232358,5.691787549347657,5.639571268360179,5.721172141947082,5.361894530596403,5.481319413455112,5.332961921280063,5.298832026073857,5.214486281642191,5.385473410818085,5.415116667021319,5.515422392129923,5.562843756627692,6.088229107169351,5.779957366233711,5.476879292663629,5.628987654766308,5.94552526349642,5.255652203749406,5.720227658809648,5.526552082991632,5.868634533345379,5.25997769604785,5.442187438984401,5.753667414209078,6.018268684164322,5.334726343520323,5.362800026441242,5.509063204019241,5.265227548952146,5.587042965940035,5.815076952576418,5.572228460331092,5.902771120510603,5.544066066819803,5.604072297253942,5.249317753609979,5.151375356713036,5.757287642859538,5.569692396428292,5.933624776677919,5.536196421240205,6.143506343292934,5.196716918122939,6.295958977449999,5.923323051725628,5.424523400478829,6.257476562303869,5.205976081919858,4.99950597843359,6.016534591170305,5.361000256142883,5.737977497675629,5.703396173145867,5.557127323160546,5.429280180832224,5.207873789181113,5.756416594595246,5.4369599259387416,6.106002189575852,5.2025197626532345,5.450781925301626,5.528053275712209,5.6526573044594635,5.433931576782043,5.754625217583414,5.238750794285808,5.238416664375548,5.266475323219306,5.151751307080775,5.484292911979833,5.6624173266444835,5.387516166735628,5.542504569371797,5.394870438398621,5.296506086488534,5.969807980934295,5.915297371972488,5.338809328943679,5.417514816585042,5.577920399383912,5.576402251874571,5.189512824616152,5.7367250674103945,5.399514949432445,5.513298896593367,5.470397433104543,5.050692662416463,5.310289715101631,5.701594876611671,5.6767791911274745,6.138717004844734,5.5710715994003195,5.537025048158937,6.516693423575787,5.86969896375554,5.528691305283036,6.3755277482699615,5.789040288804759,5.516721617562723,6.018915973293321,5.503914200181409,5.217537027160943,5.395633933369193,5.79758148034431,5.406461449503686,5.741826039266559,5.711404434535311,5.363150700328368,6.448219523849274,5.981026346075805,5.521584978099806,5.063935532077486,5.866357758697052,5.728631039969817,6.134482939572457,5.245133617765642,5.631958650717137,5.447503621534745,5.682150410921199,5.959724963715657,5.299689642624567,5.25838648397475,6.138466880718244,5.321637213166788,5.628983465761611,5.58863309038215,5.35939618352678,5.265710393867689,5.317777705627043,6.092670336870084,5.763438800590446,5.597622137878272,5.515689051724562,5.640826925532147,5.713240710388141,5.574427789706389,5.41086759384935,5.184336113127033,5.385490301278289,5.397310355179095,5.807916713097449,5.779111968339291,5.64065971389792,5.97703300039612,5.640471120398952,5.946669595771708,5.809161797233743,5.529790335775809,4.960960075761029,5.678386832071933,5.770074337728096,6.221488634139354,5.350312485211146,5.936162664963831,5.647453618300168,5.296727668842039,6.081349744756868,5.2011943870616015,6.455156588086539,5.156701696799707,5.4666215925266695,5.600656086362176,5.786318965746873,5.332158930023389,5.575689716645086,5.236538354714582,5.301861524628997,5.308481420143049,5.270469131365785,5.357541838007442,5.208155878472712,5.319315295695196,5.780412960535951,6.019148587298271,5.514836128240111,5.246236501741301,5.531527918130423,5.19822346285706,5.7301915821699,5.824438465890068,5.156338586854264,5.502426181642596,5.33434177350717,5.449190933824984,5.867826276115864,5.759282076044046,5.518154392100389,5.585919848169958,5.424648404096696,5.443368004077762,5.651815632985891,5.307970306255479,5.559929380322825,5.550421555332423,5.4295769385650665,5.5812985175456635,5.287409248197765,5.897294531635131,5.467486287396651,5.263440925206874,5.618978740786313,5.11819118618609,5.305555596604865,5.2696211150960215,4.990351757091706,5.278072032754384,5.3830428029473465,5.684870441379438,6.086924384325609,5.723594333371643,5.4249585261985676,5.213896914801966,5.5053184552085375,5.787670409515331,5.842328446334215,5.65451777328368,6.217109764488271,5.493770973232106,5.28551100019914,5.536625205103072,5.271194314242116,5.416624721092199,6.39654496732134,5.040194945307258,5.101926864838111,5.582286729698577,5.4949765504409624,5.214680625094447,5.3773002346314485,5.276682476543924,5.652961708647674,5.483595796008277,6.188616877665897,5.583667038265964,5.503667187292799,5.070796908788482,5.611677211823913,5.761163693473181,5.4293164383480725,5.676928854998905,5.749799735956988,5.274384005036521,5.552860845044727,5.6333462123925395,5.121734148204886,5.670086409250256,5.577367025223131,5.590616521003858,5.730443953690298,6.148377004146984,5.742989285044884,5.881109717288328,5.867197086903931,6.703692274162019,5.854925385355717,6.1034912134712185,6.272369874049409,5.742516655090383,5.354411211832687,5.341064905152176,5.654979352995307,5.668830754084768,5.4022682145934855,6.154133485948788,5.872449964988526,6.154296204076071,5.52147651048124,5.405792603028328,5.72994648141281,6.89935014686972,5.253141611167397,5.861910672652937,5.131339461795893,5.594620474360187,5.80212584395581,5.634246090384568,5.648874329104871,5.759219124052448,5.93661751165039,5.4795722933091735,5.382069305304236,6.197878078157575,5.581680442435141,5.57274095554817,5.3194838127499455,5.824233210109293,5.639980740691873,5.060925532446114,5.9638315238308355,5.078588604267159,5.5237440386719285,5.947428396340188,5.615662461240563,5.585685328444879,5.674710518847244,5.831485367561527,5.529098293234901,5.550792554164723,5.464161400825264,5.113183684514184,5.457427672878463,5.82503985332181,5.451951444562301,5.179154200670942,5.856263294940783,5.758611782740409,5.667690225271999,5.156288677069836,5.774215792900317,5.839234546260816,5.204119394133439,5.362768050491922,5.594893821028335,5.781568385174735,5.921519289314543,5.735652335970908,5.145495582238634,5.783882491005601,5.9081501010890705,5.437296988195057,6.078815023485706,5.666565116098967,5.18913143717778,5.105329383710775,6.349415704880633,5.226602121637222,5.391683817592193,5.6684655579754475,5.173642144767373,5.70871199571034,5.923189337513152,5.9366635925081805,5.086107993519377,5.9727317575308065,5.709188054005443,5.78021969352419,5.519632076547831,6.033568702721866,5.397189350680977,5.586887885620073,5.950841741650754,5.365928124865139,6.1845258138096355,5.622083175437912,5.6637755527660065,5.368169942298264,5.547263607565205,5.385986442171129,5.790725846386913,5.313119031590197,5.5028534845695845,5.544608792507228,5.722242688972962,5.251370911711769,5.4294768481643,5.215605089001836,5.528837831951164,5.398568938073277,5.338398680815754,5.278559769402217,5.646150958299239,5.366796517905556,5.859148278316664,5.235176015888912,5.312772791508672,5.524467566908656,5.882173404635367,5.615625955860039,5.932581086351784,5.966591654743582,6.386344632868584,5.5669005974870815,5.091992938648918,5.274745674573215,5.418834483477004,5.776832808527544,5.6785601211789185,5.193649256502248,5.545253078550717,5.543813900358861]\n},\n\"mapping\":{\n\"x\":\"original_values\",\n\"y\":\"predicted_values\"\n},\n\"data_meta\":{\n},\n\"ggtitle\":{\n\"text\":\"Contrasting Predicted House Prices with Actual House Prices\",\n\"subtitle\":\" The plot suggests that the model makes accurate predictions on the test data. This is evident from the low RMSE values, \\n            signifying a high level of accuracy. Additionally, the high R2 value indicates that the model effectively accounts for a \\n            substantial portion of the data's variance, demonstrating a strong alignment between the model's predictions and the actual data.\\n            \"\n},\n\"caption\":{\n\"text\":\"https://www.immoweb.be/\"\n},\n\"theme\":{\n\"plot_title\":{\n\"face\":\"bold\",\n\"size\":15.0,\n\"blank\":false\n},\n\"plot_subtitle\":{\n\"face\":\"italic\",\n\"size\":12.0,\n\"blank\":false\n}\n},\n\"ggsize\":{\n\"width\":800.0,\n\"height\":600.0\n},\n\"kind\":\"plot\",\n\"scales\":[{\n\"name\":\"log10 True Prices (EUR)\",\n\"aesthetic\":\"x\"\n},{\n\"name\":\"log10 Predicted Prices (EUR)\",\n\"aesthetic\":\"y\"\n}],\n\"layers\":[{\n\"geom\":\"point\",\n\"mapping\":{\n},\n\"data_meta\":{\n},\n\"data\":{\n}\n},{\n\"geom\":\"smooth\",\n\"mapping\":{\n},\n\"data_meta\":{\n},\n\"data\":{\n\"..ymin..\":[4.927255768057578,4.952795218615501,4.978330116488928,5.0038599754580275,5.029384242334826,5.054902286057649,5.080413384860043,5.105916711197485,5.131411314092702,5.156896098559321,5.182369801802468,5.207830966003595,5.233277907720211,5.258708684334698,5.284121058659274,5.309512463857677,5.3348799723957345,5.360220274861389,5.385529677147597,5.410804127325778,5.43603928570857,5.461230651587924,5.486373755838504,5.51146441803555,5.536499049687475,5.56147496499427,5.58639064491027,5.611245898676874,5.636041884736443,5.6607809858905735,5.685466568672776,5.710102679781674,5.7346937360218915,5.759244251584405,5.783758626650039,5.808241002840359,5.832695178584205,5.857124571586693,5.881532214603129,5.905920772464421,5.930292571061515,5.954649631751779,5.978993706939586,6.003326314291581,6.027648768228603,6.051962208103525,6.076267622943071,6.100565872897825,6.124857707680063,6.149143782323918,6.173424670610092,6.19770087647953,6.221972843730156,6.246240964256075,6.270505585053854,6.294767014187997,6.3190255258785815,6.343281364848576,6.367534750046388,6.39178587784062,6.416034924768336,6.440282049904888,6.464527396912458,6.488771095815179,6.513013264541141,6.537254010265127,6.561493430580669,6.585731614525527,6.609968643481044,6.6342045919626695,6.658439528316397,6.68267351533366,6.706906610795376,6.731138867954347,6.755370335963835,6.779601060259096,6.803831082897709,6.828060442863674,6.852289176339701,6.876517316951395],\n\"..ymax..\":[4.966284982310939,4.990559961040777,5.014839492455108,5.0391240627737695,5.063414225184728,5.0877106107496655,5.112013941235032,5.136325044185349,5.160644870577892,5.184974515399031,5.209315241443645,5.233668506530275,5.258035994101419,5.282419646774692,5.306821701737876,5.331244725827231,5.355691646576934,5.380165773399039,5.4046708004005914,5.429210779510169,5.453790050415136,5.478413113823542,5.5030844388607205,5.527808205951435,5.552588003587269,5.5774265175682345,5.602325266939991,5.627284442461148,5.652302885689338,5.677378213822967,5.702507060328523,5.7276853785073865,5.752908751554927,5.778172665280173,5.803472719502299,5.828804772599738,5.854165026143652,5.8795500624289225,5.904956848700247,5.930382720126714,5.955825350817379,5.981282719414874,6.006753073514827,6.032234895450591,6.057726870801329,6.083227860214167,6.10873687466238,6.134253053995385,6.159775648500905,6.185304003144811,6.210837544146396,6.236375767564718,6.261918229601851,6.287464538363692,6.31301434685367,6.338567347007288,6.364123264604464,6.389681854922228,6.415242899012177,6.440806200505703,6.466371582865747,6.4919388870169525,6.517507969297143,6.543078699682181,6.568650960243979,6.594224643807752,6.61979965277997,6.645375898122871,6.670953298455113,6.696531779261248,6.722111272195279,6.747691714465777,6.773273048291819,6.798855220420607,6.82443818169888,6.850021886691377,6.875606293340525,6.901191362662318,6.92677705847405,6.952363347150117],\n\"original_values\":[4.8750612633917,4.902483180122922,4.929905096854144,4.957327013585366,4.984748930316587,5.012170847047809,5.039592763779031,5.067014680510253,5.094436597241475,5.121858513972697,5.149280430703919,5.17670234743514,5.204124264166362,5.231546180897584,5.258968097628806,5.286390014360028,5.31381193109125,5.341233847822472,5.368655764553694,5.396077681284915,5.423499598016137,5.450921514747359,5.478343431478581,5.505765348209803,5.5331872649410245,5.5606091816722465,5.588031098403468,5.6154530151346895,5.6428749318659115,5.6702968485971335,5.697718765328355,5.725140682059577,5.752562598790799,5.77998451552202,5.807406432253242,5.834828348984464,5.862250265715686,5.889672182446908,5.91709409917813,5.944516015909352,5.971937932640573,5.999359849371795,6.026781766103017,6.054203682834239,6.081625599565461,6.109047516296683,6.136469433027905,6.163891349759126,6.191313266490348,6.21873518322157,6.246157099952792,6.273579016684014,6.301000933415236,6.328422850146458,6.355844766877679,6.383266683608901,6.410688600340123,6.438110517071345,6.465532433802567,6.492954350533789,6.520376267265011,6.547798183996232,6.575220100727454,6.602642017458676,6.630063934189898,6.65748585092112,6.684907767652342,6.712329684383564,6.739751601114785,6.767173517846007,6.794595434577229,6.8220173513084506,6.8494392680396725,6.876861184770894,6.904283101502116,6.9317050182333375,6.95912693496456,6.986548851695781,7.013970768427003,7.041392685158225],\n\"predicted_values\":[4.946770375184259,4.971677589828139,4.996584804472018,5.0214920191158985,5.046399233759777,5.071306448403657,5.096213663047537,5.121120877691417,5.146028092335297,5.170935306979176,5.195842521623057,5.220749736266935,5.245656950910815,5.270564165554695,5.295471380198575,5.320378594842454,5.3452858094863345,5.370193024130214,5.395100238774094,5.420007453417973,5.444914668061853,5.469821882705733,5.494729097349612,5.519636311993493,5.544543526637372,5.569450741281252,5.594357955925131,5.619265170569011,5.64417238521289,5.6690795998567705,5.69398681450065,5.71889402914453,5.743801243788409,5.768708458432289,5.793615673076169,5.818522887720048,5.843430102363929,5.868337317007808,5.893244531651688,5.9181517462955675,5.943058960939447,5.967966175583326,5.992873390227206,6.017780604871086,6.042687819514966,6.067595034158846,6.092502248802726,6.117409463446605,6.142316678090484,6.1672238927343646,6.192131107378244,6.217038322022124,6.2419455366660035,6.266852751309884,6.291759965953762,6.316667180597642,6.341574395241523,6.366481609885402,6.391388824529282,6.416296039173162,6.441203253817042,6.46611046846092,6.4910176831048005,6.51592489774868,6.54083211239256,6.5657393270364395,6.59064654168032,6.615553756324199,6.640460970968078,6.665368185611959,6.690275400255838,6.715182614899718,6.740089829543598,6.764997044187477,6.789904258831357,6.8148114734752365,6.839718688119117,6.864625902762996,6.8895331174068755,6.914440332050756]\n}\n},{\n\"geom\":\"text\",\n\"mapping\":{\n},\n\"data_meta\":{\n},\n\"x\":5.0,\n\"y\":6.6,\n\"label\":\"RMSE = 0.1102\",\n\"fontface\":\"bold\",\n\"data\":{\n}\n},{\n\"geom\":\"text\",\n\"mapping\":{\n},\n\"data_meta\":{\n},\n\"x\":4.965,\n\"y\":6.5,\n\"label\":\"R2 = 0.8771\",\n\"fontface\":\"bold\",\n\"data\":{\n}\n}],\n\"metainfo_list\":[]\n};\n           var plotContainer = document.getElementById(\"lzH5ce\");\n           window.letsPlotCall(function() {{\n               LetsPlot.buildPlotFromProcessedSpecs(plotSpec, -1, -1, plotContainer);\n           }});\n       })();    \n   </script>\n```\n\nContrasting Predicted House Prices with Actual House Prices\n:::\n:::\n\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\n(\n    results.pipe(lambda df: ggplot(df, aes(\"residuals\")) + geom_histogram(stat=\"bin\"))\n    + labs(\n        title=\"Assessing the Residuals from the Catboost Model\",\n        subtitle=\"\"\" Normally distributed residuals imply consistent and accurate model predictions, aligning with statistical assumptions.\n            \"\"\",\n        x=\"Distribution of Residuals\",\n        y=\"\",\n        caption=\"https://www.immoweb.be/\",\n    )\n    + theme(\n        plot_subtitle=element_text(\n            size=12, face=\"italic\"\n        ),  # Customize subtitle appearance\n        plot_title=element_text(size=15, face=\"bold\"),  # Customize title appearance\n    )\n    + ggsize(800, 600)\n)\n```\n\n::: {#fig-fig2 .cell-output .cell-output-display execution_count=35}\n```{=html}\n   <div id=\"V8FEGS\"></div>\n   <script type=\"text/javascript\" data-lets-plot-script=\"plot\">\n       (function() {\n           var plotSpec={\n\"data\":{\n},\n\"mapping\":{\n\"x\":\"residuals\"\n},\n\"data_meta\":{\n},\n\"ggtitle\":{\n\"text\":\"Assessing the Residuals from the Catboost Model\",\n\"subtitle\":\" Normally distributed residuals imply consistent and accurate model predictions, aligning with statistical assumptions.\\n            \"\n},\n\"caption\":{\n\"text\":\"https://www.immoweb.be/\"\n},\n\"theme\":{\n\"plot_title\":{\n\"face\":\"bold\",\n\"size\":15.0,\n\"blank\":false\n},\n\"plot_subtitle\":{\n\"face\":\"italic\",\n\"size\":12.0,\n\"blank\":false\n}\n},\n\"ggsize\":{\n\"width\":800.0,\n\"height\":600.0\n},\n\"kind\":\"plot\",\n\"scales\":[{\n\"name\":\"Distribution of Residuals\",\n\"aesthetic\":\"x\"\n},{\n\"name\":\"\",\n\"aesthetic\":\"y\"\n}],\n\"layers\":[{\n\"geom\":\"histogram\",\n\"stat\":\"bin\",\n\"mapping\":{\n},\n\"data_meta\":{\n},\n\"data\":{\n\"..count..\":[2.0,2.0,0.0,1.0,5.0,4.0,2.0,6.0,14.0,26.0,30.0,50.0,66.0,74.0,102.0,98.0,74.0,52.0,29.0,18.0,8.0,9.0,4.0,5.0,2.0,0.0,1.0,1.0,0.0,1.0],\n\"residuals\":[-0.4451767156871108,-0.4137652402368506,-0.38235376478659033,-0.3509422893363301,-0.31953081388606985,-0.2881193384358096,-0.25670786298554943,-0.2252963875352892,-0.19388491208502895,-0.1624734366347687,-0.13106196118450847,-0.09965048573424823,-0.06823901028398804,-0.036827534833727804,-0.005416059383467564,0.025995416066792676,0.057406891517052916,0.08881836696731316,0.1202298424175734,0.15164131786783364,0.18305279331809388,0.21446426876835412,0.24587574421861436,0.2772872196688746,0.3086986951191347,0.34011017056939497,0.3715216460196552,0.40293312146991545,0.4343445969201757,0.4657560723704359]\n}\n}],\n\"metainfo_list\":[]\n};\n           var plotContainer = document.getElementById(\"V8FEGS\");\n           window.letsPlotCall(function() {{\n               LetsPlot.buildPlotFromProcessedSpecs(plotSpec, -1, -1, plotContainer);\n           }});\n       })();    \n   </script>\n```\n\nAssessing the Residuals from the Catboost Model\n:::\n:::\n\n\nAnd there you have it! We reached the end of these series! ðŸ¥³ðŸŽ†ðŸŽ‰ðŸ¾ðŸ»ðŸ•º \n\nOver these seven articles, we've shown how to build a reliable, high-performing machine learning model for real-time house price prediction. While there's always room for improvement, like exploring geolocation-based feature engineering, blending, and stacking, our aim was to provide a comprehensive guide from start to finish. We hope you've enjoyed this journey and gained inspiration and insights for your own projects.  \n\nUntil next time! ðŸ’»ðŸðŸ¼\n\n",
    "supporting": [
      "NB_7_ACs_Fine_tuning_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}