<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Adam Cseresznye">
<meta name="dcterms.date" content="2023-12-23">

<title>Adam Cseresznye - DeepLCMS: A framework that leverages transfer learning for the classification of pseudo images in mass spectrometry-based analysis</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../favicon_lower_A.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

<script type="text/javascript">

(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-BQF3JENT9N', 'auto');

ga('send', {
  hitType: 'pageview',
  'anonymizeIp': true,
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Adam Cseresznye - DeepLCMS: A framework that leverages transfer learning for the classification of pseudo images in mass spectrometry-based analysis">
<meta property="og:description" content="">
<meta property="og:image" content="https://adamcseresznye.github.io/blog/projects/DeepLCMS/exp-5-prediction_matrix.png">
<meta property="og:site-name" content="Adam Cseresznye">
<meta property="og:image:height" content="2883">
<meta property="og:image:width" content="2849">
<meta name="twitter:title" content="Adam Cseresznye - DeepLCMS: A framework that leverages transfer learning for the classification of pseudo images in mass spectrometry-based analysis">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="https://adamcseresznye.github.io/blog/projects/DeepLCMS/exp-5-prediction_matrix.png">
<meta name="twitter:image-height" content="2883">
<meta name="twitter:image-width" content="2849">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Adam Cseresznye</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../scientific_output.html" rel="" target="">
 <span class="menu-text">Scientific Output</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../articles.html" rel="" target="">
 <span class="menu-text">Articles</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../projects.html" rel="" target="">
 <span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/adamcseresznye" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/csenye22" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/adam-cseresznye" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">DeepLCMS: A framework that leverages transfer learning for the classification of pseudo images in mass spectrometry-based analysis</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
                                <div class="quarto-categories">
                <div class="quarto-category">DeepLCMS</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Adam Cseresznye </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">December 23, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#importing-libraries" id="toc-importing-libraries" class="nav-link active" data-scroll-target="#importing-libraries">Importing libraries</a></li>
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">Introduction</a>
  <ul class="collapse">
  <li><a href="#application-of-pretrained-neural-networks-for-mass-spectrometry-data" id="toc-application-of-pretrained-neural-networks-for-mass-spectrometry-data" class="nav-link" data-scroll-target="#application-of-pretrained-neural-networks-for-mass-spectrometry-data">Application of Pretrained Neural Networks for Mass Spectrometry Data</a></li>
  <li><a href="#previous-research" id="toc-previous-research" class="nav-link" data-scroll-target="#previous-research">Previous Research</a></li>
  </ul></li>
  <li><a href="#materials-and-methods" id="toc-materials-and-methods" class="nav-link" data-scroll-target="#materials-and-methods">Materials and Methods</a>
  <ul class="collapse">
  <li><a href="#dataset" id="toc-dataset" class="nav-link" data-scroll-target="#dataset">Dataset</a></li>
  <li><a href="#software" id="toc-software" class="nav-link" data-scroll-target="#software">Software</a></li>
  <li><a href="#creating-the-pseudo-images" id="toc-creating-the-pseudo-images" class="nav-link" data-scroll-target="#creating-the-pseudo-images">Creating the pseudo images</a></li>
  <li><a href="#untargeted-data-analysis-with-ms-dial" id="toc-untargeted-data-analysis-with-ms-dial" class="nav-link" data-scroll-target="#untargeted-data-analysis-with-ms-dial">Untargeted data analysis with MS-DIAL</a></li>
  </ul></li>
  <li><a href="#results-and-discussion" id="toc-results-and-discussion" class="nav-link" data-scroll-target="#results-and-discussion">Results and Discussion</a>
  <ul class="collapse">
  <li><a href="#selecting-a-model-architecture-family" id="toc-selecting-a-model-architecture-family" class="nav-link" data-scroll-target="#selecting-a-model-architecture-family">Selecting a model architecture family</a></li>
  <li><a href="#selecting-image-properties-as-hyperparameters" id="toc-selecting-image-properties-as-hyperparameters" class="nav-link" data-scroll-target="#selecting-image-properties-as-hyperparameters">Selecting image properties as hyperparameters</a></li>
  <li><a href="#hyperparameter-tuning-with-optuna" id="toc-hyperparameter-tuning-with-optuna" class="nav-link" data-scroll-target="#hyperparameter-tuning-with-optuna">Hyperparameter tuning with Optuna</a></li>
  <li><a href="#evaluating-the-final-model-on-the-test-set" id="toc-evaluating-the-final-model-on-the-test-set" class="nav-link" data-scroll-target="#evaluating-the-final-model-on-the-test-set">Evaluating the final model on the test set</a></li>
  <li><a href="#model-interpretability" id="toc-model-interpretability" class="nav-link" data-scroll-target="#model-interpretability">Model interpretability</a>
  <ul class="collapse">
  <li><a href="#estimating-probability" id="toc-estimating-probability" class="nav-link" data-scroll-target="#estimating-probability">Estimating probability</a></li>
  <li><a href="#looking-at-layer-class-activation-maps" id="toc-looking-at-layer-class-activation-maps" class="nav-link" data-scroll-target="#looking-at-layer-class-activation-maps">Looking at Layer Class Activation Maps</a></li>
  <li><a href="#overlaying-the-untargeted-metabolomics-data-with-the-class-activation-map" id="toc-overlaying-the-untargeted-metabolomics-data-with-the-class-activation-map" class="nav-link" data-scroll-target="#overlaying-the-untargeted-metabolomics-data-with-the-class-activation-map">Overlaying the untargeted metabolomics data with the class activation map</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#conclusions" id="toc-conclusions" class="nav-link" data-scroll-target="#conclusions">Conclusions</a></li>
  <li><a href="#get-in-touch" id="toc-get-in-touch" class="nav-link" data-scroll-target="#get-in-touch">Get in touch</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="exp-5-prediction_matrix.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">Representative examples of DeepLCMS predictions accompanied by their corresponding probability estimates.</figcaption>
</figure>
</div>
<p>Welcome to DeepLCMS, a project that combines mass spectrometry analysis with the power of deep learning models!</p>
<p>Unlike conventional methods, DeepLCMS streamlines the data processing pipeline, bypassing the need for time-consuming and expertise-dependent steps such as peak alignment, data annotation and quantitation. Instead, it relies on the power of deep learning by recognizing important features to directly classify mass spectrometry-based pseudo-images with high accuracy. To demonstrate the capabilities of pre-trained neural networks for high-resolution LC/MS data, we successfully apply our convolutional neural network (CNN) to categorize substance abuse cases. We utilize the openly available Golestan Cohort Study’s metabolomics LC/MS data to train and evaluate our CNN <span class="citation" data-cites="pourshams_cohort_2010 ghanbari_metabolomics_2021 li_untargeted_2020">(<a href="#ref-pourshams_cohort_2010" role="doc-biblioref">Pourshams et al. 2010</a>; <a href="#ref-ghanbari_metabolomics_2021" role="doc-biblioref">Ghanbari et al. 2021</a>; <a href="#ref-li_untargeted_2020" role="doc-biblioref">Li et al. 2020</a>)</span>. We also go beyond classification by providing insights into the network’s decision-making process using the TorchCam library. TorchCam allows us to analyze the regions of interests that influence the network’s classifications, helping us identify key compound classes that play a crucial role in differentiating between them. This approach empowers us to unlock novel insights into the underlying molecular classes and potential pathways that drive phenotypic variations much faster compared to traditional methods.</p>
<p>We believe that applying cutting-edge CNN-based technologies, such as DeepLCMS, for non-discriminatory mass spectrometry analysis has the potential to significantly enhance the field of mass spectrometry analysis, enabling faster, more streamlined, and more informative data interpretation, particularly in domains that demand high-throughput and rapid decision-making processes. Its capacity to directly classify pseudo-images without extensive preprocessing could unlock a wealth of opportunities for researchers and clinicians alike.</p>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>DeepLCMS is an open-source project, freely available on <a href="https://github.com/adamcseresznye/DeepLCMS">GitHub</a>, that aims to provide researchers with a reproducible code-template for leveraging deep learning for mass spectrometry data analysis.</p>
<p>In contrast to traditional methods that involve laborious steps like peak alignment, data annotation, and quantitation, DeepLCMS delivers results significantly faster by approaching LC/MS problems as a computer vision task. This novel approach uses a neuronal network to directly learn from the patterns inherent in the sample in an unbiased way without the need for manual intervention, accelerating the entire workflow.</p>
<p>The study stands out from previous research by conducting a <em>comprehensive evaluation of diverse architecture families</em>, including cutting-edge architectures like vision transformers. Additionally, it employs <em>basic hyperparameter tuning</em> to optimize key parameters such as the optimizer and learning rate scheduler. Furthermore, it examines the <em>impact of image pretreatment</em> on validation metrics, exploring image sharpness and data augmentation techniques that mimic retention time shift. To enhance model generalization, this study takes advantage of <em>regularization techniques</em> like random-tilting images and random erasing during training. Finally, it also explores model interpretability by delving into the decision-making process of the pre-trained network, employing TorchVision for a comprehensive analysis.</p>
</div>
</div>
</div>
<section id="importing-libraries" class="level1">
<h1>Importing libraries</h1>
<div class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> io</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> PIL</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> ImageDraw, ImageFont</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Tuple</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>While computer vision has gained widespread adoption in various aspects of our lives<span class="citation" data-cites="dobson_birth_2023">(<a href="#ref-dobson_birth_2023" role="doc-biblioref">Dobson 2023</a>)</span>, its application in medical imaging and biosciences has lagged behind, primarily due to limitations in clinical dataset size, accessibility, privacy concerns, experimental complexity, and high acquisition costs. For such applications, transfer learning has emerged as a potential solution<span class="citation" data-cites="seddiki_towards_2020">(<a href="#ref-seddiki_towards_2020" role="doc-biblioref">Seddiki et al. 2020</a>)</span>. A technique that is particularly effective with small datasets, requiring fewer computational resources while achieving good classification accuracy compared to models trained from scratch. Transfer learning involves a two-step process. Initially, a robust data representation is learned by training a model on a dataset comprising a vast amount of annotated data encompassing numerous categories (<a href="https://www.image-net.org/">ImageNet</a> for example). This representation is then used to construct a new model based on a smaller annotated dataset containing fewer categories.</p>
<section id="application-of-pretrained-neural-networks-for-mass-spectrometry-data" class="level2">
<h2 class="anchored" data-anchor-id="application-of-pretrained-neural-networks-for-mass-spectrometry-data">Application of Pretrained Neural Networks for Mass Spectrometry Data</h2>
<p>The use of pre-trained neural networks for mass spectrometry data analysis is relatively new, with only a handful of publications available to date. These studies have demonstrated the potential of deep learning models to extract meaningful information from raw mass spectrometry data and perform predictive tasks without the need for extensive data preprocessing as required by the traditional workflows.</p>
</section>
<section id="previous-research" class="level2">
<h2 class="anchored" data-anchor-id="previous-research">Previous Research</h2>
<ul>
<li><p>In 2018, <span class="citation" data-cites="behrmann_deep_2018">Behrmann et al. (<a href="#ref-behrmann_deep_2018" role="doc-biblioref">2018</a>)</span> used deep learning techniques for tumor classification in Imaging Mass Spectrometry (IMS) data.</p></li>
<li><p>In 2020, <span class="citation" data-cites="seddiki_towards_2020">Seddiki et al. (<a href="#ref-seddiki_towards_2020" role="doc-biblioref">2020</a>)</span> utilized MALDI-TOF images of rat brain samples to assess the ability of three different CNN architectures – LeNet, Lecun, and VGG9 – to differentiate between different types of cancers based on their molecular profiles.</p></li>
<li><p>In 2021, <span class="citation" data-cites="cadow_feasibility_2021">Cadow et al. (<a href="#ref-cadow_feasibility_2021" role="doc-biblioref">2021</a>)</span> explored the use of pre-trained networks for the classification of tumors from normal prostate biopsies derived from SWATH-MS data. They delved into the potential of deep learning models for analyzing raw mass spectrometry data and performing predictive tasks without the need for protein quantification. To process raw MS images, the authors employed pre-trained neural network models to convert them into numerical vectors, enabling further processing. They then compared several classifiers, including logistic regression, support vector machines, and random forests, to accurately predict the phenotype.</p></li>
<li><p>In 2022, <span class="citation" data-cites="shen_deep_2022">Shen et al. (<a href="#ref-shen_deep_2022" role="doc-biblioref">2022</a>)</span> released deepPseudoMSI, a deep learning-based pseudo-mass spectrometry imaging platform, designed to predict the gestational age in pregnant women based on LC-MS-based metabolomics data. This application consists of two components: Pseudo-MS Image Converter: for converting LC-MS data into pseudo-images and the deep learning model itself.</p></li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Project Structure
</div>
</div>
<div class="callout-body-container callout-body">
<p>To accommodate the high computational demands of neural network training, the DeepLCMS project is divided into two main parts. The first part focuses on data preprocessing, specifically converting LC/MS data into pseudo-images using the PyOpenMS library, which is written in C++ and optimized for efficiency. This task can be handled on a CPU, and the corresponding source code is found in the <code>src/cpu_modules</code> directory.</p>
<p>To effectively train the neural networks, the project utilizes the PyTorch Lightning framework, which facilitates the development of a well-structured, modular codebase. Training experiments are conducted within Jupyter Notebooks running on Google Colab, a cloud platform offering free access to GPUs. The training code and associated modules reside within the <code>src/gpu_modules</code> directory, seamlessly integrating with Google Colab for effortless module imports.</p>
</div>
</div>
</section>
</section>
<section id="materials-and-methods" class="level1">
<h1>Materials and Methods</h1>
<section id="dataset" class="level2">
<h2 class="anchored" data-anchor-id="dataset">Dataset</h2>
<p>To ensure the feasibility of our proof-of-concept demonstration, we selected a suitable dataset from the <a href="https://www.metabolomicsworkbench.org/data/DRCCMetadata.php?Mode=Study&amp;StudyID=ST001618&amp;StudyType=MS&amp;ResultType=5">Metabolomics Workbench</a>. We prioritized studies with distinct groups and a minimum sample size of 200. Additionally, we selected a dataset with a disk requirement of less than 50 GB to minimize computational resource demands. Based on these criteria, we identified the Golestan Cohort Study <span class="citation" data-cites="pourshams_cohort_2010">(<a href="#ref-pourshams_cohort_2010" role="doc-biblioref">Pourshams et al. 2010</a>)</span>. This study, conducted in northeastern Iran comprising approximately 50,000 individuals, primarily investigates the risk factors for upper gastrointestinal cancers in this high-risk region. Quantitative targeted liquid chromatography mass spectrometric (LC-MS/MS) data was obtained for a subset of the cohort and collected at the University of North Carolina at Chapel Hill to identify distinct biochemical alterations induced by opium consumption <span class="citation" data-cites="ghanbari_metabolomics_2021 li_untargeted_2020">(<a href="#ref-ghanbari_metabolomics_2021" role="doc-biblioref">Ghanbari et al. 2021</a>; <a href="#ref-li_untargeted_2020" role="doc-biblioref">Li et al. 2020</a>)</span>. The dataset consisted of 218 opioid users and 80 non-users. After initial data inspection and conversion to mzML format using the ProteoWizard 3.0.22155 software, files were divided into training (n = 214), validation (n = 54), and test (n = 30) sets. To evaluate the impact of image characteristics and augmentation techniques on classification performance, four datasets were prepared. One dataset contained lower quality, more pixalated images due to the bin size set to 500 × 500 using the numpy library’s <code>histogram2d</code> function and was named <code>ST001618_Opium_study_LC_MS_500</code>. Another dataset, while also using bin size of 500 x 500, it also incorporated data augmentation using the <code>augment_images</code> function, generating nine additional images per training set with random offsets of up to 5 units in both x and y directions. This dataset was named <code>ST001618_Opium_study_LC_MS_500_augmented</code>. The third dataset employed a higher bin size of 1000 × 1000 leading to sharper pseudoimages and was named <code>ST001618_Opium_study_LC_MS_1000</code>. The final dataset employed image augmentation technique, generating nine additional images per training set with random offsets of up to 5 units in both x and y directions and used a bin size of 1000 × 1000. It was named <code>ST001618_Opium_study_LC_MS_1000_augmented</code>.</p>
</section>
<section id="software" class="level2">
<h2 class="anchored" data-anchor-id="software">Software</h2>
<p>The code base relied on the following software packages and versions on Google Colab: PyTorch Lightning 2.1.3, Pytorch Image Models (timm) 0.9.12, torchinfo 1.8.0, Optuna 3.5.0, TorchCam 0.4.0, pandas 2.1.3, NumPy 1.26, and Matplotlib 3.7.1. The remaining packages and their respective versions can be found in the requirements.txt file.</p>
</section>
<section id="creating-the-pseudo-images" class="level2">
<h2 class="anchored" data-anchor-id="creating-the-pseudo-images">Creating the pseudo images</h2>
<p>Mass spectrometry-based pseudo images are generated using the <code>plot_2D_spectra_overview</code> function, which creates a 2D heatmap of the LC/MS data. The function first loads the data in mzML format from the provided file path using the OpenMS library. A 2D histogram is then created, with the x-axis representing the retention time (Rt) and the y-axis representing the m/z values. The intensity values of the histogram are normalized using the highest peak at a given Rt. To enhance the visualization of the data, a Gaussian filter is applied to reduce noise, and the filtered image is scaled to the range 0-255. The histogram is then logarithmically transformed. Finally, pseudoimages are created using matplotlib, with the colormap set to ‘jet’.</p>
</section>
<section id="untargeted-data-analysis-with-ms-dial" class="level2">
<h2 class="anchored" data-anchor-id="untargeted-data-analysis-with-ms-dial">Untargeted data analysis with MS-DIAL</h2>
<p>To assess the model’s decision-making process and enable interpretability, we analyzed two pooled samples (sp_10 and sp_20) using MS-DIAL <span class="citation" data-cites="tsugawa_ms-dial_2015">(<a href="#ref-tsugawa_ms-dial_2015" role="doc-biblioref">Tsugawa et al. 2015</a>)</span>, a data-independent MS/MS deconvolution software, in positive mode applying the default settings. For metabolite identification, the MSP database from the <a href="http://prime.psc.riken.jp/compms/msdial/main.html">MS-DIAL</a> website was utilized, that contains ESI-MS/MS fragmentation data for 16,481 unique standards.</p>
</section>
</section>
<section id="results-and-discussion" class="level1">
<h1>Results and Discussion</h1>
<section id="selecting-a-model-architecture-family" class="level2">
<h2 class="anchored" data-anchor-id="selecting-a-model-architecture-family">Selecting a model architecture family</h2>
<p>From the readily available model architecture families provided by <a href="https://github.com/huggingface/pytorch-image-models#models">Pytorch Image Models</a>, 68 unique ones were selected as representative examples of a given class and filtered based on their parameter count, selecting those with parameters counts between 10 and 20 million to allow for an unbiased comparison. Out of the 68 selected 32 were subsequently underwent training with validation metrics recorded. According to <a href="#tbl-exp-1-result">Table&nbsp;1</a> (arranged based on validation loss), the MobileOne S3 emerged as the top performer with an F1 score of 0.95. It was followed by DenseNet (F1 = 0.92), MobileViTV2 (F1 = 0.90), and ConvNeXt Nano (F1 = 0.84). RepVit M3 rounded out the top five with an F1 score of 0.86. To delve deeper into the performance of each architecture family, we also evaluated all individual architectures within each family (<a href="#tbl-exp-1-best_models">Table&nbsp;2</a>).</p>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    pd.read_csv(<span class="st">"exp-1-result.csv"</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    .rename(</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>        columns<span class="op">=</span><span class="kw">lambda</span> df: df.replace(<span class="st">"_"</span>, <span class="st">" "</span>).replace(<span class="st">"val"</span>, <span class="st">"validation"</span>).title()</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    .rename(columns<span class="op">=</span>{<span class="st">"Minimal Param Model Count"</span>: <span class="st">"Parameter Count (M)"</span>})</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    .<span class="bu">round</span>(<span class="dv">2</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="2">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>

<div id="tbl-exp-1-result" class="anchored">
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<caption>Table&nbsp;1: Validation metrics of model architectural families from the PyTorch Image Models library using models with parameter counts ranging from 10 to 20 million.</caption>
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Model Family</th>
<th data-quarto-table-cell-role="th">Validation Accuracy</th>
<th data-quarto-table-cell-role="th">Validation F1</th>
<th data-quarto-table-cell-role="th">Validation Precision</th>
<th data-quarto-table-cell-role="th">Validation Recall</th>
<th data-quarto-table-cell-role="th">Validation Loss</th>
<th data-quarto-table-cell-role="th">Parameter Count (M)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>mobileone_s3.apple_in1k</td>
<td>0.93</td>
<td>0.95</td>
<td>1.00</td>
<td>1.00</td>
<td>0.26</td>
<td>10.17</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>densenet169.tv_in1k</td>
<td>0.91</td>
<td>0.92</td>
<td>0.88</td>
<td>1.00</td>
<td>0.36</td>
<td>14.15</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>mobilevitv2_150.cvnets_in22k_ft_in1k_384</td>
<td>0.89</td>
<td>0.90</td>
<td>1.00</td>
<td>0.88</td>
<td>0.38</td>
<td>10.59</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>mobilevitv2_150.cvnets_in22k_ft_in1k_384</td>
<td>0.89</td>
<td>0.90</td>
<td>1.00</td>
<td>0.88</td>
<td>0.38</td>
<td>10.59</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>convnext_nano.in12k_ft_in1k</td>
<td>0.76</td>
<td>0.84</td>
<td>0.74</td>
<td>1.00</td>
<td>0.42</td>
<td>15.59</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5</td>
<td>repvit_m3.dist_in1k</td>
<td>0.87</td>
<td>0.86</td>
<td>0.95</td>
<td>1.00</td>
<td>0.42</td>
<td>10.68</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">6</td>
<td>cait_xxs24_224.fb_dist_in1k</td>
<td>0.85</td>
<td>0.88</td>
<td>0.82</td>
<td>1.00</td>
<td>0.44</td>
<td>11.96</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">7</td>
<td>repvgg_a1.rvgg_in1k</td>
<td>0.80</td>
<td>0.86</td>
<td>0.77</td>
<td>1.00</td>
<td>0.45</td>
<td>14.09</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">8</td>
<td>efficientformer_l1.snap_dist_in1k</td>
<td>0.91</td>
<td>0.92</td>
<td>0.86</td>
<td>1.00</td>
<td>0.45</td>
<td>12.29</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">9</td>
<td>hrnet_w18_small.gluon_in1k</td>
<td>0.74</td>
<td>0.83</td>
<td>0.73</td>
<td>1.00</td>
<td>0.46</td>
<td>13.19</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">10</td>
<td>gmlp_s16_224.ra3_in1k</td>
<td>0.74</td>
<td>0.83</td>
<td>0.73</td>
<td>1.00</td>
<td>0.46</td>
<td>19.42</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">11</td>
<td>fastvit_sa12.apple_dist_in1k</td>
<td>0.74</td>
<td>0.83</td>
<td>0.73</td>
<td>1.00</td>
<td>0.48</td>
<td>11.58</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">12</td>
<td>lambda_resnet26t.c1_in1k</td>
<td>0.78</td>
<td>0.84</td>
<td>0.76</td>
<td>1.00</td>
<td>0.49</td>
<td>10.96</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">13</td>
<td>efficientvit_m5.r224_in1k</td>
<td>0.74</td>
<td>0.83</td>
<td>1.00</td>
<td>1.00</td>
<td>0.49</td>
<td>12.47</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">14</td>
<td>eca_resnext26ts.ch_in1k</td>
<td>0.76</td>
<td>0.82</td>
<td>0.75</td>
<td>1.00</td>
<td>0.49</td>
<td>10.30</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">15</td>
<td>ghostnetv2_160.in1k</td>
<td>0.72</td>
<td>0.82</td>
<td>0.72</td>
<td>1.00</td>
<td>0.50</td>
<td>12.39</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">16</td>
<td>nf_regnet_b1.ra2_in1k</td>
<td>0.89</td>
<td>0.91</td>
<td>0.88</td>
<td>1.00</td>
<td>0.50</td>
<td>10.22</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">17</td>
<td>coatnet_nano_rw_224.sw_in1k</td>
<td>0.76</td>
<td>0.84</td>
<td>0.77</td>
<td>1.00</td>
<td>0.50</td>
<td>15.14</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">18</td>
<td>legacy_seresnet18.in1k</td>
<td>0.72</td>
<td>0.82</td>
<td>0.72</td>
<td>1.00</td>
<td>0.51</td>
<td>11.78</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">19</td>
<td>haloregnetz_b.ra3_in1k</td>
<td>0.74</td>
<td>0.83</td>
<td>0.74</td>
<td>1.00</td>
<td>0.51</td>
<td>11.68</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">20</td>
<td>xcit_tiny_24_p8_384.fb_dist_in1k</td>
<td>0.78</td>
<td>0.84</td>
<td>0.77</td>
<td>1.00</td>
<td>0.52</td>
<td>12.11</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">21</td>
<td>resmlp_12_224.fb_distilled_in1k</td>
<td>0.72</td>
<td>0.82</td>
<td>0.72</td>
<td>1.00</td>
<td>0.52</td>
<td>15.35</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">22</td>
<td>gcvit_xxtiny.in1k</td>
<td>0.72</td>
<td>0.82</td>
<td>0.83</td>
<td>1.00</td>
<td>0.53</td>
<td>12.00</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">23</td>
<td>resnest14d.gluon_in1k</td>
<td>0.78</td>
<td>0.84</td>
<td>0.82</td>
<td>1.00</td>
<td>0.53</td>
<td>10.61</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">24</td>
<td>resnest14d.gluon_in1k</td>
<td>0.78</td>
<td>0.84</td>
<td>0.82</td>
<td>1.00</td>
<td>0.53</td>
<td>10.61</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">25</td>
<td>edgenext_base.in21k_ft_in1k</td>
<td>0.72</td>
<td>0.82</td>
<td>0.72</td>
<td>1.00</td>
<td>0.54</td>
<td>18.51</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">26</td>
<td>maxvit_nano_rw_256.sw_in1k</td>
<td>0.78</td>
<td>0.84</td>
<td>0.78</td>
<td>1.00</td>
<td>0.54</td>
<td>15.45</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">27</td>
<td>coat_mini.in1k</td>
<td>0.72</td>
<td>0.82</td>
<td>0.72</td>
<td>1.00</td>
<td>0.58</td>
<td>10.34</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">28</td>
<td>tf_efficientnetv2_b2.in1k</td>
<td>0.80</td>
<td>0.84</td>
<td>1.00</td>
<td>0.90</td>
<td>0.68</td>
<td>10.10</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">29</td>
<td>fbnetv3_d.ra2_in1k</td>
<td>0.65</td>
<td>0.76</td>
<td>0.76</td>
<td>0.82</td>
<td>0.82</td>
<td>10.31</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">30</td>
<td>rexnet_200.nav_in1k</td>
<td>0.70</td>
<td>0.78</td>
<td>0.80</td>
<td>0.82</td>
<td>0.91</td>
<td>16.37</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">31</td>
<td>mixnet_xl.ra_in1k</td>
<td>0.72</td>
<td>0.82</td>
<td>0.76</td>
<td>1.00</td>
<td>1.11</td>
<td>11.90</td>
</tr>
</tbody>
</table>
</div>

</div>
</div>
</div>
<p>As shown in <a href="#tbl-exp-1-best_models">Table&nbsp;2</a>, the top four positions based on the validation metrics were occupied by models belonging to the ConvNeXt family. These models, particularly the first two (convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384 with validation loss 0.19, convnext_large_mlp.clip_laion2b_augreg_ft_in1k with validation loss 0.22), were pre-trained on the extensive LAION-2B dataset and fine-tuned on ImageNet-1k, enabling them to learn complex patterns that generalize well to unseen data. It is plausible that this extensive dataset and sophisticated model architecture played a pivotal role in their exceptional performance in this task. The dominance of the ConvNeXt family is noteworthy, suggesting their effectiveness in handling complex data such as mass spectrometry pseudoimages. Apple’s MobileOne also demonstrated remarkable results (validation loss = 0.27), ranking fourth in terms of validation loss. Finally, MobileViT (validation loss = 0.27), a lightweight network, secured the seventh position. While the ConvNeXt models consistently outperformed other models in terms of validation metrics, it is essential to recognize that their larger size and higher parameter count translate into increased computational demands for training and heightened susceptibility to overfitting. Their complexity demands careful hyperparameter tuning and regularization techniques to mitigate overfitting and achieve optimal performance.</p>
<div class="cell" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    pd.read_csv(<span class="st">"exp-1-best_models.csv"</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    .rename(</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>        columns<span class="op">=</span><span class="kw">lambda</span> df: df.replace(<span class="st">"_"</span>, <span class="st">" "</span>).replace(<span class="st">"val"</span>, <span class="st">"validation"</span>).title()</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    .<span class="bu">round</span>(<span class="dv">2</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    .head(<span class="dv">10</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="3">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>

<div id="tbl-exp-1-best_models" class="anchored">
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<caption>Table&nbsp;2: Top 10 models based on evaluation metrics of all model architectures in families from the PyTorch Image Models library, regardless of their parameter counts.</caption>
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Epoch</th>
<th data-quarto-table-cell-role="th">Experiment</th>
<th data-quarto-table-cell-role="th">Param Count</th>
<th data-quarto-table-cell-role="th">Train Accuracy</th>
<th data-quarto-table-cell-role="th">Train F1</th>
<th data-quarto-table-cell-role="th">Train Loss</th>
<th data-quarto-table-cell-role="th">Train Precision</th>
<th data-quarto-table-cell-role="th">Train Recall</th>
<th data-quarto-table-cell-role="th">Validation Accuracy</th>
<th data-quarto-table-cell-role="th">Validation F1</th>
<th data-quarto-table-cell-role="th">Validation Loss</th>
<th data-quarto-table-cell-role="th">Validation Precision</th>
<th data-quarto-table-cell-role="th">Validation Recall</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>35</td>
<td>convnext_large_mlp.clip_laion2b_augreg_ft_in1k...</td>
<td>200.13</td>
<td>0.92</td>
<td>0.94</td>
<td>0.23</td>
<td>0.94</td>
<td>0.94</td>
<td>0.94</td>
<td>0.95</td>
<td>0.19</td>
<td>0.96</td>
<td>0.93</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>42</td>
<td>convnext_large_mlp.clip_laion2b_augreg_ft_in1k</td>
<td>200.13</td>
<td>0.86</td>
<td>0.90</td>
<td>0.31</td>
<td>0.90</td>
<td>0.89</td>
<td>0.93</td>
<td>0.93</td>
<td>0.22</td>
<td>0.91</td>
<td>0.97</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>41</td>
<td>convnext_large_mlp.clip_laion2b_soup_ft_in12k_...</td>
<td>200.13</td>
<td>0.90</td>
<td>0.93</td>
<td>0.25</td>
<td>0.91</td>
<td>0.96</td>
<td>0.94</td>
<td>0.95</td>
<td>0.22</td>
<td>0.96</td>
<td>0.93</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>42</td>
<td>convnext_large_mlp.clip_laion2b_soup_ft_in12k_...</td>
<td>200.13</td>
<td>0.88</td>
<td>0.92</td>
<td>0.25</td>
<td>0.92</td>
<td>0.92</td>
<td>0.93</td>
<td>0.94</td>
<td>0.23</td>
<td>0.96</td>
<td>0.91</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>39</td>
<td>mobileone_s3.apple_in1k</td>
<td>10.17</td>
<td>0.87</td>
<td>0.92</td>
<td>0.35</td>
<td>0.88</td>
<td>0.96</td>
<td>0.89</td>
<td>0.91</td>
<td>0.27</td>
<td>0.88</td>
<td>0.95</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5</td>
<td>45</td>
<td>convnext_base.clip_laion2b_augreg_ft_in12k_in1...</td>
<td>88.59</td>
<td>0.88</td>
<td>0.92</td>
<td>0.31</td>
<td>0.89</td>
<td>0.95</td>
<td>0.83</td>
<td>0.87</td>
<td>0.27</td>
<td>0.84</td>
<td>0.93</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">6</td>
<td>35</td>
<td>convnextv2_large.fcmae_ft_in22k_in1k_384</td>
<td>197.96</td>
<td>0.86</td>
<td>0.91</td>
<td>0.35</td>
<td>0.87</td>
<td>0.94</td>
<td>0.94</td>
<td>0.96</td>
<td>0.27</td>
<td>0.97</td>
<td>0.95</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">7</td>
<td>49</td>
<td>mobilevitv2_200.cvnets_in22k_ft_in1k_384</td>
<td>18.45</td>
<td>0.91</td>
<td>0.94</td>
<td>0.34</td>
<td>0.95</td>
<td>0.93</td>
<td>0.94</td>
<td>0.95</td>
<td>0.27</td>
<td>1.00</td>
<td>0.91</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">8</td>
<td>44</td>
<td>convnext_large.fb_in22k_ft_in1k_384</td>
<td>197.77</td>
<td>0.86</td>
<td>0.90</td>
<td>0.35</td>
<td>0.87</td>
<td>0.94</td>
<td>0.85</td>
<td>0.88</td>
<td>0.30</td>
<td>0.85</td>
<td>0.91</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">9</td>
<td>37</td>
<td>convnextv2_base.fcmae_ft_in22k_in1k_384</td>
<td>88.72</td>
<td>0.81</td>
<td>0.87</td>
<td>0.37</td>
<td>0.84</td>
<td>0.91</td>
<td>0.91</td>
<td>0.90</td>
<td>0.30</td>
<td>0.96</td>
<td>0.86</td>
</tr>
</tbody>
</table>
</div>

</div>
</div>
</div>
<p>To further assess the suitability of these models and the consistency of their performance, we trained the top three performing models from each family five consecutive times and calculated the median and standard deviation of their validation metrics (<a href="#fig-exp-2-replicates_result">Figure&nbsp;1</a>). This approach allowed us to identify the models that exhibited the most consistent validation performance across multiple training runs. Statistical significance across replicates was evaluated using Dunn’s test with Benjamini-Hochberg correction. The results revealed that the MobileViT2 family performed similarly to the ConvNeXt across all five training runs, except for the validation losses where ConvNeXt exhibited the lowest values with 0.21. According to the replication study, the ConvNeXt emerged as the most effective model, surpassing both MobileOne and MobileViT2. Despite exhibiting similar patterns, ConvNeXt exhibited a statistically significant advantage over MobileViT2 in validation loss (p = 0.01) and achieved significantly better validation recall (p &lt; 0.05). Among the three models tested, MobileOne consistently underperformed its counterparts in all performance metrics, except for validation recall, where it narrowly outperformed MobileViT2 (p &lt; 0.05). In light of these findings, we opted to use the ConvNeXt large model in further experiments.</p>
<div class="cell" data-execution_count="4">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>PIL.Image.<span class="bu">open</span>(<span class="st">"exp-2-replicates_result.png"</span>).convert(<span class="st">"RGB"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="4">
<div id="fig-exp-2-replicates_result" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="DeepLCMS_files/figure-html/fig-exp-2-replicates_result-output-1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;1: Median and standard deviation of best validation metrics achieved during consecutive trainings</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="selecting-image-properties-as-hyperparameters" class="level2">
<h2 class="anchored" data-anchor-id="selecting-image-properties-as-hyperparameters">Selecting image properties as hyperparameters</h2>
<p>Next, we explored the impact of image sharpness and offsets along the x and y axis as a method for image augmentation to replicate the effects of Rt shifts and incidental mass calibration drifts. <a href="#fig-exp-3-image_example">Figure&nbsp;2</a> illustrates how the number of bins employed in the <code>np.histogram2d</code> function within the <code>plot_2D_spectra_overview</code> function—which serves to transform LC/MS files into pseudo images—influences the image sharpness. Here, the bins parameter signifies the number of bins in both the x and y dimensions, which are represented by nx and ny, respectively. This results in the formation of an nx by ny bin grid across the data for the histogram. By increasing the number of bins (from 500, 500 to 1000, 1000), one could obtain a sharper image, but it could also lead to increased noise and computational complexity.</p>
<p>Furthermore, we sought to evaluate the impact of image augmentation, specifically the generation of multiple, modified images prior to image training, on the validation metrics. The <code>augment_images</code> function applies random offsets to the specified image, producing a designated number— 9 in our case—of augmented images with a maximum horizontal offset of five and a maximum vertical offset of five pixels.</p>
<div class="cell" data-execution_count="5">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>img_paths <span class="op">=</span> <span class="bu">list</span>(Path.cwd().glob(<span class="st">"exp_3_U_2*"</span>))</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">9</span>, <span class="dv">9</span>))</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>rows, cols <span class="op">=</span> <span class="dv">2</span>, <span class="dv">2</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx, img_path <span class="kw">in</span> <span class="bu">zip</span>(<span class="bu">range</span>(<span class="dv">1</span>, rows <span class="op">*</span> cols <span class="op">+</span> <span class="dv">1</span>), img_paths):</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    fig.add_subplot(rows, cols, idx)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    img <span class="op">=</span> PIL.Image.<span class="bu">open</span>(img_path).convert(<span class="st">"RGB"</span>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    plt.imshow(img)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    plt.title(img_path.stem[<span class="dv">6</span>:])</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    plt.axis(<span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-exp-3-image_example" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="DeepLCMS_files/figure-html/fig-exp-3-image_example-output-1.png" width="687" height="704" class="figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;2: Examples of images obtained after applying different bin sizes and offsets as imitation of Rt shift. The amount of offset along the x and y axes is given in the image name. Image quality as a function of the number of bins is set to either 500, indicating a lower-quality image, or 1000, indicating a higher-quality image.</figcaption>
</figure>
</div>
</div>
</div>
<p><a href="#fig-exp_3_experiment_result_1">Figure&nbsp;3</a> illustrates, image sharpness in combination with image augmentation had a significant impact on the validation loss. Setting the bin size to 500 and subsequent augmentation resulted in the lowest validation loss (0.129) and the highest validation F1 (0.982). Based on these findings, less image sharpness and more training images, obtained through data augmentation prior to training, had a beneficial effect on the validation metrics. Taken together, the final training and fine-tuning is conducted on images with bin size set to 500 and subsequent augmentation applied resulting in the generation of nine extra samples per training image.</p>
<div class="cell" data-execution_count="6">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>PIL.Image.<span class="bu">open</span>(<span class="st">"exp_3_experiment_result_1.png"</span>).convert(<span class="st">"RGB"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="6">
<div id="fig-exp_3_experiment_result_1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="DeepLCMS_files/figure-html/fig-exp_3_experiment_result_1-output-1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;3: Training and validation metrics of experiments involving the selection of image properties as hyperparameters</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="hyperparameter-tuning-with-optuna" class="level2">
<h2 class="anchored" data-anchor-id="hyperparameter-tuning-with-optuna">Hyperparameter tuning with Optuna</h2>
<p>Optuna, an open-source hyperparameter optimization framework, delivers an automated and efficient approach for optimizing hyperparameters for machine learning and deep learning models. The framework operates by defining an objective function that could be either maximized or minimized. This function encapsulates the model and the metric to be optimized. Within the objective function, hyperparameters are suggested using a trial object. Optuna supports various types of hyperparameters, including float, integer, and categorical. The core of Optuna’s operation lies in its optimization process, which employs state-of-the-art algorithms to search the hyperparameter space efficiently. It can also prunes unpromising trials to conserve computational resources. To optimize the model’s performance, we employed Optuna to identify the most effective optimizer from a pool of four: <code>Adam</code>, <code>AdamW</code>, <code>Adamax</code>, and <code>RMSprop</code>. Additionally, we sought an optimal learning rate scheduler, choosing between <code>CosineAnnealingLR</code> and <code>ReduceLROnPlateau</code>, based on the validation loss achieved during training. After extensive optimization, the combination of <code>Adamax</code> optimizer and <code>CosineAnnealingLR</code> learning rate scheduler yielded the lowest validation loss (0.23), resulting in its selection for subsequent experiments.</p>
</section>
<section id="evaluating-the-final-model-on-the-test-set" class="level2">
<h2 class="anchored" data-anchor-id="evaluating-the-final-model-on-the-test-set">Evaluating the final model on the test set</h2>
<p>The final model training was conducted using the optimal settings determined during the optimization experiments, employing transfer learning of a pre-trained ConvNeXt large model, with all of its layers frozen except for the last classification head. The learning rate was automatically determined to be 0.006 based on the learning rate finder tool implemented by PyTorch Lightning. For the training, we utilized the augmented dataset (<a href="#fig-exp-5-transformed_grid">Figure&nbsp;4</a>) that featured bin size set to 500 for both dimensions. To prevent overfitting and promote better generalization capabilities, random erasing (probability=1.00) and color jitter (probability=0.25) were used. This technique effectively augments the training data, prompting the model to minimize its dependence on specific regions of the image thus preveting the model from memorizing limited patterns and encourages it to generalize better to unseen data. The underlying concept resembles a dropout layer, a common component of neural network architectures. The optimized model achieved validation metrics with <strong>loss</strong> of 0.138, <strong>validation precision</strong> of 0.963, <strong>F1</strong> 0.946, <strong>validation accuracy</strong> exceeding 0.944, and <strong>validation recall</strong> reaching 0.93.</p>
<p>During training, we employed early stopping to prevent overfitting, terminating training if the validation loss fails to improve for 10 consecutive epochs. Additionally, we utilized PyTorch Lightning’s built-in <code>ModelCheckpoint</code> to save the best-performing model, ensuring that the model that achieved the highest validation metrics was preserved and evaluated later on.</p>
<div class="cell" data-execution_count="7">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>PIL.Image.<span class="bu">open</span>(<span class="st">"exp-5-transformed_grid.png"</span>).convert(<span class="st">"RGB"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="7">
<div id="fig-exp-5-transformed_grid" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="DeepLCMS_files/figure-html/fig-exp-5-transformed_grid-output-1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;4: Example of augmented dataloader images used during training time, demonstrating the application of random erasing and color jitter to reduce overfitting</figcaption>
</figure>
</div>
</div>
</div>
<p>After the training was completed, the best-performing model was reloaded, and test metrics were calculated using the 30 test samples present in our dataset. According to the final evaluation, the model produced 20 True Positives, 7 True Negatives, 2 False Negatives, and 1 False Positive.<br>
These results translate to: <strong>Precision</strong> = 0.952, <strong>Recall</strong> = 0.909, <strong>F1</strong> = 0.930, <strong>Accuracy</strong> = 0.90.</p>
<p>While the model achieved impressive validation metrics, the slight drop of performance on the test set compared to the validation metrics indicates potential overfitting. Further optimization of the model, particularly during the method optimization phase, could be achieved through cross-validation strategies. However, this approach might necessitate excessive computational resources.</p>
</section>
<section id="model-interpretability" class="level2">
<h2 class="anchored" data-anchor-id="model-interpretability">Model interpretability</h2>
<section id="estimating-probability" class="level3">
<h3 class="anchored" data-anchor-id="estimating-probability">Estimating probability</h3>
<p>To assess the model’s confidence in its predictions, we can transform the model’s raw output, which is the binary cross-entropy loss derived from a set of logits obtained using PyTorch’s <code>BCEWithLogitsLoss</code> function, into prediction probabilities using a sigmoid function. These converted values reflect the probability that the model assigns to a given prediction belonging to Class 1 (Opioid User). <a href="#fig-exp-5-prediction_matrix">Figure&nbsp;5</a> illustrates this concept, showing that in the first instance, the model estimates a 43% probability of the instance belonging to Class 1, suggesting a higher likelihood of it being a Non-Opioid User (and this prediction proved correct).</p>
<div class="cell" data-execution_count="8">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>PIL.Image.<span class="bu">open</span>(<span class="st">"exp-5-prediction_matrix.png"</span>).convert(<span class="st">"RGB"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="8">
<div id="fig-exp-5-prediction_matrix" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="DeepLCMS_files/figure-html/fig-exp-5-prediction_matrix-output-1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;5: Set of images from the test set, along with their corresponding prediction probabilities for the Class 1 (Opidid User). The true labels are displayed beside the predictions. Green labels indicate successful predictions, whereas red labels denote unsuccessful ones</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="looking-at-layer-class-activation-maps" class="level3">
<h3 class="anchored" data-anchor-id="looking-at-layer-class-activation-maps">Looking at Layer Class Activation Maps</h3>
<p>To gain insights into the regions of an image our model focuses on when making a prediction, we employed LayerCAM, a tool from the TorchCam package. LayerCAM <span class="citation" data-cites="jiang_layercam_2021">(<a href="#ref-jiang_layercam_2021" role="doc-biblioref">Jiang et al. 2021</a>)</span>, short for Layer Class Activation Maps, is a technique for generating class activation maps from distinct layers of a CNN. These regions of interests highlight key features the model utilizes to differentiate between opioid users and non-opioid users. Class activation maps are typically derived from the final convolutional layer of a CNN, highlighting discriminative object regions for the class of interest. Such a tool could be particularly valuable for validating the insights derived from our deep learning model, particularly when compared to more targeted or untargeted metabolomics data.</p>
<div class="cell" data-execution_count="9">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>PIL.Image.<span class="bu">open</span>(<span class="st">"exp-5-plot_activation.png"</span>).convert(<span class="st">"RGB"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="9">
<div id="fig-exp-5-plot_activation" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="DeepLCMS_files/figure-html/fig-exp-5-plot_activation-output-1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;6: Regions of interest identified by LayerCAM, demonstrating the specific areas of the image that our model deems most relevant for making predictions.</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="overlaying-the-untargeted-metabolomics-data-with-the-class-activation-map" class="level3">
<h3 class="anchored" data-anchor-id="overlaying-the-untargeted-metabolomics-data-with-the-class-activation-map">Overlaying the untargeted metabolomics data with the class activation map</h3>
<p>To gain a better understanding of the specific metabolic features that influence the model’s decision-making process in classifying individuals into either group, we analyzed two examples of the pooled samples using MS-DIAL <span class="citation" data-cites="tsugawa_ms-dial_2015">(<a href="#ref-tsugawa_ms-dial_2015" role="doc-biblioref">Tsugawa et al. 2015</a>)</span>, an open-source software pipeline designed for data-independent MS/MS deconvolution and identification of small molecules. Subsequently, metabolite annotation was performed using the MSP database, which houses ESI-MS/MS fragmentation data for 16,481 unique authentic standards. The MSP database served as the basis for assigning compound identification levels: 999 denoted instances where MS-DIAL could not identify the feature due to missing MS/MS fragmentation spectrum or non-matching library entries. A tentative identification was assigned with 530, indicating a match based on accurate mass comparison. A more confident tentative identification was marked with 430, indicating spectral correspondence between the feature and the library entry’s MS/MS spectrum.</p>
<p>To leverage the untargeted metabolomics data, we can, for example, construct a function similar to <code>overlay_untargeted_data</code>, which facilitates the extraction of specific regions from the feature map derived from MS-DIAL based on retention time and accurate mass. These extracted features can be overlaid as a scatter plot alongside our activation map. This approach, in theory, should provide a deeper understanding of the features that play a crucial role in differentiating between the various classes.</p>
<div class="cell" data-execution_count="10">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> overlay_untargeted_data(</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    query_str: <span class="bu">str</span>,</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    ms_dial_location: Path,</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    background_image_location: Path,</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    crop_area: Tuple[<span class="bu">int</span>, <span class="bu">int</span>, <span class="bu">int</span>, <span class="bu">int</span>],</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Overlay untargeted metabolomics data on a background image.</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Parameters:</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="co">    - query_str (str): Query string for filtering the data.</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="co">    - ms_dial_location (str): Location of the mass spectrometry data file.</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a><span class="co">    - background_image_location (str): Location of the background image.</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a><span class="co">    - crop_area (tuple): Coordinates for cropping the background image (left, upper, right, lower).</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a><span class="co">    - composite_img (PIL.Image.Image): Overlay of data on the background image.</span></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a><span class="co">    - df (pd.DataFrame): Filtered and processed DataFrame.</span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Read and preprocess the mass spectrometry data</span></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> (</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>        pd.read_csv(</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>            ms_dial_location,</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>            skiprows<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>            usecols<span class="op">=</span>[</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>                <span class="st">"Average Rt(min)"</span>,</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>                <span class="st">"Average Mz"</span>,</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>                <span class="st">"Metabolite name"</span>,</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>                <span class="st">"Ontology"</span>,</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>                <span class="st">"Annotation tag (VS1.0)"</span>,</span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>                <span class="st">"MS/MS matched"</span>,</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>                <span class="st">"SP_10"</span>,</span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>                <span class="st">"SP_20"</span>,</span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>            ],</span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a>        .rename(columns<span class="op">=</span><span class="kw">lambda</span> x: re.sub(<span class="vs">r"\W|[%/]"</span>, <span class="st">"_"</span>, x.lower()))</span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a>        .query(<span class="st">"annotation_tag__vs1_0_ == '430' or annotation_tag__vs1_0_ == '530'"</span>)</span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a>        .query(query_str)</span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a>        .drop_duplicates(subset<span class="op">=</span><span class="st">"metabolite_name"</span>)</span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create scatterplot</span></span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a>    sns.scatterplot(</span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a>        data<span class="op">=</span>df,</span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a>        x<span class="op">=</span><span class="st">"average_rt_min_"</span>,</span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a>        y<span class="op">=</span><span class="st">"average_mz"</span>,</span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a>        alpha<span class="op">=</span><span class="fl">0.5</span>,</span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a>        s<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a>        color<span class="op">=</span><span class="st">"black"</span>,</span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb10-51"><a href="#cb10-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-52"><a href="#cb10-52" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Set plot limits and turn off axis</span></span>
<span id="cb10-53"><a href="#cb10-53" aria-hidden="true" tabindex="-1"></a>    plt.ylim(<span class="dv">50</span>, <span class="dv">750</span>)</span>
<span id="cb10-54"><a href="#cb10-54" aria-hidden="true" tabindex="-1"></a>    plt.xlim(<span class="dv">0</span>, <span class="dv">20</span>)</span>
<span id="cb10-55"><a href="#cb10-55" aria-hidden="true" tabindex="-1"></a>    plt.axis(<span class="va">False</span>)</span>
<span id="cb10-56"><a href="#cb10-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-57"><a href="#cb10-57" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create the foreground image</span></span>
<span id="cb10-58"><a href="#cb10-58" aria-hidden="true" tabindex="-1"></a>    buf <span class="op">=</span> io.BytesIO()</span>
<span id="cb10-59"><a href="#cb10-59" aria-hidden="true" tabindex="-1"></a>    plt.savefig(buf, <span class="bu">format</span><span class="op">=</span><span class="st">"jpeg"</span>, bbox_inches<span class="op">=</span><span class="st">"tight"</span>, pad_inches<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb10-60"><a href="#cb10-60" aria-hidden="true" tabindex="-1"></a>    buf.seek(<span class="dv">0</span>)</span>
<span id="cb10-61"><a href="#cb10-61" aria-hidden="true" tabindex="-1"></a>    foreground <span class="op">=</span> PIL.Image.<span class="bu">open</span>(buf)</span>
<span id="cb10-62"><a href="#cb10-62" aria-hidden="true" tabindex="-1"></a>    plt.close()</span>
<span id="cb10-63"><a href="#cb10-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-64"><a href="#cb10-64" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create the background image</span></span>
<span id="cb10-65"><a href="#cb10-65" aria-hidden="true" tabindex="-1"></a>    background <span class="op">=</span> (</span>
<span id="cb10-66"><a href="#cb10-66" aria-hidden="true" tabindex="-1"></a>        PIL.Image.<span class="bu">open</span>(background_image_location)</span>
<span id="cb10-67"><a href="#cb10-67" aria-hidden="true" tabindex="-1"></a>        .convert(<span class="st">"RGB"</span>)</span>
<span id="cb10-68"><a href="#cb10-68" aria-hidden="true" tabindex="-1"></a>        .crop(crop_area)</span>
<span id="cb10-69"><a href="#cb10-69" aria-hidden="true" tabindex="-1"></a>        .resize(foreground.size)</span>
<span id="cb10-70"><a href="#cb10-70" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb10-71"><a href="#cb10-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-72"><a href="#cb10-72" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a mask</span></span>
<span id="cb10-73"><a href="#cb10-73" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> PIL.Image.new(<span class="st">"L"</span>, foreground.size, <span class="dv">128</span>)</span>
<span id="cb10-74"><a href="#cb10-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-75"><a href="#cb10-75" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Composite the images</span></span>
<span id="cb10-76"><a href="#cb10-76" aria-hidden="true" tabindex="-1"></a>    composite_img <span class="op">=</span> PIL.Image.composite(background, foreground, mask)</span>
<span id="cb10-77"><a href="#cb10-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-78"><a href="#cb10-78" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> composite_img, df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>As illustrated in <a href="#fig-overlay_example1">Figure&nbsp;7</a>, and <a href="#fig-overlay_example2">Figure&nbsp;8</a>, the most notable features identified by MS-DIAL with levels 430 and 530 are overlaid with two examples of the activation map generated by TorchCam. Importantly, morphine-3-glucuronide (Level 530), a major metabolite of morphine, stands out as a key feature influencing the model’s decision-making process. Additionally, EDDP (Level 430), a major metabolite of methadone, a synthetic opioid agonist used for chronic pain and opioid use disorder, is also considered by the model. While we observed tentative evidence suggesting that the metabolic pathways involving Kreb’s cycle, purine metabolism, central carbon metabolism, histone modification, and acetylation are also considered by the model, as it was pointed out by <span class="citation" data-cites="li_untargeted_2020">Li et al. (<a href="#ref-li_untargeted_2020" role="doc-biblioref">2020</a>)</span>, the level of identification (530) indicates that these identifications are based solely on accurate mass measurement alone thus further investigation is warranted to confirm the identities of these potential biomarkers (<a href="#tbl-overlaid-identifications">Table&nbsp;3</a>).</p>
<div class="cell" data-execution_count="11">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>composite_img_pattern_1, pattern_df_1 <span class="op">=</span> overlay_untargeted_data(</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    query_str<span class="op">=</span><span class="st">"3.5&lt;average_rt_min_&lt;5 and 460&lt;average_mz&lt;500 or 5&lt;average_rt_min_&lt;6.5 and 575&lt;average_mz&lt;650 or 9.5&lt;average_rt_min_&lt;10 and 280&lt;average_mz&lt;320"</span>,</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    ms_dial_location<span class="op">=</span><span class="st">"PeakID_0_2023_12_25_17_39_09.csv"</span>,</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    background_image_location<span class="op">=</span><span class="st">"exp-5-plot_activation.png"</span>,</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    crop_area<span class="op">=</span>(<span class="dv">25</span>, <span class="dv">90</span>, <span class="dv">850</span>, <span class="dv">910</span>),</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>font <span class="op">=</span> ImageFont.truetype(<span class="st">"arial.ttf"</span>, <span class="dv">16</span>)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a Draw object</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>draw <span class="op">=</span> ImageDraw.Draw(composite_img_pattern_1)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw the texts at different locations</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>draw.text((<span class="dv">25</span>, <span class="dv">100</span>), <span class="st">"Morphine-3-Glucuronide"</span>, (<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>), font<span class="op">=</span>font)</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>draw.text((<span class="dv">100</span>, <span class="dv">200</span>), <span class="st">"EDDP, Octanoyl-L-Carnitine, Aspirine"</span>, (<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>), font<span class="op">=</span>font)</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>composite_img_pattern_1</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="11">
<div id="fig-overlay_example1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="DeepLCMS_files/figure-html/fig-overlay_example1-output-1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;7: Example-1 of interest overlayed with proposed feature names tentativelyi identified by MS-DIAL.</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell" data-execution_count="12">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>composite_img_pattern_2, pattern_df_2 <span class="op">=</span> overlay_untargeted_data(</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    query_str<span class="op">=</span><span class="st">"(16.5&lt;average_rt_min_&lt;18 and 180&lt;average_mz&lt;250) or (4&lt;average_rt_min_&lt;5.5 and 50&lt;average_mz&lt;100) or (8&lt;average_rt_min_&lt;12 and 50&lt;average_mz&lt;100) or (8.5&lt;average_rt_min_&lt;9 and 250&lt;average_mz&lt;300) or (12&lt;average_rt_min_&lt;14 and 200&lt;average_mz&lt;220) or (18&lt;average_rt_min_&lt;20 and 280&lt;average_mz&lt;350)"</span>,</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    ms_dial_location<span class="op">=</span><span class="st">"PeakID_0_2023_12_25_17_39_09.csv"</span>,</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    background_image_location<span class="op">=</span><span class="st">"exp-5-plot_activation.png"</span>,</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    crop_area<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">1070</span>, <span class="dv">850</span>, <span class="dv">1890</span>),</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>ImageDraw.Draw(composite_img_pattern_2).text(</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">100</span>, <span class="dv">210</span>), <span class="st">"EDDP, Octanoyl-L-Carnitine, Aspirine"</span>, (<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>), font<span class="op">=</span>font</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>composite_img_pattern_2</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="12">
<div id="fig-overlay_example2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="DeepLCMS_files/figure-html/fig-overlay_example2-output-1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;8: Example-2 of interest overlayed with proposed feature names tentativelyi identified by MS-DIAL.</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell" data-execution_count="13">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>pd.concat([pattern_df_1, pattern_df_2], axis<span class="op">=</span><span class="dv">0</span>).drop(columns<span class="op">=</span>[<span class="st">"sp_10"</span>, <span class="st">"sp_20"</span>]).head(</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    <span class="dv">50</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="13">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>

<div id="tbl-overlaid-identifications" class="anchored">
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<caption>Table&nbsp;3: List of 50 metabolites residing at regions of interests that were tentatively identified by MS-DIAL following the overlay of the activation map and the feature map</caption>
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">average_rt_min_</th>
<th data-quarto-table-cell-role="th">average_mz</th>
<th data-quarto-table-cell-role="th">metabolite_name</th>
<th data-quarto-table-cell-role="th">ontology</th>
<th data-quarto-table-cell-role="th">annotation_tag__vs1_0_</th>
<th data-quarto-table-cell-role="th">ms_ms_matched</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">3076</td>
<td>9.998</td>
<td>283.24149</td>
<td>w/o MS2: 2-(N-butylacetamido)-N-cyclohexylbuta...</td>
<td>Tertiary carboxylic acid amides</td>
<td>530</td>
<td>False</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3297</td>
<td>9.938</td>
<td>297.06689</td>
<td>w/o MS2: clausenin methyl ether</td>
<td>Linear pyranocoumarins</td>
<td>530</td>
<td>False</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">3379</td>
<td>9.938</td>
<td>301.06268</td>
<td>w/o MS2: 3(2'-Chlorophenyl)-7-ethoxycoumarin</td>
<td>Isoflav-3-enones</td>
<td>530</td>
<td>False</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3459</td>
<td>9.599</td>
<td>305.09894</td>
<td>heraclenol</td>
<td>Psoralens</td>
<td>430</td>
<td>True</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">3510</td>
<td>9.917</td>
<td>308.22070</td>
<td>w/o MS2: Dihydrocapsaicin</td>
<td>Methoxyphenols</td>
<td>530</td>
<td>False</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3571</td>
<td>9.543</td>
<td>312.15945</td>
<td>w/o MS2: atropine</td>
<td>Tropane alkaloids</td>
<td>530</td>
<td>False</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">3587</td>
<td>9.622</td>
<td>313.06686</td>
<td>w/o MS2: (-)Catechin</td>
<td>Catechins</td>
<td>530</td>
<td>False</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5424</td>
<td>3.893</td>
<td>460.15945</td>
<td>w/o MS2: Villol</td>
<td>Rotenones</td>
<td>530</td>
<td>False</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">5425</td>
<td>4.167</td>
<td>460.19565</td>
<td>w/o MS2: Apixaban</td>
<td>Phenylpiperidines</td>
<td>530</td>
<td>False</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5444</td>
<td>4.369</td>
<td>462.17502</td>
<td>w/o MS2: Morphine_3_Glucuronide</td>
<td>Morphinans</td>
<td>530</td>
<td>False</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">5557</td>
<td>4.553</td>
<td>474.17270</td>
<td>w/o MS2: Raloxifene</td>
<td>Aryl-phenylketones</td>
<td>530</td>
<td>False</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5611</td>
<td>3.744</td>
<td>480.09711</td>
<td>w/o MS2: FOLIC ACID</td>
<td>Glutamic acid and derivatives</td>
<td>530</td>
<td>False</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">5655</td>
<td>4.720</td>
<td>486.19623</td>
<td>w/o MS2: Smenathiazole B</td>
<td>N-acyl-alpha amino acids and derivatives</td>
<td>530</td>
<td>False</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5668</td>
<td>4.802</td>
<td>488.15448</td>
<td>w/o MS2: Dasatinib (BMS-354825)</td>
<td>Aromatic anilides</td>
<td>530</td>
<td>False</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">5686</td>
<td>4.539</td>
<td>490.07681</td>
<td>w/o MS2: Vemurafenib (PLX4032)</td>
<td>Aryl-phenylketones</td>
<td>530</td>
<td>False</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5709</td>
<td>4.486</td>
<td>492.24979</td>
<td>w/o MS2: Hypaconine</td>
<td>Aconitane-type diterpenoid alkaloids</td>
<td>530</td>
<td>False</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">5731</td>
<td>4.002</td>
<td>496.15677</td>
<td>w/o MS2: NCGC00380381-01_C24H27NO9_</td>
<td>Saccharolipids</td>
<td>530</td>
<td>False</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5743</td>
<td>4.409</td>
<td>498.17252</td>
<td>w/o MS2: Tebipenem pivoxil (L-084)</td>
<td>Thienamycins</td>
<td>530</td>
<td>False</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">6249</td>
<td>5.595</td>
<td>591.15735</td>
<td>w/o MS2: Mulberroside A</td>
<td>Stilbene glycosides</td>
<td>530</td>
<td>False</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">6264</td>
<td>5.595</td>
<td>595.13904</td>
<td>w/o MS2: Tiliroside</td>
<td>Flavonoid 3-O-p-coumaroyl glycosides</td>
<td>530</td>
<td>False</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">6292</td>
<td>5.613</td>
<td>604.17505</td>
<td>w/o MS2: SSR126768</td>
<td>Dimethoxybenzenes</td>
<td>530</td>
<td>False</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">6294</td>
<td>5.595</td>
<td>605.17346</td>
<td>w/o MS2: Naringin dihydrochalcone</td>
<td>Flavonoid O-glycosides</td>
<td>530</td>
<td>False</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">6359</td>
<td>6.428</td>
<td>630.22260</td>
<td>w/o MS2: NCGC00347814-02_C29H37NO13_</td>
<td>Benzylisoquinolines</td>
<td>530</td>
<td>False</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">72</td>
<td>4.042</td>
<td>72.08070</td>
<td>w/o MS2: Pyrrolidine; CE10; RWRDLPDLKQPQOW-UHF...</td>
<td>Pyrrolidines</td>
<td>530</td>
<td>False</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">75</td>
<td>4.985</td>
<td>72.08073</td>
<td>w/o MS2: Pyrrolidine; CE0; RWRDLPDLKQPQOW-UHFF...</td>
<td>Pyrrolidines</td>
<td>530</td>
<td>False</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">78</td>
<td>4.943</td>
<td>73.06469</td>
<td>w/o MS2: BUTANAL</td>
<td>Alpha-hydrogen aldehydes</td>
<td>530</td>
<td>False</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">94</td>
<td>5.348</td>
<td>76.03080</td>
<td>w/o MS2: Glycine</td>
<td>Alpha amino acids</td>
<td>530</td>
<td>False</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">104</td>
<td>4.844</td>
<td>78.03378</td>
<td>2-Aminoethanethiol</td>
<td>Alkylthiols</td>
<td>430</td>
<td>True</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">173</td>
<td>4.943</td>
<td>89.05956</td>
<td>w/o MS2: Isobutyric acid</td>
<td>Carboxylic acids</td>
<td>530</td>
<td>False</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">218</td>
<td>5.358</td>
<td>95.04900</td>
<td>w/o MS2: Phenol</td>
<td>1-hydroxy-4-unsubstituted benzenoids</td>
<td>530</td>
<td>False</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">222</td>
<td>4.071</td>
<td>95.06035</td>
<td>w/o MS2: 4-Methylpyrimidine</td>
<td>Pyrimidines and pyrimidine derivatives</td>
<td>530</td>
<td>False</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">226</td>
<td>4.844</td>
<td>96.04424</td>
<td>3-Hydroxypyridine</td>
<td>Hydroxypyridines</td>
<td>430</td>
<td>True</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">252</td>
<td>8.776</td>
<td>98.98405</td>
<td>w/o MS2: Ortophosphate</td>
<td>Non-metal phosphates</td>
<td>530</td>
<td>False</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">253</td>
<td>4.658</td>
<td>99.04393</td>
<td>w/o MS2: alpha-Methylene-gamma-butyrolactone</td>
<td>Gamma butyrolactones</td>
<td>530</td>
<td>False</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">1393</td>
<td>16.813</td>
<td>181.04948</td>
<td>Aspirin</td>
<td>Acylsalicylic acids</td>
<td>430</td>
<td>True</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1566</td>
<td>17.420</td>
<td>191.98239</td>
<td>w/o MS2: 2-Chlorobenzenesulfonamide</td>
<td>Benzenesulfonamides</td>
<td>530</td>
<td>False</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">1613</td>
<td>17.092</td>
<td>194.97618</td>
<td>w/o MS2: Orotic Acid</td>
<td>Pyrimidinecarboxylic acids</td>
<td>530</td>
<td>False</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1689</td>
<td>17.184</td>
<td>198.99387</td>
<td>w/o MS2: Tioconazole</td>
<td>Benzylethers</td>
<td>530</td>
<td>False</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">1690</td>
<td>17.155</td>
<td>199.01105</td>
<td>w/o MS2: PUTRESCINE DIHYDROCHLORIDE</td>
<td>Monoalkylamines</td>
<td>530</td>
<td>False</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1701</td>
<td>17.223</td>
<td>199.13049</td>
<td>w/o MS2: Tacrine</td>
<td>Acridines</td>
<td>530</td>
<td>False</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">1720</td>
<td>12.290</td>
<td>200.23743</td>
<td>w/o MS2: N-Methyldodecylamine</td>
<td>Dialkylamines</td>
<td>530</td>
<td>False</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1784</td>
<td>12.903</td>
<td>204.08629</td>
<td>w/o MS2: alpha-oxo-1h-indole-3-propanoic acid</td>
<td>Indolyl carboxylic acids and derivatives</td>
<td>530</td>
<td>False</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">1802</td>
<td>17.464</td>
<td>205.00497</td>
<td>w/o MS2: Tilt(TM)</td>
<td>Dichlorobenzenes</td>
<td>530</td>
<td>False</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1862</td>
<td>17.487</td>
<td>209.00053</td>
<td>w/o MS2: Kynurenine</td>
<td>Alkyl-phenylketones</td>
<td>530</td>
<td>False</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">1872</td>
<td>12.139</td>
<td>209.11678</td>
<td>w/o MS2: Primin</td>
<td>P-benzoquinones</td>
<td>530</td>
<td>False</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1897</td>
<td>17.206</td>
<td>210.97739</td>
<td>w/o MS2: AZELAIC ACID</td>
<td>Medium-chain fatty acids</td>
<td>530</td>
<td>False</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">1923</td>
<td>16.999</td>
<td>212.97305</td>
<td>w/o MS2: Aconitic Acid</td>
<td>Tricarboxylic acids and derivatives</td>
<td>530</td>
<td>False</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">2034</td>
<td>13.514</td>
<td>219.19563</td>
<td>w/o MS2: Tributylphosphine oxide</td>
<td>Organophosphine oxides</td>
<td>530</td>
<td>False</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2107</td>
<td>17.939</td>
<td>224.99295</td>
<td>w/o MS2: 3-Hydroxykynurenine</td>
<td>Alkyl-phenylketones</td>
<td>530</td>
<td>False</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">2119</td>
<td>17.230</td>
<td>225.99634</td>
<td>w/o MS2: Cyprodinil</td>
<td>Aniline and substituted anilines</td>
<td>530</td>
<td>False</td>
</tr>
</tbody>
</table>
</div>

</div>
</div>
</div>
<p>Although this is not the only approach to understanding the principles upon which the model bases its decisions, we believe that combining CNNs with MS/MS fragmentation-based untargeted metabolomics can be used as an example to gain a better understanding of the underlying mechanisms. Conversely, the model can also guide researchers in directing resources and efforts towards identifying specific regions of interest more effectively by performing more targeted measurements, such as MS/MS experiments, since we already know which features play significant roles in distinguishing between the classes. In particular, we recognize the potential benefits of such a model in high-throughput settings where rapid decisions are required, although thorough validation remains essential. This approach could be highly valuable in conjunction with techniques such as untargeted metabolomics and lipidomics, where an unbiased assessment of the phenotype is desirable, especially, when combined with simple sample preparation techniques such as protein precipitation.</p>
<p>While the use of CNNs in high-throughput settings holds immense potential, it is crucial to acknowledge potential drawbacks. Further research is essential to understand how image preprocessing techniques can influence the decision-making process. Currently, we only implemented basic techniques, such as scaling, Gaussian noise reduction, and logarithmic transformation. The number of training examples could also pose a challenge. While transfer learning, as we demonstrated, does not demand a plethora of training examples, we still required over 200 samples for training. Nonetheless, once sufficient number of images are obtained for the initial training, one can retrain the model as new data becomes available. Additionally, it is essential to exercise caution against overfitting. While we implemented several strategies to mitigate overfitting and enhance generalizability, our testing metrics indicate a slight overfitting compared to the validation metrics. However, employing cross-validation may help address this issue. Finally, GPU availability is crucial for neural network training. While transfer learning does not require as much computational resources as training a network from scratch, utilizing a GPU is still recommended. Hence, having access to a GPU is highly advised. This project utilized the freely available GPU (NVIDIA Tesla T4) as part of the Google Colab. Once experimentation is complete, depending on the size of the model selected, inference may be able to be performed using a CPU.</p>
</section>
</section>
</section>
<section id="conclusions" class="level1">
<h1>Conclusions</h1>
<p>This project serves as a proof-of-concept demonstrating the application of transfer learning in a binary classification task using untargeted LC/HRMS metabolomics data. We explored various CNN architectures and optimized hyperparameters, such as optimizer type and learning rate scheduler, to achieve optimal performance. Additionally, we utilized TorchCam to visualize the regions of the pseudoimage the neural network focuses on for distinguishing between opioid users and non-users. Furthermore, we incorporated traditional techniques, including spectral deconvolution and MS/MS fragmentation pattern-based identification, to enhance our understanding of the CNN’s decision-making process. We hope this open-source project could serve as a valuable guideline for researchers seeking to develop CNN based applications following rigorous validation. We could also envision a system, for example, where positive instances identified by the initial network could be further evaluated by a second one capable of estimating quantities using a regression model with mean squared error loss. Overall, this project aims to demonstrate the practical application of untargeted metabolomics data in combination with modern techniques such as computer vision and neural networks. We believe that the primary obstacle hindering the widespread adoption of metabolomics and lipidomics is not sample preparation or instrumentation but the human expertise required for data interpretation and analysis. We hope that projects like DeepLCMS encourage researchers to embrace novel resources and empower a broader audience to benefit from recent technological advancements.</p>
</section>
<section id="get-in-touch" class="level1">
<h1>Get in touch</h1>
<p>Did this project help with your research? Do you have any ideas for making it better? Get in touch! We would love to hear from you.</p>


<!-- -->


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-behrmann_deep_2018" class="csl-entry" role="listitem">
Behrmann, Jens, Christian Etmann, Tobias Boskamp, Rita Casadonte, Jörg Kriegsmann, and Peter Maaβ. 2018. <span>“Deep Learning for Tumor Classification in Imaging Mass Spectrometry.”</span> <em>Bioinformatics</em> 34 (7): 1215–23. <a href="https://doi.org/10.1093/bioinformatics/btx724">https://doi.org/10.1093/bioinformatics/btx724</a>.
</div>
<div id="ref-cadow_feasibility_2021" class="csl-entry" role="listitem">
Cadow, Joris, Matteo Manica, Roland Mathis, Roger R Reddel, Phillip J Robinson, Peter J Wild, Peter G Hains, et al. 2021. <span>“On the Feasibility of Deep Learning Applications Using Raw Mass Spectrometry Data.”</span> <em>Bioinformatics</em> 37 (Suppl 1): i245–53. <a href="https://doi.org/10.1093/bioinformatics/btab311">https://doi.org/10.1093/bioinformatics/btab311</a>.
</div>
<div id="ref-dobson_birth_2023" class="csl-entry" role="listitem">
Dobson, James E. 2023. <em>The <span>Birth</span> of <span>Computer</span> <span>Vision</span></em>. U of Minnesota Press.
</div>
<div id="ref-ghanbari_metabolomics_2021" class="csl-entry" role="listitem">
Ghanbari, Reza, Yuanyuan Li, Wimal Pathmasiri, Susan McRitchie, Arash Etemadi, Jonathan D. Pollock, Hossein Poustchi, et al. 2021. <span>“Metabolomics Reveals Biomarkers of Opioid Use Disorder.”</span> <em>Translational Psychiatry</em> 11 (February): 103. <a href="https://doi.org/10.1038/s41398-021-01228-7">https://doi.org/10.1038/s41398-021-01228-7</a>.
</div>
<div id="ref-jiang_layercam_2021" class="csl-entry" role="listitem">
Jiang, Peng-Tao, Chang-Bin Zhang, Qibin Hou, Ming-Ming Cheng, and Yunchao Wei. 2021. <span>“<span>LayerCAM</span>: <span>Exploring</span> <span>Hierarchical</span> <span>Class</span> <span>Activation</span> <span>Maps</span> for <span>Localization</span>.”</span> <em>IEEE Transactions on Image Processing</em> 30: 5875–88. <a href="https://doi.org/10.1109/TIP.2021.3089943">https://doi.org/10.1109/TIP.2021.3089943</a>.
</div>
<div id="ref-li_untargeted_2020" class="csl-entry" role="listitem">
Li, Yuan-Yuan, Reza Ghanbari, Wimal Pathmasiri, Susan McRitchie, Hossein Poustchi, Amaneh Shayanrad, Gholamreza Roshandel, et al. 2020. <span>“Untargeted <span>Metabolomics</span>: <span>Biochemical</span> <span>Perturbations</span> in <span>Golestan</span> <span>Cohort</span> <span>Study</span> <span>Opium</span> <span>Users</span> <span>Inform</span> <span>Intervention</span> <span>Strategies</span>.”</span> <em>Frontiers in Nutrition</em> 7 (December): 584585. <a href="https://doi.org/10.3389/fnut.2020.584585">https://doi.org/10.3389/fnut.2020.584585</a>.
</div>
<div id="ref-pourshams_cohort_2010" class="csl-entry" role="listitem">
Pourshams, Akram, Hooman Khademi, Akbar Fazeltabar Malekshah, Farhad Islami, Mehdi Nouraei, Ali Reza Sadjadi, Elham Jafari, et al. 2010. <span>“Cohort <span>Profile</span>: <span>The</span> <span>Golestan</span> <span>Cohort</span> <span>Study</span>—a Prospective Study of Oesophageal Cancer in Northern <span>Iran</span>.”</span> <em>International Journal of Epidemiology</em> 39 (1): 52–59. <a href="https://doi.org/10.1093/ije/dyp161">https://doi.org/10.1093/ije/dyp161</a>.
</div>
<div id="ref-seddiki_towards_2020" class="csl-entry" role="listitem">
Seddiki, Khawla, Philippe Saudemont, Frédéric Precioso, Nina Ogrinc, Maxence Wisztorski, Michel Salzet, Isabelle Fournier, and Arnaud Droit. 2020. <span>“Towards <span>CNN</span> <span>Representations</span> for <span>Small</span> <span>Mass</span> <span>Spectrometry</span> <span>Data</span> <span>Classification</span>: <span>From</span> <span>Transfer</span> <span>Learning</span> to <span>Cumulative</span> <span>Learning</span>.”</span> Preprint. Bioinformatics. <a href="https://doi.org/10.1101/2020.03.24.005975">https://doi.org/10.1101/2020.03.24.005975</a>.
</div>
<div id="ref-shen_deep_2022" class="csl-entry" role="listitem">
Shen, Xiaotao, Wei Shao, Chuchu Wang, Liang Liang, Songjie Chen, Sai Zhang, Mirabela Rusu, and Michael P. Snyder. 2022. <span>“Deep Learning-Based Pseudo-Mass Spectrometry Imaging Analysis for Precision Medicine.”</span> <em>Briefings in Bioinformatics</em> 23 (5): bbac331. <a href="https://doi.org/10.1093/bib/bbac331">https://doi.org/10.1093/bib/bbac331</a>.
</div>
<div id="ref-tsugawa_ms-dial_2015" class="csl-entry" role="listitem">
Tsugawa, Hiroshi, Tomas Cajka, Tobias Kind, Yan Ma, Brendan Higgins, Kazutaka Ikeda, Mitsuhiro Kanazawa, Jean VanderGheynst, Oliver Fiehn, and Masanori Arita. 2015. <span>“<span>MS</span>-<span>DIAL</span>: Data-Independent <span>MS</span>/<span>MS</span> Deconvolution for Comprehensive Metabolome Analysis.”</span> <em>Nature Methods</em> 12 (6): 523–26. <a href="https://doi.org/10.1038/nmeth.3393">https://doi.org/10.1038/nmeth.3393</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb14" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> 'DeepLCMS: A framework that leverages transfer learning for the classification of pseudo images in mass spectrometry-based analysis'</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> Adam Cseresznye</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> '2023-12-23'</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="an">bibliography:</span><span class="co"> DeepLCMS.bib</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="co">  - DeepLCMS</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span><span class="co"> python3</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="an">toc:</span><span class="co"> true</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: true</span></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a><span class="al">![Representative examples of DeepLCMS predictions accompanied by their corresponding probability estimates.](exp-5-prediction_matrix.png)</span>{fig-align="center" width=100%}</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>Welcome to DeepLCMS, a project that combines mass spectrometry analysis with the power of deep learning models!</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>Unlike conventional methods, DeepLCMS streamlines the data processing pipeline, bypassing the need for time-consuming and expertise-dependent steps such as peak alignment, data annotation and quantitation. Instead, it relies on the power of deep learning by recognizing important features to directly classify mass spectrometry-based pseudo-images with high accuracy. To demonstrate the capabilities of pre-trained neural networks for high-resolution LC/MS data, we successfully apply our convolutional neural network (CNN) to categorize substance abuse cases. We utilize the openly available Golestan Cohort Study's metabolomics LC/MS data to train and evaluate our CNN <span class="co">[</span><span class="ot">@pourshams_cohort_2010; @ghanbari_metabolomics_2021; @li_untargeted_2020</span><span class="co">]</span>. We also go beyond classification by providing insights into the network's decision-making process using the TorchCam library. TorchCam allows us to analyze the regions of interests that influence the network's classifications, helping us identify key compound classes that play a crucial role in differentiating between them. This approach empowers us to unlock novel insights into the underlying molecular classes and potential pathways that drive phenotypic variations much faster compared to traditional methods.</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>We believe that applying cutting-edge CNN-based technologies, such as DeepLCMS, for non-discriminatory mass spectrometry analysis has the potential to significantly enhance the field of mass spectrometry analysis, enabling faster, more streamlined, and more informative data interpretation, particularly in domains that demand high-throughput and rapid decision-making processes. Its capacity to directly classify pseudo-images without extensive preprocessing could unlock a wealth of opportunities for researchers and clinicians alike.</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>::: {.callout-note appearance="simple"}</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>DeepLCMS is an open-source project, freely available on <span class="co">[</span><span class="ot">GitHub</span><span class="co">](https://github.com/adamcseresznye/DeepLCMS)</span>, that aims to provide researchers with a reproducible code-template for leveraging deep learning for mass spectrometry data analysis.</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>In contrast to traditional methods that involve laborious steps like peak alignment, data annotation, and quantitation, DeepLCMS delivers results significantly faster by approaching LC/MS problems as a computer vision task. This novel approach uses a neuronal network to directly learn from the patterns inherent in the sample in an unbiased way without the need for manual intervention, accelerating the entire workflow.</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>The study stands out from previous research by conducting a *comprehensive evaluation of diverse architecture families*, including cutting-edge architectures like vision transformers. Additionally, it employs *basic hyperparameter tuning* to optimize key parameters such as the optimizer and learning rate scheduler. Furthermore, it examines the *impact of image pretreatment* on validation metrics, exploring image sharpness and data augmentation techniques that mimic retention time shift. To enhance model generalization, this study takes advantage of *regularization techniques* like random-tilting images and random erasing during training. Finally, it also explores model interpretability by delving into the decision-making process of the pre-trained network, employing TorchVision for a comprehensive analysis.</span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a><span class="fu"># Importing libraries</span></span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> io</span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> PIL</span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> ImageDraw, ImageFont</span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Tuple</span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-48"><a href="#cb14-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-49"><a href="#cb14-49" aria-hidden="true" tabindex="-1"></a><span class="fu"># Introduction</span></span>
<span id="cb14-50"><a href="#cb14-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-51"><a href="#cb14-51" aria-hidden="true" tabindex="-1"></a>While computer vision has gained widespread adoption in various aspects of our lives<span class="co">[</span><span class="ot">@dobson_birth_2023</span><span class="co">]</span>, its application in medical imaging and biosciences has lagged behind, primarily due to limitations in clinical dataset size, accessibility, privacy concerns, experimental complexity, and high acquisition costs. For such applications, transfer learning has emerged as a potential solution<span class="co">[</span><span class="ot">@seddiki_towards_2020</span><span class="co">]</span>. A technique that is particularly effective with small datasets, requiring fewer computational resources while achieving good classification accuracy compared to models trained from scratch. Transfer learning involves a two-step process. Initially, a robust data representation is learned by training a model on a dataset comprising a vast amount of annotated data encompassing numerous categories (<span class="co">[</span><span class="ot">ImageNet</span><span class="co">](https://www.image-net.org/)</span> for example). This representation is then used to construct a new model based on a smaller annotated dataset containing fewer categories. </span>
<span id="cb14-52"><a href="#cb14-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-53"><a href="#cb14-53" aria-hidden="true" tabindex="-1"></a><span class="fu">## Application of Pretrained Neural Networks for Mass Spectrometry Data</span></span>
<span id="cb14-54"><a href="#cb14-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-55"><a href="#cb14-55" aria-hidden="true" tabindex="-1"></a>The use of pre-trained neural networks for mass spectrometry data analysis is relatively new, with only a handful of publications available to date. These studies have demonstrated the potential of deep learning models to extract meaningful information from raw mass spectrometry data and perform predictive tasks without the need for extensive data preprocessing as required by the traditional workflows.</span>
<span id="cb14-56"><a href="#cb14-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-57"><a href="#cb14-57" aria-hidden="true" tabindex="-1"></a><span class="fu">## Previous Research</span></span>
<span id="cb14-58"><a href="#cb14-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-59"><a href="#cb14-59" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>In 2018, @behrmann_deep_2018 used deep learning techniques for tumor classification in Imaging Mass Spectrometry (IMS) data.</span>
<span id="cb14-60"><a href="#cb14-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-61"><a href="#cb14-61" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>In 2020, @seddiki_towards_2020 utilized MALDI-TOF images of rat brain samples to assess the ability of three different CNN architectures – LeNet, Lecun, and VGG9 – to differentiate between different types of cancers based on their molecular profiles.</span>
<span id="cb14-62"><a href="#cb14-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-63"><a href="#cb14-63" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>In 2021, @cadow_feasibility_2021 explored the use of pre-trained networks for the classification of tumors from normal prostate biopsies derived from SWATH-MS data. They delved into the potential of deep learning models for analyzing raw mass spectrometry data and performing predictive tasks without the need for protein quantification. To process raw MS images, the authors employed pre-trained neural network models to convert them into numerical vectors, enabling further processing. They then compared several classifiers, including logistic regression, support vector machines, and random forests, to accurately predict the phenotype.</span>
<span id="cb14-64"><a href="#cb14-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-65"><a href="#cb14-65" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>In 2022, @shen_deep_2022 released deepPseudoMSI, a deep learning-based pseudo-mass spectrometry imaging platform, designed to predict the gestational age in pregnant women based on LC-MS-based metabolomics data. This application consists of two components: Pseudo-MS Image Converter: for converting LC-MS data into pseudo-images and the deep learning model itself.</span>
<span id="cb14-66"><a href="#cb14-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-67"><a href="#cb14-67" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb14-68"><a href="#cb14-68" aria-hidden="true" tabindex="-1"></a><span class="fu">## Project Structure</span></span>
<span id="cb14-69"><a href="#cb14-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-70"><a href="#cb14-70" aria-hidden="true" tabindex="-1"></a>To accommodate the high computational demands of neural network training, the DeepLCMS project is divided into two main parts. The first part focuses on data preprocessing, specifically converting LC/MS data into pseudo-images using the PyOpenMS library, which is written in C++ and optimized for efficiency. This task can be handled on a CPU, and the corresponding source code is found in the <span class="in">`src/cpu_modules`</span> directory.</span>
<span id="cb14-71"><a href="#cb14-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-72"><a href="#cb14-72" aria-hidden="true" tabindex="-1"></a>To effectively train the neural networks, the project utilizes the PyTorch Lightning framework, which facilitates the development of a well-structured, modular codebase. Training experiments are conducted within Jupyter Notebooks running on Google Colab, a cloud platform offering free access to GPUs. The training code and associated modules reside within the <span class="in">`src/gpu_modules`</span> directory, seamlessly integrating with Google Colab for effortless module imports.</span>
<span id="cb14-73"><a href="#cb14-73" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-74"><a href="#cb14-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-75"><a href="#cb14-75" aria-hidden="true" tabindex="-1"></a><span class="fu"># Materials and Methods</span></span>
<span id="cb14-76"><a href="#cb14-76" aria-hidden="true" tabindex="-1"></a><span class="fu">## Dataset</span></span>
<span id="cb14-77"><a href="#cb14-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-78"><a href="#cb14-78" aria-hidden="true" tabindex="-1"></a>To ensure the feasibility of our proof-of-concept demonstration, we selected a suitable dataset from the <span class="co">[</span><span class="ot">Metabolomics Workbench</span><span class="co">](https://www.metabolomicsworkbench.org/data/DRCCMetadata.php?Mode=Study&amp;StudyID=ST001618&amp;StudyType=MS&amp;ResultType=5)</span>. We prioritized studies with distinct groups and a minimum sample size of 200. Additionally, we selected a dataset with a disk requirement of less than 50 GB to minimize computational resource demands. Based on these criteria, we identified the Golestan Cohort Study <span class="co">[</span><span class="ot">@pourshams_cohort_2010</span><span class="co">]</span>. This study, conducted in northeastern Iran comprising approximately 50,000 individuals, primarily investigates the risk factors for upper gastrointestinal cancers in this high-risk region. Quantitative targeted liquid chromatography mass spectrometric (LC-MS/MS) data was obtained for a subset of the cohort and collected at the University of North Carolina at Chapel Hill to identify distinct biochemical alterations induced by opium consumption <span class="co">[</span><span class="ot">@ghanbari_metabolomics_2021; @li_untargeted_2020</span><span class="co">]</span>. The dataset consisted of 218 opioid users and 80 non-users. After initial data inspection and conversion to mzML format using the ProteoWizard 3.0.22155 software, files were divided into training (n = 214), validation (n = 54), and test (n = 30) sets. To evaluate the impact of image characteristics and augmentation techniques on classification performance, four datasets were prepared. One dataset contained lower quality, more pixalated images due to the bin size set to 500 × 500 using the numpy library's <span class="in">`histogram2d`</span> function and was named <span class="in">`ST001618_Opium_study_LC_MS_500`</span>. Another dataset, while also using bin size of 500 x 500, it also incorporated data augmentation using the <span class="in">`augment_images`</span> function, generating nine additional images per training set with random offsets of up to 5 units in both x and y directions. This dataset was named <span class="in">`ST001618_Opium_study_LC_MS_500_augmented`</span>. The third dataset employed a higher bin size of 1000 × 1000 leading to sharper pseudoimages and was named <span class="in">`ST001618_Opium_study_LC_MS_1000`</span>. The final dataset employed image augmentation technique, generating nine additional images per training set with random offsets of up to 5 units in both x and y directions and used a bin size of 1000 × 1000. It was named <span class="in">`ST001618_Opium_study_LC_MS_1000_augmented`</span>.</span>
<span id="cb14-79"><a href="#cb14-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-80"><a href="#cb14-80" aria-hidden="true" tabindex="-1"></a><span class="fu">## Software</span></span>
<span id="cb14-81"><a href="#cb14-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-82"><a href="#cb14-82" aria-hidden="true" tabindex="-1"></a>The code base relied on the following software packages and versions on Google Colab: PyTorch Lightning 2.1.3, Pytorch Image Models (timm)  0.9.12, torchinfo 1.8.0, Optuna 3.5.0, TorchCam 0.4.0, pandas 2.1.3, NumPy 1.26, and Matplotlib 3.7.1. The remaining packages and their respective versions can be found in the requirements.txt file.</span>
<span id="cb14-83"><a href="#cb14-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-84"><a href="#cb14-84" aria-hidden="true" tabindex="-1"></a><span class="fu">## Creating the pseudo images</span></span>
<span id="cb14-85"><a href="#cb14-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-86"><a href="#cb14-86" aria-hidden="true" tabindex="-1"></a>Mass spectrometry-based pseudo images are generated using the <span class="in">`plot_2D_spectra_overview`</span> function, which creates a 2D heatmap of the LC/MS data. The function first loads the data in mzML format from the provided file path using the OpenMS library. A 2D histogram is then created, with the x-axis representing the retention time (Rt) and the y-axis representing the m/z values. The intensity values of the histogram are normalized using the highest peak at a given Rt. To enhance the visualization of the data, a Gaussian filter is applied to reduce noise, and the filtered image is scaled to the range 0-255. The histogram is then logarithmically transformed. Finally, pseudoimages are created using matplotlib, with the colormap set to 'jet'. </span>
<span id="cb14-87"><a href="#cb14-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-88"><a href="#cb14-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-89"><a href="#cb14-89" aria-hidden="true" tabindex="-1"></a><span class="fu">## Untargeted data analysis with MS-DIAL</span></span>
<span id="cb14-90"><a href="#cb14-90" aria-hidden="true" tabindex="-1"></a>To assess the model's decision-making process and enable interpretability, we analyzed two pooled samples (sp_10 and sp_20) using MS-DIAL <span class="co">[</span><span class="ot">@tsugawa_ms-dial_2015</span><span class="co">]</span>, a data-independent MS/MS deconvolution software, in positive mode applying the default settings. For metabolite identification, the MSP database from the <span class="co">[</span><span class="ot">MS-DIAL</span><span class="co">](http://prime.psc.riken.jp/compms/msdial/main.html)</span> website was utilized, that contains ESI-MS/MS fragmentation data for 16,481 unique standards.</span>
<span id="cb14-91"><a href="#cb14-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-92"><a href="#cb14-92" aria-hidden="true" tabindex="-1"></a><span class="fu"># Results and Discussion</span></span>
<span id="cb14-93"><a href="#cb14-93" aria-hidden="true" tabindex="-1"></a><span class="fu">## Selecting a model architecture family</span></span>
<span id="cb14-94"><a href="#cb14-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-95"><a href="#cb14-95" aria-hidden="true" tabindex="-1"></a>From the readily available model architecture families provided by <span class="co">[</span><span class="ot">Pytorch Image Models</span><span class="co">](https://github.com/huggingface/pytorch-image-models#models)</span>, 68 unique ones were selected as representative examples of a given class and filtered based on their parameter count, selecting those with parameters counts between 10 and 20 million to allow for an unbiased comparison. Out of the 68 selected 32 were subsequently underwent training with validation metrics recorded. According to @tbl-exp-1-result (arranged based on validation loss), the MobileOne S3 emerged as the top performer with an F1 score of 0.95. It was followed by DenseNet (F1 = 0.92), MobileViTV2 (F1 = 0.90), and ConvNeXt Nano (F1 = 0.84). RepVit M3 rounded out the top five with an F1 score of 0.86. To delve deeper into the performance of each architecture family, we also evaluated all individual architectures within each family (@tbl-exp-1-best_models).</span>
<span id="cb14-96"><a href="#cb14-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-99"><a href="#cb14-99" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-100"><a href="#cb14-100" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-exp-1-result</span></span>
<span id="cb14-101"><a href="#cb14-101" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: Validation metrics of model architectural families from the PyTorch Image Models library using models with parameter counts ranging from 10 to 20 million.</span></span>
<span id="cb14-102"><a href="#cb14-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-103"><a href="#cb14-103" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb14-104"><a href="#cb14-104" aria-hidden="true" tabindex="-1"></a>    pd.read_csv(<span class="st">"exp-1-result.csv"</span>)</span>
<span id="cb14-105"><a href="#cb14-105" aria-hidden="true" tabindex="-1"></a>    .rename(</span>
<span id="cb14-106"><a href="#cb14-106" aria-hidden="true" tabindex="-1"></a>        columns<span class="op">=</span><span class="kw">lambda</span> df: df.replace(<span class="st">"_"</span>, <span class="st">" "</span>).replace(<span class="st">"val"</span>, <span class="st">"validation"</span>).title()</span>
<span id="cb14-107"><a href="#cb14-107" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb14-108"><a href="#cb14-108" aria-hidden="true" tabindex="-1"></a>    .rename(columns<span class="op">=</span>{<span class="st">"Minimal Param Model Count"</span>: <span class="st">"Parameter Count (M)"</span>})</span>
<span id="cb14-109"><a href="#cb14-109" aria-hidden="true" tabindex="-1"></a>    .<span class="bu">round</span>(<span class="dv">2</span>)</span>
<span id="cb14-110"><a href="#cb14-110" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-111"><a href="#cb14-111" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-112"><a href="#cb14-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-113"><a href="#cb14-113" aria-hidden="true" tabindex="-1"></a>As shown in @tbl-exp-1-best_models, the top four positions based on the validation metrics were occupied by models belonging to the ConvNeXt family. These models, particularly the first two (convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384 with validation loss 0.19, convnext_large_mlp.clip_laion2b_augreg_ft_in1k with validation loss 0.22), were pre-trained on the extensive LAION-2B dataset and fine-tuned on ImageNet-1k, enabling them to learn complex patterns that generalize well to unseen data. It is plausible that this extensive dataset and sophisticated model architecture played a pivotal role in their exceptional performance in this task. The dominance of the ConvNeXt family is noteworthy, suggesting their effectiveness in handling complex data such as mass spectrometry pseudoimages. Apple's MobileOne also demonstrated remarkable results (validation loss = 0.27), ranking fourth in terms of validation loss. Finally, MobileViT (validation loss = 0.27), a lightweight network, secured the seventh position. While the ConvNeXt models consistently outperformed other models in terms of validation metrics, it is essential to recognize that their larger size and higher parameter count translate into increased computational demands for training and heightened susceptibility to overfitting. Their complexity demands careful hyperparameter tuning and regularization techniques to mitigate overfitting and achieve optimal performance.</span>
<span id="cb14-114"><a href="#cb14-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-117"><a href="#cb14-117" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-118"><a href="#cb14-118" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-exp-1-best_models</span></span>
<span id="cb14-119"><a href="#cb14-119" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: 'Top 10 models based on evaluation metrics of all model architectures in families from the PyTorch Image Models library, regardless of their parameter counts.'</span></span>
<span id="cb14-120"><a href="#cb14-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-121"><a href="#cb14-121" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb14-122"><a href="#cb14-122" aria-hidden="true" tabindex="-1"></a>    pd.read_csv(<span class="st">"exp-1-best_models.csv"</span>)</span>
<span id="cb14-123"><a href="#cb14-123" aria-hidden="true" tabindex="-1"></a>    .rename(</span>
<span id="cb14-124"><a href="#cb14-124" aria-hidden="true" tabindex="-1"></a>        columns<span class="op">=</span><span class="kw">lambda</span> df: df.replace(<span class="st">"_"</span>, <span class="st">" "</span>).replace(<span class="st">"val"</span>, <span class="st">"validation"</span>).title()</span>
<span id="cb14-125"><a href="#cb14-125" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb14-126"><a href="#cb14-126" aria-hidden="true" tabindex="-1"></a>    .<span class="bu">round</span>(<span class="dv">2</span>)</span>
<span id="cb14-127"><a href="#cb14-127" aria-hidden="true" tabindex="-1"></a>    .head(<span class="dv">10</span>)</span>
<span id="cb14-128"><a href="#cb14-128" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-129"><a href="#cb14-129" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-130"><a href="#cb14-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-131"><a href="#cb14-131" aria-hidden="true" tabindex="-1"></a>To further assess the suitability of these models and the consistency of their performance, we trained the top three performing models from each family five consecutive times and calculated the median and standard deviation of their validation metrics (@fig-exp-2-replicates_result). This approach allowed us to identify the models that exhibited the most consistent validation performance across multiple training runs. Statistical significance across replicates was evaluated using Dunn's test with Benjamini-Hochberg correction. The results revealed that the MobileViT2 family performed similarly to the ConvNeXt across all five training runs, except for the validation losses where ConvNeXt exhibited the lowest values with 0.21. According to the replication study, the ConvNeXt emerged as the most effective model, surpassing both MobileOne and MobileViT2. Despite exhibiting similar patterns, ConvNeXt exhibited a statistically significant advantage over MobileViT2 in validation loss (p = 0.01) and achieved significantly better validation recall (p &lt; 0.05). Among the three models tested, MobileOne consistently underperformed its counterparts in all performance metrics, except for validation recall, where it narrowly outperformed MobileViT2 (p &lt; 0.05). In light of these findings, we opted to use the ConvNeXt large model in further experiments.</span>
<span id="cb14-132"><a href="#cb14-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-135"><a href="#cb14-135" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-136"><a href="#cb14-136" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-exp-2-replicates_result</span></span>
<span id="cb14-137"><a href="#cb14-137" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Median and standard deviation of best validation metrics achieved during consecutive trainings</span></span>
<span id="cb14-138"><a href="#cb14-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-139"><a href="#cb14-139" aria-hidden="true" tabindex="-1"></a>PIL.Image.<span class="bu">open</span>(<span class="st">"exp-2-replicates_result.png"</span>).convert(<span class="st">"RGB"</span>)</span>
<span id="cb14-140"><a href="#cb14-140" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-141"><a href="#cb14-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-142"><a href="#cb14-142" aria-hidden="true" tabindex="-1"></a><span class="fu">## Selecting image properties as hyperparameters</span></span>
<span id="cb14-143"><a href="#cb14-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-144"><a href="#cb14-144" aria-hidden="true" tabindex="-1"></a>Next, we explored the impact of image sharpness and offsets along the x and y axis as a method for image augmentation to replicate the effects of Rt shifts and incidental mass calibration drifts. @fig-exp-3-image_example illustrates how the number of bins employed in the <span class="in">`np.histogram2d`</span> function within the <span class="in">`plot_2D_spectra_overview`</span> function—which serves to transform LC/MS files into pseudo images—influences the image sharpness. Here, the bins parameter signifies the number of bins in both the x and y dimensions, which are represented by nx and ny, respectively. This results in the formation of an nx by ny bin grid across the data for the histogram. By increasing the number of bins (from 500, 500 to 1000, 1000), one could obtain a sharper image, but it could also lead to increased noise and computational complexity. </span>
<span id="cb14-145"><a href="#cb14-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-146"><a href="#cb14-146" aria-hidden="true" tabindex="-1"></a>Furthermore, we sought to evaluate the impact of image augmentation, specifically the generation of multiple, modified images prior to image training, on the validation metrics. The <span class="in">`augment_images`</span> function applies random offsets to the specified image, producing a designated number— 9 in our case—of augmented images with a maximum horizontal offset of five and a maximum vertical offset of five pixels. </span>
<span id="cb14-147"><a href="#cb14-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-150"><a href="#cb14-150" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-151"><a href="#cb14-151" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-exp-3-image_example</span></span>
<span id="cb14-152"><a href="#cb14-152" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: 'Examples of images obtained after applying different bin sizes and offsets as imitation of Rt shift. The amount of offset along the x and y axes is given in the image name. Image quality as a function of the number of bins is set to either 500, indicating a lower-quality image, or 1000, indicating a higher-quality image.'</span></span>
<span id="cb14-153"><a href="#cb14-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-154"><a href="#cb14-154" aria-hidden="true" tabindex="-1"></a>img_paths <span class="op">=</span> <span class="bu">list</span>(Path.cwd().glob(<span class="st">"exp_3_U_2*"</span>))</span>
<span id="cb14-155"><a href="#cb14-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-156"><a href="#cb14-156" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">9</span>, <span class="dv">9</span>))</span>
<span id="cb14-157"><a href="#cb14-157" aria-hidden="true" tabindex="-1"></a>rows, cols <span class="op">=</span> <span class="dv">2</span>, <span class="dv">2</span></span>
<span id="cb14-158"><a href="#cb14-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-159"><a href="#cb14-159" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx, img_path <span class="kw">in</span> <span class="bu">zip</span>(<span class="bu">range</span>(<span class="dv">1</span>, rows <span class="op">*</span> cols <span class="op">+</span> <span class="dv">1</span>), img_paths):</span>
<span id="cb14-160"><a href="#cb14-160" aria-hidden="true" tabindex="-1"></a>    fig.add_subplot(rows, cols, idx)</span>
<span id="cb14-161"><a href="#cb14-161" aria-hidden="true" tabindex="-1"></a>    img <span class="op">=</span> PIL.Image.<span class="bu">open</span>(img_path).convert(<span class="st">"RGB"</span>)</span>
<span id="cb14-162"><a href="#cb14-162" aria-hidden="true" tabindex="-1"></a>    plt.imshow(img)</span>
<span id="cb14-163"><a href="#cb14-163" aria-hidden="true" tabindex="-1"></a>    plt.title(img_path.stem[<span class="dv">6</span>:])</span>
<span id="cb14-164"><a href="#cb14-164" aria-hidden="true" tabindex="-1"></a>    plt.axis(<span class="va">False</span>)</span>
<span id="cb14-165"><a href="#cb14-165" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-166"><a href="#cb14-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-167"><a href="#cb14-167" aria-hidden="true" tabindex="-1"></a>@fig-exp_3_experiment_result_1 illustrates, image sharpness in combination with image augmentation had a significant impact on the validation loss. Setting the bin size to 500 and subsequent augmentation resulted in the lowest validation loss (0.129) and the highest validation F1 (0.982). Based on these findings, less image sharpness and more training images, obtained through data augmentation prior to training, had a beneficial effect on the validation metrics. Taken together, the final training and fine-tuning is conducted on images with bin size set to 500 and subsequent augmentation applied resulting in the generation of nine extra samples per training image.</span>
<span id="cb14-168"><a href="#cb14-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-171"><a href="#cb14-171" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-172"><a href="#cb14-172" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-exp_3_experiment_result_1</span></span>
<span id="cb14-173"><a href="#cb14-173" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Training and validation metrics of experiments involving the selection of image properties as hyperparameters</span></span>
<span id="cb14-174"><a href="#cb14-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-175"><a href="#cb14-175" aria-hidden="true" tabindex="-1"></a>PIL.Image.<span class="bu">open</span>(<span class="st">"exp_3_experiment_result_1.png"</span>).convert(<span class="st">"RGB"</span>)</span>
<span id="cb14-176"><a href="#cb14-176" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-177"><a href="#cb14-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-178"><a href="#cb14-178" aria-hidden="true" tabindex="-1"></a><span class="fu">## Hyperparameter tuning with Optuna</span></span>
<span id="cb14-179"><a href="#cb14-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-180"><a href="#cb14-180" aria-hidden="true" tabindex="-1"></a>Optuna, an open-source hyperparameter optimization framework, delivers an automated and efficient approach for optimizing hyperparameters for machine learning and deep learning models. The framework operates by defining an objective function that could be either maximized or minimized. This function encapsulates the model and the metric to be optimized. Within the objective function, hyperparameters are suggested using a trial object. Optuna supports various types of hyperparameters, including float, integer, and categorical. The core of Optuna's operation lies in its optimization process, which employs state-of-the-art algorithms to search the hyperparameter space efficiently. It can also prunes unpromising trials to conserve computational resources. To optimize the model's performance, we employed Optuna to identify the most effective optimizer from a pool of four: <span class="in">`Adam`</span>, <span class="in">`AdamW`</span>, <span class="in">`Adamax`</span>, and <span class="in">`RMSprop`</span>. Additionally, we sought an optimal learning rate scheduler, choosing between <span class="in">`CosineAnnealingLR`</span> and <span class="in">`ReduceLROnPlateau`</span>, based on the validation loss achieved during training. After extensive optimization, the combination of <span class="in">`Adamax`</span> optimizer and <span class="in">`CosineAnnealingLR`</span> learning rate scheduler yielded the lowest validation loss (0.23), resulting in its selection for subsequent experiments.</span>
<span id="cb14-181"><a href="#cb14-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-182"><a href="#cb14-182" aria-hidden="true" tabindex="-1"></a><span class="fu">## Evaluating the final model on the test set</span></span>
<span id="cb14-183"><a href="#cb14-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-184"><a href="#cb14-184" aria-hidden="true" tabindex="-1"></a>The final model training was conducted using the optimal settings determined during the optimization experiments, employing transfer learning of a pre-trained ConvNeXt large model, with all of its layers frozen except for the last classification head. The learning rate was automatically determined to be 0.006 based on the learning rate finder tool implemented by PyTorch Lightning. For the training, we utilized the augmented dataset (@fig-exp-5-transformed_grid) that featured bin size set to 500 for both dimensions. To prevent overfitting and promote better generalization capabilities, random erasing (probability=1.00) and color jitter (probability=0.25) were used. This technique effectively augments the training data, prompting the model to minimize its dependence on specific regions of the image thus preveting the model from memorizing limited patterns and encourages it to generalize better to unseen data. The underlying concept resembles a dropout layer, a common component of neural network architectures. The optimized model achieved validation metrics with **loss** of 0.138, **validation precision** of 0.963, **F1** 0.946, **validation accuracy** exceeding 0.944, and **validation recall** reaching 0.93.</span>
<span id="cb14-185"><a href="#cb14-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-186"><a href="#cb14-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-187"><a href="#cb14-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-188"><a href="#cb14-188" aria-hidden="true" tabindex="-1"></a>During training, we employed early stopping to prevent overfitting, terminating training if the validation loss fails to improve for 10 consecutive epochs. Additionally, we utilized PyTorch Lightning's built-in <span class="in">`ModelCheckpoint`</span> to save the best-performing model, ensuring that the model that achieved the highest validation metrics was preserved and evaluated later on.</span>
<span id="cb14-189"><a href="#cb14-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-192"><a href="#cb14-192" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-193"><a href="#cb14-193" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-exp-5-transformed_grid</span></span>
<span id="cb14-194"><a href="#cb14-194" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: 'Example of augmented dataloader images used during training time, demonstrating the application of random erasing and color jitter to reduce overfitting'</span></span>
<span id="cb14-195"><a href="#cb14-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-196"><a href="#cb14-196" aria-hidden="true" tabindex="-1"></a>PIL.Image.<span class="bu">open</span>(<span class="st">"exp-5-transformed_grid.png"</span>).convert(<span class="st">"RGB"</span>)</span>
<span id="cb14-197"><a href="#cb14-197" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-198"><a href="#cb14-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-199"><a href="#cb14-199" aria-hidden="true" tabindex="-1"></a>After the training was completed, the best-performing model was reloaded, and test metrics were calculated using the 30 test samples present in our dataset. According to the final evaluation, the model produced 20 True Positives, 7 True Negatives, 2 False Negatives, and 1 False Positive.  </span>
<span id="cb14-200"><a href="#cb14-200" aria-hidden="true" tabindex="-1"></a>These results translate to: **Precision** = 0.952, **Recall** = 0.909, **F1** = 0.930, **Accuracy** = 0.90. </span>
<span id="cb14-201"><a href="#cb14-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-202"><a href="#cb14-202" aria-hidden="true" tabindex="-1"></a>While the model achieved impressive validation metrics, the slight drop of performance on the test set compared to the validation metrics indicates potential overfitting. Further optimization of the model, particularly during the method optimization phase, could be achieved through cross-validation strategies. However, this approach might necessitate excessive computational resources.</span>
<span id="cb14-203"><a href="#cb14-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-204"><a href="#cb14-204" aria-hidden="true" tabindex="-1"></a><span class="fu">## Model interpretability </span></span>
<span id="cb14-205"><a href="#cb14-205" aria-hidden="true" tabindex="-1"></a><span class="fu">### Estimating probability</span></span>
<span id="cb14-206"><a href="#cb14-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-207"><a href="#cb14-207" aria-hidden="true" tabindex="-1"></a>To assess the model's confidence in its predictions, we can transform the model's raw output, which is the binary cross-entropy loss derived from a set of logits obtained using PyTorch's <span class="in">`BCEWithLogitsLoss`</span> function, into prediction probabilities using a sigmoid function. These converted values reflect the probability that the model assigns to a given prediction belonging to Class 1 (Opioid User). @fig-exp-5-prediction_matrix illustrates this concept, showing that in the first instance, the model estimates a 43% probability of the instance belonging to Class 1, suggesting a higher likelihood of it being a Non-Opioid User (and this prediction proved correct).</span>
<span id="cb14-208"><a href="#cb14-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-211"><a href="#cb14-211" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-212"><a href="#cb14-212" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-exp-5-prediction_matrix</span></span>
<span id="cb14-213"><a href="#cb14-213" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: 'Set of images from the test set, along with their corresponding prediction probabilities for the Class 1 (Opidid User). The true labels are displayed beside the predictions. Green labels indicate successful predictions, whereas red labels denote unsuccessful ones'</span></span>
<span id="cb14-214"><a href="#cb14-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-215"><a href="#cb14-215" aria-hidden="true" tabindex="-1"></a>PIL.Image.<span class="bu">open</span>(<span class="st">"exp-5-prediction_matrix.png"</span>).convert(<span class="st">"RGB"</span>)</span>
<span id="cb14-216"><a href="#cb14-216" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-217"><a href="#cb14-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-218"><a href="#cb14-218" aria-hidden="true" tabindex="-1"></a><span class="fu">### Looking at Layer Class Activation Maps</span></span>
<span id="cb14-219"><a href="#cb14-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-220"><a href="#cb14-220" aria-hidden="true" tabindex="-1"></a>To gain insights into the regions of an image our model focuses on when making a prediction, we employed LayerCAM, a tool from the TorchCam package. LayerCAM <span class="co">[</span><span class="ot">@jiang_layercam_2021</span><span class="co">]</span>, short for Layer Class Activation Maps, is a technique for generating class activation maps from distinct layers of a CNN. These regions of interests highlight key features the model utilizes to differentiate between opioid users and non-opioid users. Class activation maps are typically derived from the final convolutional layer of a CNN, highlighting discriminative object regions for the class of interest. Such a tool could be particularly valuable for validating the insights derived from our deep learning model, particularly when compared to more targeted or untargeted metabolomics data. </span>
<span id="cb14-221"><a href="#cb14-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-224"><a href="#cb14-224" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-225"><a href="#cb14-225" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-exp-5-plot_activation</span></span>
<span id="cb14-226"><a href="#cb14-226" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: 'Regions of interest identified by LayerCAM, demonstrating the specific areas of the image that our model deems most relevant for making predictions.'</span></span>
<span id="cb14-227"><a href="#cb14-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-228"><a href="#cb14-228" aria-hidden="true" tabindex="-1"></a>PIL.Image.<span class="bu">open</span>(<span class="st">"exp-5-plot_activation.png"</span>).convert(<span class="st">"RGB"</span>)</span>
<span id="cb14-229"><a href="#cb14-229" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-230"><a href="#cb14-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-231"><a href="#cb14-231" aria-hidden="true" tabindex="-1"></a><span class="fu">### Overlaying the untargeted metabolomics data with the class activation map</span></span>
<span id="cb14-232"><a href="#cb14-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-233"><a href="#cb14-233" aria-hidden="true" tabindex="-1"></a>To gain a better understanding of the specific metabolic features that influence the model's decision-making process in classifying individuals into either group, we analyzed two examples of the pooled samples using MS-DIAL <span class="co">[</span><span class="ot">@tsugawa_ms-dial_2015</span><span class="co">]</span>, an open-source software pipeline designed for data-independent MS/MS deconvolution and identification of small molecules. Subsequently, metabolite annotation was performed using the MSP database, which houses ESI-MS/MS fragmentation data for 16,481 unique authentic standards. The MSP database served as the basis for assigning compound identification levels: 999 denoted instances where MS-DIAL could not identify the feature due to missing MS/MS fragmentation spectrum or non-matching library entries. A tentative identification was assigned with 530, indicating a match based on accurate mass comparison. A more confident tentative identification was marked with 430, indicating spectral correspondence between the feature and the library entry's MS/MS spectrum.</span>
<span id="cb14-234"><a href="#cb14-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-235"><a href="#cb14-235" aria-hidden="true" tabindex="-1"></a>To leverage the untargeted metabolomics data, we can, for example, construct a function similar to <span class="in">`overlay_untargeted_data`</span>, which facilitates the extraction of specific regions from the feature map derived from MS-DIAL based on retention time and accurate mass. These extracted features can be overlaid as a scatter plot alongside our activation map. This approach, in theory, should provide a deeper understanding of the features that play a crucial role in differentiating between the various classes.</span>
<span id="cb14-236"><a href="#cb14-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-239"><a href="#cb14-239" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-240"><a href="#cb14-240" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> overlay_untargeted_data(</span>
<span id="cb14-241"><a href="#cb14-241" aria-hidden="true" tabindex="-1"></a>    query_str: <span class="bu">str</span>,</span>
<span id="cb14-242"><a href="#cb14-242" aria-hidden="true" tabindex="-1"></a>    ms_dial_location: Path,</span>
<span id="cb14-243"><a href="#cb14-243" aria-hidden="true" tabindex="-1"></a>    background_image_location: Path,</span>
<span id="cb14-244"><a href="#cb14-244" aria-hidden="true" tabindex="-1"></a>    crop_area: Tuple[<span class="bu">int</span>, <span class="bu">int</span>, <span class="bu">int</span>, <span class="bu">int</span>],</span>
<span id="cb14-245"><a href="#cb14-245" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb14-246"><a href="#cb14-246" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb14-247"><a href="#cb14-247" aria-hidden="true" tabindex="-1"></a><span class="co">    Overlay untargeted metabolomics data on a background image.</span></span>
<span id="cb14-248"><a href="#cb14-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-249"><a href="#cb14-249" aria-hidden="true" tabindex="-1"></a><span class="co">    Parameters:</span></span>
<span id="cb14-250"><a href="#cb14-250" aria-hidden="true" tabindex="-1"></a><span class="co">    - query_str (str): Query string for filtering the data.</span></span>
<span id="cb14-251"><a href="#cb14-251" aria-hidden="true" tabindex="-1"></a><span class="co">    - ms_dial_location (str): Location of the mass spectrometry data file.</span></span>
<span id="cb14-252"><a href="#cb14-252" aria-hidden="true" tabindex="-1"></a><span class="co">    - background_image_location (str): Location of the background image.</span></span>
<span id="cb14-253"><a href="#cb14-253" aria-hidden="true" tabindex="-1"></a><span class="co">    - crop_area (tuple): Coordinates for cropping the background image (left, upper, right, lower).</span></span>
<span id="cb14-254"><a href="#cb14-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-255"><a href="#cb14-255" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb14-256"><a href="#cb14-256" aria-hidden="true" tabindex="-1"></a><span class="co">    - composite_img (PIL.Image.Image): Overlay of data on the background image.</span></span>
<span id="cb14-257"><a href="#cb14-257" aria-hidden="true" tabindex="-1"></a><span class="co">    - df (pd.DataFrame): Filtered and processed DataFrame.</span></span>
<span id="cb14-258"><a href="#cb14-258" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb14-259"><a href="#cb14-259" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Read and preprocess the mass spectrometry data</span></span>
<span id="cb14-260"><a href="#cb14-260" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> (</span>
<span id="cb14-261"><a href="#cb14-261" aria-hidden="true" tabindex="-1"></a>        pd.read_csv(</span>
<span id="cb14-262"><a href="#cb14-262" aria-hidden="true" tabindex="-1"></a>            ms_dial_location,</span>
<span id="cb14-263"><a href="#cb14-263" aria-hidden="true" tabindex="-1"></a>            skiprows<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb14-264"><a href="#cb14-264" aria-hidden="true" tabindex="-1"></a>            usecols<span class="op">=</span>[</span>
<span id="cb14-265"><a href="#cb14-265" aria-hidden="true" tabindex="-1"></a>                <span class="st">"Average Rt(min)"</span>,</span>
<span id="cb14-266"><a href="#cb14-266" aria-hidden="true" tabindex="-1"></a>                <span class="st">"Average Mz"</span>,</span>
<span id="cb14-267"><a href="#cb14-267" aria-hidden="true" tabindex="-1"></a>                <span class="st">"Metabolite name"</span>,</span>
<span id="cb14-268"><a href="#cb14-268" aria-hidden="true" tabindex="-1"></a>                <span class="st">"Ontology"</span>,</span>
<span id="cb14-269"><a href="#cb14-269" aria-hidden="true" tabindex="-1"></a>                <span class="st">"Annotation tag (VS1.0)"</span>,</span>
<span id="cb14-270"><a href="#cb14-270" aria-hidden="true" tabindex="-1"></a>                <span class="st">"MS/MS matched"</span>,</span>
<span id="cb14-271"><a href="#cb14-271" aria-hidden="true" tabindex="-1"></a>                <span class="st">"SP_10"</span>,</span>
<span id="cb14-272"><a href="#cb14-272" aria-hidden="true" tabindex="-1"></a>                <span class="st">"SP_20"</span>,</span>
<span id="cb14-273"><a href="#cb14-273" aria-hidden="true" tabindex="-1"></a>            ],</span>
<span id="cb14-274"><a href="#cb14-274" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb14-275"><a href="#cb14-275" aria-hidden="true" tabindex="-1"></a>        .rename(columns<span class="op">=</span><span class="kw">lambda</span> x: re.sub(<span class="vs">r"\W|[%/]"</span>, <span class="st">"_"</span>, x.lower()))</span>
<span id="cb14-276"><a href="#cb14-276" aria-hidden="true" tabindex="-1"></a>        .query(<span class="st">"annotation_tag__vs1_0_ == '430' or annotation_tag__vs1_0_ == '530'"</span>)</span>
<span id="cb14-277"><a href="#cb14-277" aria-hidden="true" tabindex="-1"></a>        .query(query_str)</span>
<span id="cb14-278"><a href="#cb14-278" aria-hidden="true" tabindex="-1"></a>        .drop_duplicates(subset<span class="op">=</span><span class="st">"metabolite_name"</span>)</span>
<span id="cb14-279"><a href="#cb14-279" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb14-280"><a href="#cb14-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-281"><a href="#cb14-281" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create scatterplot</span></span>
<span id="cb14-282"><a href="#cb14-282" aria-hidden="true" tabindex="-1"></a>    sns.scatterplot(</span>
<span id="cb14-283"><a href="#cb14-283" aria-hidden="true" tabindex="-1"></a>        data<span class="op">=</span>df,</span>
<span id="cb14-284"><a href="#cb14-284" aria-hidden="true" tabindex="-1"></a>        x<span class="op">=</span><span class="st">"average_rt_min_"</span>,</span>
<span id="cb14-285"><a href="#cb14-285" aria-hidden="true" tabindex="-1"></a>        y<span class="op">=</span><span class="st">"average_mz"</span>,</span>
<span id="cb14-286"><a href="#cb14-286" aria-hidden="true" tabindex="-1"></a>        alpha<span class="op">=</span><span class="fl">0.5</span>,</span>
<span id="cb14-287"><a href="#cb14-287" aria-hidden="true" tabindex="-1"></a>        s<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb14-288"><a href="#cb14-288" aria-hidden="true" tabindex="-1"></a>        color<span class="op">=</span><span class="st">"black"</span>,</span>
<span id="cb14-289"><a href="#cb14-289" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb14-290"><a href="#cb14-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-291"><a href="#cb14-291" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Set plot limits and turn off axis</span></span>
<span id="cb14-292"><a href="#cb14-292" aria-hidden="true" tabindex="-1"></a>    plt.ylim(<span class="dv">50</span>, <span class="dv">750</span>)</span>
<span id="cb14-293"><a href="#cb14-293" aria-hidden="true" tabindex="-1"></a>    plt.xlim(<span class="dv">0</span>, <span class="dv">20</span>)</span>
<span id="cb14-294"><a href="#cb14-294" aria-hidden="true" tabindex="-1"></a>    plt.axis(<span class="va">False</span>)</span>
<span id="cb14-295"><a href="#cb14-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-296"><a href="#cb14-296" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create the foreground image</span></span>
<span id="cb14-297"><a href="#cb14-297" aria-hidden="true" tabindex="-1"></a>    buf <span class="op">=</span> io.BytesIO()</span>
<span id="cb14-298"><a href="#cb14-298" aria-hidden="true" tabindex="-1"></a>    plt.savefig(buf, <span class="bu">format</span><span class="op">=</span><span class="st">"jpeg"</span>, bbox_inches<span class="op">=</span><span class="st">"tight"</span>, pad_inches<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb14-299"><a href="#cb14-299" aria-hidden="true" tabindex="-1"></a>    buf.seek(<span class="dv">0</span>)</span>
<span id="cb14-300"><a href="#cb14-300" aria-hidden="true" tabindex="-1"></a>    foreground <span class="op">=</span> PIL.Image.<span class="bu">open</span>(buf)</span>
<span id="cb14-301"><a href="#cb14-301" aria-hidden="true" tabindex="-1"></a>    plt.close()</span>
<span id="cb14-302"><a href="#cb14-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-303"><a href="#cb14-303" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create the background image</span></span>
<span id="cb14-304"><a href="#cb14-304" aria-hidden="true" tabindex="-1"></a>    background <span class="op">=</span> (</span>
<span id="cb14-305"><a href="#cb14-305" aria-hidden="true" tabindex="-1"></a>        PIL.Image.<span class="bu">open</span>(background_image_location)</span>
<span id="cb14-306"><a href="#cb14-306" aria-hidden="true" tabindex="-1"></a>        .convert(<span class="st">"RGB"</span>)</span>
<span id="cb14-307"><a href="#cb14-307" aria-hidden="true" tabindex="-1"></a>        .crop(crop_area)</span>
<span id="cb14-308"><a href="#cb14-308" aria-hidden="true" tabindex="-1"></a>        .resize(foreground.size)</span>
<span id="cb14-309"><a href="#cb14-309" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb14-310"><a href="#cb14-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-311"><a href="#cb14-311" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a mask</span></span>
<span id="cb14-312"><a href="#cb14-312" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> PIL.Image.new(<span class="st">"L"</span>, foreground.size, <span class="dv">128</span>)</span>
<span id="cb14-313"><a href="#cb14-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-314"><a href="#cb14-314" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Composite the images</span></span>
<span id="cb14-315"><a href="#cb14-315" aria-hidden="true" tabindex="-1"></a>    composite_img <span class="op">=</span> PIL.Image.composite(background, foreground, mask)</span>
<span id="cb14-316"><a href="#cb14-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-317"><a href="#cb14-317" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> composite_img, df</span>
<span id="cb14-318"><a href="#cb14-318" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-319"><a href="#cb14-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-320"><a href="#cb14-320" aria-hidden="true" tabindex="-1"></a>As illustrated in @fig-overlay_example1, and @fig-overlay_example2, the most notable features identified by MS-DIAL with levels 430 and 530 are overlaid with two examples of the activation map generated by TorchCam. Importantly, morphine-3-glucuronide (Level 530), a major metabolite of morphine, stands out as a key feature influencing the model's decision-making process. Additionally, EDDP (Level 430), a major metabolite of methadone, a synthetic opioid agonist used for chronic pain and opioid use disorder, is also considered by the model. While we observed tentative evidence suggesting that the metabolic pathways involving Kreb's cycle, purine metabolism, central carbon metabolism, histone modification, and acetylation are also considered by the model, as it was pointed out by @li_untargeted_2020, the level of identification (530) indicates that these identifications are based solely on accurate mass measurement alone thus further investigation is warranted to confirm the identities of these potential biomarkers (@tbl-overlaid-identifications).</span>
<span id="cb14-321"><a href="#cb14-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-324"><a href="#cb14-324" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-325"><a href="#cb14-325" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-overlay_example1</span></span>
<span id="cb14-326"><a href="#cb14-326" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Example-1 of interest overlayed with proposed feature names tentativelyi identified by MS-DIAL.</span></span>
<span id="cb14-327"><a href="#cb14-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-328"><a href="#cb14-328" aria-hidden="true" tabindex="-1"></a>composite_img_pattern_1, pattern_df_1 <span class="op">=</span> overlay_untargeted_data(</span>
<span id="cb14-329"><a href="#cb14-329" aria-hidden="true" tabindex="-1"></a>    query_str<span class="op">=</span><span class="st">"3.5&lt;average_rt_min_&lt;5 and 460&lt;average_mz&lt;500 or 5&lt;average_rt_min_&lt;6.5 and 575&lt;average_mz&lt;650 or 9.5&lt;average_rt_min_&lt;10 and 280&lt;average_mz&lt;320"</span>,</span>
<span id="cb14-330"><a href="#cb14-330" aria-hidden="true" tabindex="-1"></a>    ms_dial_location<span class="op">=</span><span class="st">"PeakID_0_2023_12_25_17_39_09.csv"</span>,</span>
<span id="cb14-331"><a href="#cb14-331" aria-hidden="true" tabindex="-1"></a>    background_image_location<span class="op">=</span><span class="st">"exp-5-plot_activation.png"</span>,</span>
<span id="cb14-332"><a href="#cb14-332" aria-hidden="true" tabindex="-1"></a>    crop_area<span class="op">=</span>(<span class="dv">25</span>, <span class="dv">90</span>, <span class="dv">850</span>, <span class="dv">910</span>),</span>
<span id="cb14-333"><a href="#cb14-333" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-334"><a href="#cb14-334" aria-hidden="true" tabindex="-1"></a>font <span class="op">=</span> ImageFont.truetype(<span class="st">"arial.ttf"</span>, <span class="dv">16</span>)</span>
<span id="cb14-335"><a href="#cb14-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-336"><a href="#cb14-336" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a Draw object</span></span>
<span id="cb14-337"><a href="#cb14-337" aria-hidden="true" tabindex="-1"></a>draw <span class="op">=</span> ImageDraw.Draw(composite_img_pattern_1)</span>
<span id="cb14-338"><a href="#cb14-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-339"><a href="#cb14-339" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw the texts at different locations</span></span>
<span id="cb14-340"><a href="#cb14-340" aria-hidden="true" tabindex="-1"></a>draw.text((<span class="dv">25</span>, <span class="dv">100</span>), <span class="st">"Morphine-3-Glucuronide"</span>, (<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>), font<span class="op">=</span>font)</span>
<span id="cb14-341"><a href="#cb14-341" aria-hidden="true" tabindex="-1"></a>draw.text((<span class="dv">100</span>, <span class="dv">200</span>), <span class="st">"EDDP, Octanoyl-L-Carnitine, Aspirine"</span>, (<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>), font<span class="op">=</span>font)</span>
<span id="cb14-342"><a href="#cb14-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-343"><a href="#cb14-343" aria-hidden="true" tabindex="-1"></a>composite_img_pattern_1</span>
<span id="cb14-344"><a href="#cb14-344" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-345"><a href="#cb14-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-348"><a href="#cb14-348" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-349"><a href="#cb14-349" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-overlay_example2</span></span>
<span id="cb14-350"><a href="#cb14-350" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Example-2 of interest overlayed with proposed feature names tentativelyi identified by MS-DIAL.</span></span>
<span id="cb14-351"><a href="#cb14-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-352"><a href="#cb14-352" aria-hidden="true" tabindex="-1"></a>composite_img_pattern_2, pattern_df_2 <span class="op">=</span> overlay_untargeted_data(</span>
<span id="cb14-353"><a href="#cb14-353" aria-hidden="true" tabindex="-1"></a>    query_str<span class="op">=</span><span class="st">"(16.5&lt;average_rt_min_&lt;18 and 180&lt;average_mz&lt;250) or (4&lt;average_rt_min_&lt;5.5 and 50&lt;average_mz&lt;100) or (8&lt;average_rt_min_&lt;12 and 50&lt;average_mz&lt;100) or (8.5&lt;average_rt_min_&lt;9 and 250&lt;average_mz&lt;300) or (12&lt;average_rt_min_&lt;14 and 200&lt;average_mz&lt;220) or (18&lt;average_rt_min_&lt;20 and 280&lt;average_mz&lt;350)"</span>,</span>
<span id="cb14-354"><a href="#cb14-354" aria-hidden="true" tabindex="-1"></a>    ms_dial_location<span class="op">=</span><span class="st">"PeakID_0_2023_12_25_17_39_09.csv"</span>,</span>
<span id="cb14-355"><a href="#cb14-355" aria-hidden="true" tabindex="-1"></a>    background_image_location<span class="op">=</span><span class="st">"exp-5-plot_activation.png"</span>,</span>
<span id="cb14-356"><a href="#cb14-356" aria-hidden="true" tabindex="-1"></a>    crop_area<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">1070</span>, <span class="dv">850</span>, <span class="dv">1890</span>),</span>
<span id="cb14-357"><a href="#cb14-357" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-358"><a href="#cb14-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-359"><a href="#cb14-359" aria-hidden="true" tabindex="-1"></a>ImageDraw.Draw(composite_img_pattern_2).text(</span>
<span id="cb14-360"><a href="#cb14-360" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">100</span>, <span class="dv">210</span>), <span class="st">"EDDP, Octanoyl-L-Carnitine, Aspirine"</span>, (<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>), font<span class="op">=</span>font</span>
<span id="cb14-361"><a href="#cb14-361" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-362"><a href="#cb14-362" aria-hidden="true" tabindex="-1"></a>composite_img_pattern_2</span>
<span id="cb14-363"><a href="#cb14-363" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-364"><a href="#cb14-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-367"><a href="#cb14-367" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-368"><a href="#cb14-368" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-overlaid-identifications</span></span>
<span id="cb14-369"><a href="#cb14-369" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: List of 50 metabolites residing at regions of interests that were tentatively identified by MS-DIAL following the overlay of the activation map and the feature map</span></span>
<span id="cb14-370"><a href="#cb14-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-371"><a href="#cb14-371" aria-hidden="true" tabindex="-1"></a>pd.concat([pattern_df_1, pattern_df_2], axis<span class="op">=</span><span class="dv">0</span>).drop(columns<span class="op">=</span>[<span class="st">"sp_10"</span>, <span class="st">"sp_20"</span>]).head(</span>
<span id="cb14-372"><a href="#cb14-372" aria-hidden="true" tabindex="-1"></a>    <span class="dv">50</span></span>
<span id="cb14-373"><a href="#cb14-373" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-374"><a href="#cb14-374" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-375"><a href="#cb14-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-376"><a href="#cb14-376" aria-hidden="true" tabindex="-1"></a>Although this is not the only approach to understanding the principles upon which the model bases its decisions, we believe that combining CNNs with MS/MS fragmentation-based untargeted metabolomics can be used as an example to gain a better understanding of the underlying mechanisms. Conversely, the model can also guide researchers in directing resources and efforts towards identifying specific regions of interest more effectively by performing more targeted measurements, such as MS/MS experiments, since we already know which features play significant roles in distinguishing between the classes. In particular, we recognize the potential benefits of such a model in high-throughput settings where rapid decisions are required, although thorough validation remains essential. This approach could be highly valuable in conjunction with techniques such as untargeted metabolomics and lipidomics, where an unbiased assessment of the phenotype is desirable, especially, when combined with simple sample preparation techniques such as protein precipitation. </span>
<span id="cb14-377"><a href="#cb14-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-378"><a href="#cb14-378" aria-hidden="true" tabindex="-1"></a>While the use of CNNs in high-throughput settings holds immense potential, it is crucial to acknowledge potential drawbacks. Further research is essential to understand how image preprocessing techniques can influence the decision-making process. Currently, we only implemented basic techniques, such as scaling, Gaussian noise reduction, and logarithmic transformation. The number of training examples could also pose a challenge. While transfer learning, as we demonstrated, does not demand a plethora of training examples, we still required over 200 samples for training. Nonetheless, once sufficient number of images are obtained for the initial training, one can retrain the model as new data becomes available. Additionally, it is essential to exercise caution against overfitting. While we implemented several strategies to mitigate overfitting and enhance generalizability, our testing metrics indicate a slight overfitting compared to the validation metrics. However, employing cross-validation may help address this issue. Finally, GPU availability is crucial for neural network training. While transfer learning does not require as much computational resources as training a network from scratch, utilizing a GPU is still recommended. Hence, having access to a GPU is highly advised. This project utilized the freely available GPU (NVIDIA Tesla T4) as part of the Google Colab. Once experimentation is complete, depending on the size of the model selected, inference may be able to be performed using a CPU.</span>
<span id="cb14-379"><a href="#cb14-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-380"><a href="#cb14-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-381"><a href="#cb14-381" aria-hidden="true" tabindex="-1"></a><span class="fu"># Conclusions</span></span>
<span id="cb14-382"><a href="#cb14-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-383"><a href="#cb14-383" aria-hidden="true" tabindex="-1"></a>This project serves as a proof-of-concept demonstrating the application of transfer learning in a binary classification task using untargeted LC/HRMS metabolomics data. We explored various CNN architectures and optimized hyperparameters, such as optimizer type and learning rate scheduler, to achieve optimal performance. Additionally, we utilized TorchCam to visualize the regions of the pseudoimage the neural network focuses on for distinguishing between opioid users and non-users. Furthermore, we incorporated traditional techniques, including spectral deconvolution and MS/MS fragmentation pattern-based identification, to enhance our understanding of the CNN's decision-making process. We hope this open-source project could serve as a valuable guideline for researchers seeking to develop CNN based applications following rigorous validation. We could also envision a system, for example, where positive instances identified by the initial network could be further evaluated by a second one capable of estimating quantities using a regression model with mean squared error loss. Overall, this project aims to demonstrate the practical application of untargeted metabolomics data in combination with modern techniques such as computer vision and neural networks. We believe that the primary obstacle hindering the widespread adoption of metabolomics and lipidomics is not sample preparation or instrumentation but the human expertise required for data interpretation and analysis. We hope that projects like DeepLCMS encourage researchers to embrace novel resources and empower a broader audience to benefit from recent technological advancements.</span>
<span id="cb14-384"><a href="#cb14-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-385"><a href="#cb14-385" aria-hidden="true" tabindex="-1"></a><span class="fu"># Get in touch</span></span>
<span id="cb14-386"><a href="#cb14-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-387"><a href="#cb14-387" aria-hidden="true" tabindex="-1"></a>Did this project help with your research? Do you have any ideas for making it better? Get in touch! We would love to hear from you.</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">Copyright 2024, Adam Cseresznye</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>



</body></html>