[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Coming soon!\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Lets-Plot/2023_08_05_How_to_use_the_Lets_Plot_library_by_JetBrains.html",
    "href": "posts/Lets-Plot/2023_08_05_How_to_use_the_Lets_Plot_library_by_JetBrains.html",
    "title": "How to use the Lets-Plot library by JetBrains",
    "section": "",
    "text": "When I embarked on my data science journey, due to my academics background I quickly gravitated towards the R programming language. Like many R novices, I began with Hadley Wickham‚Äôs R for Data Science book which introduced me to the wonderful ggplot2 library. As my interest in machine learning grew, I made the switch to Python. Nowadays, for most of my data plotting needs, I rely mainly on matplotlib or seaborn. Though I love these libraries, their multiple ways of accomplishing the same tasks can be a bit cumbersome and challenging to learn at first.\nThat‚Äôs why in this article, I‚Äôm excited to introduce you to the Lets-Plot library by JetBrains. It is the closest you can get to ggplot‚Äôs syntax while using Python. While some traditional Python users might find the syntax a bit unfamiliar initially, I‚Äôm here to make a case for this fantastic library and hopefully inspire you to incorporate it into your day-to-day data science activities.\nTo showcase (some of) the key features of Lets-Plot, we will be utilizing the penguins dataset üêß from Github.\nWithout further ado, let‚Äôs dive right in and discover the power and beauty of Lets-Plot! üêçüìä\n\n# Installation\n# pip install lets-plot \n\n\n# Import libraries\nimport numpy as np\nimport pandas as pd\nfrom lets_plot import *\nfrom lets_plot.mapping import as_discrete\nLetsPlot.setup_html()\n\n\n            \n            \n            \n\n\n\naddress='https://gist.githubusercontent.com/slopp/ce3b90b9168f2f921784de84fa445651/raw/4ecf3041f0ed4913e7c230758733948bc561f434/penguins.csv'\ndf=pd.read_csv(address)\ndf.head()\n\n\n\n\n\n\n\n\nrowid\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\n1\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\n2\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\n3\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\n4\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\n5\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n\n\n\n\n\n\nSyntax Similarities: Lets-Plot and ggplot2\nFor our first exercise, I thought it would be beneficial to replicate a basic plot inspired by Hadley‚Äôs book. When comparing my code here with the one presented by him, you‚Äôll notice that there is very little difference between the two. The syntax is nearly identical, making it a smooth transition from ggplot to Lets-Plot.\nNow, let‚Äôs take a closer look at the code. In the ggplot function, we define the DataFrame we‚Äôll be working with, and the aesthetic mappings are set at the global level. We assign the values for the x and y axes, as well as the color argument, which groups the data based on the categorical variable representing the three different penguin species: Adelie, Gentoo, and Chinstrap. This color parameter is quite similar to seaborn‚Äôs hue, making it easy for those familiar with seaborn to adapt to Lets-Plot seamlessly.\nAfter the ggplot() function sets the global aesthetic mappings, the geom_point() function comes into play and draws the points defined by the x and y parameters, effectively creating a scatter plot. These points represent the data points from the penguins dataset, with x and y coordinates corresponding to the specified variables.\nAdditionally, we enhance the plot by using geom_smooth(method=‚Äòlm‚Äô), which adds a smoothed conditional mean. The lm method stands for ‚Äòlinear model,‚Äô indicating that the smoothing is based on a linear regression. This smoothed line helps reveal trends and patterns in the data, making it easier to observe any overall relationships between the variables.\nLet‚Äôs continue exploring more of what Lets-Plot has in store for us! üìäüêßüåà\n\n(ggplot(df,\n        aes(x='flipper_length_mm',\n            y = 'body_mass_g',\n            color='species'\n           )\n       )\n + geom_point() # Draw points defined by an x and y coordinate, as for a scatter plot.\n + geom_smooth(method='lm') # Add a smoothed conditional mean. ‚Äòlm‚Äô stands for 'linear model' as Smoothing method\n) \n\n   \n   \n\n\nIn the previous example, we highlighted the importance of placing the color parameter at the global level, which grouped the data by the three penguin species and showed separate regression lines for each group. However, if we prefer to depict the regression line for the entire dataset, regardless of the group association, we can do so just as easily. All we need to do is remove the color parameter from the aesthetics of the ggplot function and place it solely in the geom_point.\nAdditionally, to enhance the plot further, we can properly label the x and y axes, add a title and subtitle. With these simple adjustments, we can achieve the same output as Hadley‚Äôs original code, with little to no modification.\n\n(ggplot(df, \n        aes(x='flipper_length_mm',\n            y = 'body_mass_g',\n           )\n       )\n + geom_point(aes(color='species', shape='species'))\n + geom_smooth(method='lm')\n + labs(\n    title = \"Body mass and flipper length\",\n    subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n    x = \"Flipper length (mm)\", y = \"Body mass (g)\",\n  ) \n + scale_color_viridis() # lets-plots equivalent of the scale_color_colorblind()\n) \n\n   \n   \n\n\n\n\nVisualizing Data Based on Categorical Variables\nLets-Plot provides numerous options to showcase our data using categorical variables. From bar plots, box plots, and violin plots to pie charts, the possibilities are diverse. You can check out their API for reference. Let‚Äôs explore some examples to demonstrate the versatility of Lets-Plot in visualizing categorical data.\n\npenguin_bar = (ggplot(df,aes(x='species'))\n               + geom_bar()\n              )\n\npenguin_box = (ggplot(df,aes(x = 'species', y = 'body_mass_g'))\n               + geom_boxplot()\n              )\n\npenguin_density = (ggplot(df,aes('body_mass_g', color='species', fill='species'))\n                   + geom_density(alpha=0.5)\n                  )\n\npenguin_rel_frequency = (ggplot(df,aes(x = 'island', fill = 'species'))\n                         + geom_bar(position='fill')\n                        )\ngggrid([penguin_bar, penguin_box, penguin_density, penguin_rel_frequency], ncol=2)\n\n   \n   \n\n\n\n\nIncorporate Multiple Variables with facet_wrap\nSo far we‚Äôve discovered how easy it is to plot data based on a single categorical variable. However, what if we want to depict relationships involving two or more categorical variables? That‚Äôs where facet_wrap comes in handy. This versatile function bears resemblance to similar functions found in seaborn or ggplot2 libraries.\nTo unlock the potential of facet_wrap, we simply need to define aesthetics, which can either be global or local to the mapping function. Then, we can use facet_wrap with the relevant categorical variable we want to visualize. It‚Äôs as simple as that!\n\n(ggplot(df, aes(x = 'flipper_length_mm', y = 'body_mass_g'))  \n + geom_point(aes(color = 'species', shape = 'species'), size = 4) \n + facet_wrap('island', nrow=1)\n + labs(title = \"Body mass and flipper length based on island\",\n        subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n        x = \"Flipper length (mm)\", y = \"Body mass (g)\",\n       )\n + theme(plot_title=element_text(size=20,face='bold'))\n + ggsize(1000,500)\n)\n\n   \n   \n\n\n\n\n\n\nReordering Categorical Variables Based On Statistics\nWhen visualizing data, a task I frequently encounter is ordering categorical variables in either ascending or descending order, based on statistics like median or mean. In my previous point on ‚ÄúVisualizing Data Based on Categorical Variables,‚Äù you noticed that the boxplot displayed categories in an unordered manner. However, consider how we can present them in an ascending order, determined by the median. This not only enhances the aesthetics of the plot but also provides valuable insights into the relationships among the categories.\n\n(ggplot(df,aes(as_discrete('species', order=1, order_by='..middle..'), \n               y = 'body_mass_g'))\n + geom_boxplot()\n)\n\n   \n   \n\n\nBy incorporating the as_discrete function, specifying the column, the ordering direction (1 for ascending, -1 for descending), and setting the order_by variable to middle (representing the median), the plot has become significantly more informative. This simple addition has allowed us to organize the categorical variables in a meaningful manner, improving the visualization‚Äôs clarity and aiding in the interpretation of relationships among the categories.\n\n\nChaining Pandas Code with Lets Plot Visualization\nOne of the best features of the pandas library is its remarkable customizability. With the help of the pd.pipe function, we can seamlessly integrate any of our own functions into method chains, as long as they return a DataFrame or Series. This opens up exciting possibilities to fully incorporate Lets-Plot into our code, just like pandas‚Äô own built-in plotting functionality.\nWhile Lets-Plot may be slightly more verbose than pandas plotting, it offers significantly more flexibility and freedom for customization. Not to mention that some may consider it visually more appealing. With Lets-Plot integrated into our pandas code, we can effortlessly create stunning and informative plots, making data analysis an even more enjoyable experience.\n\n(df\n .groupby('species')\n [['body_mass_g', 'flipper_length_mm']]\n .mean()\n .reset_index()\n .pipe(lambda df: (ggplot(df)\n                   + geom_pie(aes(slice='body_mass_g', fill='species'), \n                              stat='identity',size=30, hole=0.2, stroke=1.0,\n                              labels=layer_labels().line('@body_mass_g').format('@body_mass_g', '{.0f}').size(20)\n                             )\n                   + labs(title = \"Body mass based on species\",\n                          subtitle = \"Representing how Lets-Plot can be used with pd. pipe\",\n                          x = \"\", y = \"\",\n                         )\n                   + theme(axis='blank',\n                          plot_title=element_text(size=20,face='bold'))\n                   + ggsize(500,400)\n                  )\n )\n)\n\n   \n   \n\n\nThat‚Äôs a wrap on the Lets-Plot library! There‚Äôs so much more to explore and learn about this powerful tool. I hope you found this introduction helpful and consider integrating Lets-Plot into your daily data analysis routine.\nHappy coding üêçüñ•Ô∏èüîçüöÄ"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "Hi there! Welcome to my website! \n\nI am Adam, a PhD candidate at the University of Antwerp focusing on the field of environmental toxicology. If you‚Äôre curious about my academic journey and accomplishments, feel free to explore Pubmed for more information.\nI would like to dedicate this website to my pursuit of mastering data science. Here, you‚Äôll find projects, blog posts, and valuable insights to inspire your own endeavors.  Remember:\n\n‚ÄúConstant dripping wears away a stone‚Äù\n\nsuccess comes from persistence and determination. Let‚Äôs embark on this data-driven journey together!"
  },
  {
    "objectID": "articles.html",
    "href": "articles.html",
    "title": "Articles",
    "section": "",
    "text": "How to use the Lets-Plot library by JetBrains\n\n\n\n\n\n\n\nLets-Plot\n\n\nggplot2\n\n\n\n\n\n\n\n\n\n\n\nAug 5, 2023\n\n\nAdam Cseresznye\n\n\n\n\n\n\n  \n\n\n\n\nTop 10 things I learned from the book Effective Pandas by Matt Harrison\n\n\n\n\n\n\n\nPandas\n\n\n\n\n\n\n\n\n\n\n\nAug 2, 2023\n\n\nAdam Cseresznye\n\n\n\n\n\n\n  \n\n\n\n\nüèÜü•â3rd place at Kaggle‚Äôs Classification with a Tabular Vector Borne Disease Dataset\n\n\n\n\n\n\n\ncelebration\n\n\nKaggle\n\n\n\n\n\n\n\n\n\n\n\nJun 4, 2023\n\n\nAdam Cseresznye\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Kaggle/Kaggle.html",
    "href": "posts/Kaggle/Kaggle.html",
    "title": "üèÜü•â3rd place at Kaggle‚Äôs Classification with a Tabular Vector Borne Disease Dataset",
    "section": "",
    "text": "Greetings everyone! I am thrilled to announce that I have achieved a significant milestone in my data science journey. I am delighted to share that I secured the 3rd position in Kaggle‚Äôs ‚ÄúClassification with a Tabular Vector Borne Disease Dataset‚Äù competition, surpassing 931 talented contenders! If you‚Äôre interested in learning more about my approach to the competition, I invite you to check out my detailed solution on the Kaggle discussion page.\n\n\n\n\n\nIn this exciting competition, the aim was to predict the three most probable diseases out of a total of 11 vector-borne diseases, including well-known names such as Zika, Yellow Fever, Malaria, and more. Our task involved analyzing the provided symptoms and using the MPA@3 evaluation metric for accurate predictions.\nMy journey in the world of Kaggle has been a relatively short one, with this competition marking my third participation. To give you some context, let me share a bit about my previous placements:\n\nReaching this point has been a long and rewarding endeavor. Over the past two years, I have been studying this topic in my free time while also pursuing a full-time Ph.D.¬†research. It has been an exciting balancing act!\nParticipating in Kaggle competitions has been an invaluable learning experience for me. It has allowed me to apply my knowledge, explore new techniques, and connect with fellow data enthusiasts. I am deeply grateful to the organizers of Kaggle for providing a platform that fosters growth and healthy competition.\nWhile this achievement brings a sense of accomplishment, I am already diving into new competitions and expanding my skills. Currently, I am particularly interested in reproducible data and developing robust architectures for machine learning projects to enhance code reproducibility. In fact, I am considering writing a blog on this often overlooked topic.\nStay tuned for more :)\n#DataScience #KaggleCompetition #VectorBorneDiseases #MachineLearning #ReproducibleData"
  },
  {
    "objectID": "posts/Top 10 things I learned from Effective Pandas/2023_08_02-Top_10_things_I_learned_from_Effective_Pandas.html",
    "href": "posts/Top 10 things I learned from Effective Pandas/2023_08_02-Top_10_things_I_learned_from_Effective_Pandas.html",
    "title": "Top 10 things I learned from the book Effective Pandas by Matt Harrison",
    "section": "",
    "text": "Effective Pandas by Matt Harrison is a guide to the Pandas library, a powerful Python tool for data manipulation and analysis. The book covers a wide range of topics, from basic data structures to advanced techniques for data cleaning, transformation, and visualization.\nI have found Effective Pandas to be a captivating and thought-provoking read. The book offers a genuinely unique take on data wrangling, putting a great emphasis on the utility of chaining methods and utilizing the lambda function. I have found these ideas to be so useful and practical that I have revisited the book multiple times just to make sure I keep them fresh in my memory. I must have read the book from back to back at least 3-4 times.\nIn this article, I will share the top 10 (+1) things I learned from Effective Pandas. These are the concepts and techniques that I found to be the most useful and practical.\nWe will use the Real World Smartphone‚Äôs Dataset by Abhijit Dahatonde from Kaggle. Let‚Äôs get to it.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\npd.set_option('display.max_columns', None)\n\n\nLoad the dataset\n\ndf=pd.read_csv('smartphones.csv')\n# some info about the dataframe, such as dimensions and dtypes of columns\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 980 entries, 0 to 979\nData columns (total 22 columns):\n #   Column                     Non-Null Count  Dtype  \n---  ------                     --------------  -----  \n 0   brand_name                 980 non-null    object \n 1   model                      980 non-null    object \n 2   price                      980 non-null    int64  \n 3   avg_rating                 879 non-null    float64\n 4   5G_or_not                  980 non-null    int64  \n 5   processor_brand            960 non-null    object \n 6   num_cores                  974 non-null    float64\n 7   processor_speed            938 non-null    float64\n 8   battery_capacity           969 non-null    float64\n 9   fast_charging_available    980 non-null    int64  \n 10  fast_charging              769 non-null    float64\n 11  ram_capacity               980 non-null    int64  \n 12  internal_memory            980 non-null    int64  \n 13  screen_size                980 non-null    float64\n 14  refresh_rate               980 non-null    int64  \n 15  num_rear_cameras           980 non-null    int64  \n 16  os                         966 non-null    object \n 17  primary_camera_rear        980 non-null    float64\n 18  primary_camera_front       975 non-null    float64\n 19  extended_memory_available  980 non-null    int64  \n 20  resolution_height          980 non-null    int64  \n 21  resolution_width           980 non-null    int64  \ndtypes: float64(8), int64(10), object(4)\nmemory usage: 168.6+ KB\n\n\n\n\nTip #1: Use pd.assign more extensively\nThe pd.assign method in Pandas is a very powerful tool that can be used to create new columns, modify existing columns, or both. It is a very versatile method that can be used in a variety of ways.\nOne of the most important benefits of using the assign method is that it can be incorporated into method chaining. This means that you can chain multiple assign methods together to create a more concise and readable code. Another benefit of using the assign method is that it completely sidesteps the infamous SettingWithCopyWarning. This warning is often triggered when you try to modify an existing column in a DataFrame. However, the assign method creates a new DataFrame, so there is no need to worry about this warning.\nProblem statement: Let‚Äôs say we would like to capitalize the brand names located in the brand_name column as well as calculate the Pixels Per Inch (PPI). PPI can be calculated following the equation described by the Pixel density page on Wikipedia.\n\n(df\n .assign(brand_name=lambda df: df.brand_name.str.capitalize(), # capitalizes the brand names \n         PPI=lambda df: (np.sqrt(np.square(df.resolution_height) + np.square(df.resolution_width))\n                         .div(df.screen_size)\n                         .round(1)\n                        )\n        )\n .loc[:, ['brand_name','model','PPI']]\n .sort_values(by='PPI',ascending=False)\n .head(5)\n)\n\n\n\n\n\n\n\n\nbrand_name\nmodel\nPPI\n\n\n\n\n689\nSony\nSony Xperia 1 IV (12GB RAM + 512GB)\n642.6\n\n\n696\nSony\nSony Xperia Pro-I\n642.6\n\n\n688\nSony\nSony Xperia 1 II\n642.6\n\n\n655\nSamsung\nSamsung Galaxy S20\n566.0\n\n\n656\nSamsung\nSamsung Galaxy S20 5G\n566.0\n\n\n\n\n\n\n\n\n\nTip #2: Simplify the management of multiple if-else conditions using np.select\nIf our goal is to incorporate if-else logic seamlessly into our code, we can effortlessly achieve this using either pd.mask or pd.where. Yet, what approach should we adopt when we need to evaluate multiple conditions instead of just two? In such situations, we have two options: we can either employ successive pd.mask or pd.where calls, or we can take advantage of the np.select function as an alternative solution.\nProblem statement: We want to identify the top 3 and top 5 most popular processor brands in smartphones. To do this, we will first create two lists, one for the top 3 brands and one for the top 5 brands. Any processor brand that is not in either of these lists will be categorized as ‚ÄúOther‚Äù.\n\n# Let's create the two lists that contain the top3 and top5 brand names\ntop3=df.processor_brand.value_counts().head(3).index\ntop5=df.processor_brand.value_counts().head(5).index\nprint(f'Top 3 most popular processors: {top3.tolist()}')\nprint(f'Top 5 most popular processors: {top5.tolist()}')\n\nTop 3 most popular processors: ['snapdragon', 'helio', 'dimensity']\nTop 5 most popular processors: ['snapdragon', 'helio', 'dimensity', 'exynos', 'bionic']\n\n\n\n'''\nHere's an example that employs two successive pd.where calls:\nIn the first pd.where call, it checks whether the brand is in the top 3; if not, it assigns the label \"Top5\" to it.\nThen, in the second call, it checks if the value is in the top 5; if not, it appends the category \"Other\".\nAs you can see, the logic can become intricate and difficult to grasp, especially when dealing with numerous conditions, \nmaking the code cumbersome and hard to manage.\n'''\n(df\n .assign(frequency=lambda df: df.processor_brand\n         .where(df.processor_brand.isin(top3), other = 'Top5')\n         .where(df.processor_brand.isin(top5), other = 'Other')\n        )\n .loc[:, 'frequency']\n .value_counts()\n)\n\nsnapdragon    413\nhelio         201\ndimensity     177\nTop5           95\nOther          94\nName: frequency, dtype: int64\n\n\n\n'''\n Now Let's see np.select!\n It simplifies the process significantly. By providing a list of conditions we want to evaluate and their \n corresponding values if the condition evaluates to True, we can handle multiple conditions effortlessly. \n Additionally, we can specify a default value if none of the conditions evaluates to True, making the code \n much more straightforward and easier to manage. \n'''\n(df\n .assign(frequency=lambda df: np.select(condlist=[df.processor_brand.isin(top3), df.processor_brand.isin(top5)],\n                                        choicelist=[df.processor_brand,'Top5'],\n                                        default='Other'\n                                       )\n        )\n .loc[:, 'frequency']\n .value_counts()\n)\n\nsnapdragon    413\nhelio         201\ndimensity     177\nTop5           95\nOther          94\nName: frequency, dtype: int64\n\n\n\n\nTip #3 Filter rows or columns with the combination of pd.loc and lambda\nSome experienced Pandas users might consider the following concept trivial, but it was an eye-opener for me after reading the book. It turns out that combining pd.loc and lambda (or any custom functions) allows us to filter both rows and columns, depending on our specific needs.\nProblem statement: We are interested in identifying phones with a battery capacity greater than 5000mAh.\n\n(df\n .loc[lambda df: df.battery_capacity.gt(5000),['model', 'battery_capacity']] # here we use pd.gt() to select values greater than 5000\n .sort_values(by='battery_capacity')\n)\n\n\n\n\n\n\n\n\nmodel\nbattery_capacity\n\n\n\n\n70\nGoogle Pixel 6 Pro (12GB RAM + 256GB)\n5003.0\n\n\n69\nGoogle Pixel 6 Pro\n5003.0\n\n\n977\nXiaomi Redmi Note 9 Pro Max\n5020.0\n\n\n922\nXiaomi Redmi Note 10 Lite\n5020.0\n\n\n923\nXiaomi Redmi Note 10 Lite (4GB RAM + 128GB)\n5020.0\n\n\n...\n...\n...\n\n\n624\nSamsung Galaxy F63\n7000.0\n\n\n411\nOukitel WP9\n8000.0\n\n\n410\nOukitel WP21\n9800.0\n\n\n409\nOukitel WP19\n21000.0\n\n\n58\nDoogee V Max\n22000.0\n\n\n\n\n113 rows √ó 2 columns\n\n\n\n\n\nTip #4 Rename multiple columns effortlessly with rename and replace\nOK. This is a big one for me. I used this one multiple times already. The title pretty much says it all. Let‚Äôs say our column names contain spaces, which makes column selection by attribute access pretty much impossible. Now, what do we do? Well‚Ä¶Let‚Äôs see.\nProblem statement: We would like to remove all the underscores from our column names.\n\ndf.columns # original column names for reference\n\nIndex(['brand_name', 'model', 'price', 'avg_rating', '5G_or_not',\n       'processor_brand', 'num_cores', 'processor_speed', 'battery_capacity',\n       'fast_charging_available', 'fast_charging', 'ram_capacity',\n       'internal_memory', 'screen_size', 'refresh_rate', 'num_rear_cameras',\n       'os', 'primary_camera_rear', 'primary_camera_front',\n       'extended_memory_available', 'resolution_height', 'resolution_width'],\n      dtype='object')\n\n\n\n# column names after replacing underscores\n(df\n .rename(columns = lambda x: x.replace('_', ''))\n .columns\n)\n\nIndex(['brandname', 'model', 'price', 'avgrating', '5Gornot', 'processorbrand',\n       'numcores', 'processorspeed', 'batterycapacity',\n       'fastchargingavailable', 'fastcharging', 'ramcapacity',\n       'internalmemory', 'screensize', 'refreshrate', 'numrearcameras', 'os',\n       'primarycamerarear', 'primarycamerafront', 'extendedmemoryavailable',\n       'resolutionheight', 'resolutionwidth'],\n      dtype='object')\n\n\n\n\nTip #5: Use pd.clip to easily remove outliers\n\n# First, we'll identify the phone brands with the most number of handsets present in our dataset.\"\ntop10_brand_names = (df\n                     .brand_name\n                     .value_counts()\n                     .head(10)\n                     .index\n                     .tolist()\n                    )\nprint(top10_brand_names)\n\n# Then we will sort them based on median price\ntop10_brand_names_ordered = (df\n                             .loc[lambda x: x.brand_name.isin(top10_brand_names),['brand_name', 'price']]\n                             .groupby('brand_name')\n                             .median()\n                             .sort_values(by='price')\n                             .index\n                             .to_list()\n                            )\nprint(top10_brand_names_ordered)\n\n['xiaomi', 'samsung', 'vivo', 'realme', 'oppo', 'motorola', 'apple', 'oneplus', 'poco', 'tecno']\n['tecno', 'poco', 'realme', 'xiaomi', 'motorola', 'vivo', 'oppo', 'samsung', 'oneplus', 'apple']\n\n\n\nfig, axs = plt.subplots(1,2, figsize=(10,5))\n\n# For reference, this is what our box plot looks if we leave in the outlier values\n(df\n .loc[lambda x: x.brand_name.isin(top10_brand_names),['brand_name', 'price']] # filter rows based on top10_brand_names and select columns\n .pivot(columns='brand_name',values='price') # pivot to get the brand names on the x axis later on\n .loc[:, top10_brand_names_ordered] # order the columns based on median price\n .plot\n .box(title='Top 10 most popular smartphone brands \\n -outlier values included-', \n      rot=90,\n      ax=axs[0]\n     )\n)\n\n(df\n .loc[lambda x: x['brand_name'].isin(top10_brand_names), ['brand_name', 'price']]\n .pivot(columns='brand_name', values='price')\n .loc[:, top10_brand_names_ordered]\n .pipe(lambda df: df.assign(**{col : df[col].clip(lower=df[col].quantile(0.05), # this is called dictionary unpacking\n                                                  upper=df[col].quantile(0.95))\n                              for col in df.columns}))\n .plot\n .box(title='Top 10 most popular smartphone brands \\n -only values between 5th and 95th percentiles included-',\n      rot=90,\n      ax=axs[1]\n     )\n)\naxs[0].set(ylabel='Price in local currency')\nplt.tight_layout()\n\n\n\n\n\n\nTip #6: Find corrupted entries with str.extract\nHow often have you encountered the situation where, for some reason, a column that is expected to only contain numerical values displays object as its dtype? This often indicates the presence of some string values mixed within the column. It would be beneficial to promptly identify all the erroneous values, correct them, and proceed with our analysis smoothly. Let‚Äôs explore what we can do in such scenarios.\nProblem statement: We would like to identify any cells in a specific column that contain non-numerical values.\n\ndf_bad_values = df.copy(deep=True) # let's prepare a copy of the original dataframe \n\n# let's modify some of the values in the price column randomly:\ndf_bad_values.loc[np.random.randint(0, high=df_bad_values.shape[0], size=10), 'price'] = '.'\ndf_bad_values.loc[np.random.randint(0, high=df_bad_values.shape[0], size=10), 'price'] = '-'\ndf_bad_values.loc[np.random.randint(0, high=df_bad_values.shape[0], size=10), 'price'] = '*'\ndf_bad_values.info() # the modified dataframe's price column now returns object as dtype\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 980 entries, 0 to 979\nData columns (total 22 columns):\n #   Column                     Non-Null Count  Dtype  \n---  ------                     --------------  -----  \n 0   brand_name                 980 non-null    object \n 1   model                      980 non-null    object \n 2   price                      980 non-null    object \n 3   avg_rating                 879 non-null    float64\n 4   5G_or_not                  980 non-null    int64  \n 5   processor_brand            960 non-null    object \n 6   num_cores                  974 non-null    float64\n 7   processor_speed            938 non-null    float64\n 8   battery_capacity           969 non-null    float64\n 9   fast_charging_available    980 non-null    int64  \n 10  fast_charging              769 non-null    float64\n 11  ram_capacity               980 non-null    int64  \n 12  internal_memory            980 non-null    int64  \n 13  screen_size                980 non-null    float64\n 14  refresh_rate               980 non-null    int64  \n 15  num_rear_cameras           980 non-null    int64  \n 16  os                         966 non-null    object \n 17  primary_camera_rear        980 non-null    float64\n 18  primary_camera_front       975 non-null    float64\n 19  extended_memory_available  980 non-null    int64  \n 20  resolution_height          980 non-null    int64  \n 21  resolution_width           980 non-null    int64  \ndtypes: float64(8), int64(9), object(5)\nmemory usage: 168.6+ KB\n\n\n\n# let's find the corrupted values easily:\n(df_bad_values\n .price\n .str.extract(r'([^a-zA-Z])') # returns NON-matching alphabetical characters\n .value_counts()\n)\n\n*    10\n-    10\n.    10\ndtype: int64\n\n\n\n\nTip #7: Sort values based on the key parameter\nThe pd.sort_values function surprised me with its versatility. Previously, I had only used its by, axis, and ascending parameters for sorting. However, Matt‚Äôs book introduced me to its key parameter, which allows us to apply any function to sort the values. The only constraint is that the function must return a Series.\nProblem statement: For whatever reason, we would like to sort the phone model names in ascending order based on their second letter.\n\n(df\n .iloc[:, 1:3]\n .sort_values(by='model',\n              key = lambda x: x.str[1],\n              ascending = True\n             )\n .head(10)\n)\n\n\n\n\n\n\n\n\nmodel\nprice\n\n\n\n\n55\nCAT S22 Flip\n14999\n\n\n697\nTCL Ion X\n8990\n\n\n195\nLG V60 ThinQ\n79990\n\n\n196\nLG Velvet 5G\n54999\n\n\n197\nLG Wing 5G\n54999\n\n\n108\niKall Z19 Pro\n8099\n\n\n107\niKall Z19\n7999\n\n\n106\niKall Z18\n6799\n\n\n54\nBLU F91 5G\n14990\n\n\n413\nPOCO C31 (4GB RAM + 64GB)\n7499\n\n\n\n\n\n\n\n\n\nTip #8: Reference an existing variable inside pd.query with @\nThis resembles Tip #4, as it‚Äôs a technique I frequently use. Since reading Matt‚Äôs book, I have started using pd.query extensively to filter rows based on values, instead of relying on .loc or .iloc. In case you choose to adopt pd.query as well, it‚Äôs essential to be aware of its capability to use ‚Äú@‚Äù to reference variables in the environment. This feature enhances its flexibility and makes it even more convenient to apply in various data filtering scenarios.\nProblem statement: Our objective is to identify phones that meet three specific criteria: being priced below the average market price, having more processor cores than the average, and possessing a battery capacity greater than the average.\n\naverage_price=df.price.mean()\naverage_cores=df.num_cores.mean()\naverage_battery=df.battery_capacity.mean()\n\n(df\n .query(\"(price &lt;= @average_price) and (num_cores &gt;= @average_cores) and (battery_capacity &gt;= @average_battery)\")\n .iloc[:,:3]\n .sort_values(by='price')\n .head()\n)\n\n\n\n\n\n\n\n\nbrand_name\nmodel\nprice\n\n\n\n\n498\nrealme\nRealme C30\n5299\n\n\n179\nitel\nitel Vision 3 (2GB RAM + 32GB)\n5785\n\n\n202\nmicromax\nMicromax IN 2C\n5999\n\n\n729\ntecno\nTecno Spark Go 2022\n6249\n\n\n499\nrealme\nRealme C30 (3GB RAM + 32GB)\n6299\n\n\n\n\n\n\n\n\n\nTip #9: Gain more insights by using style.background_gradient\nBeing a visual creature, I often struggle to comprehend data quickly just by examining the raw table alone. Fortunately, with the help of style.background_gradient, similar to the heatmap function in the seaborn library, we can represent cells, in terms of color gradient, based on their values. This enables us to identify trends and patterns in our data swiftly, making data analysis more intuitive and insightful.\nProblem statement: Our goal is to identify the overall trends related to key descriptors found among the Top 10 smartphone brands, aiming to determine which brand offers the most value for our money.\n\n(df\n .query(\"brand_name.isin(@top10_brand_names_ordered)\") # filter rows based on top10 brands\n .groupby(['brand_name',])\n [['avg_rating', 'processor_speed', 'ram_capacity', \n   'screen_size', 'battery_capacity', 'price']]\n .mean()\n .sort_values(by='avg_rating')\n .transpose()\n .rename(columns=str.capitalize) # capitalize brand names\n .style\n .set_caption(\"Key descriptors of the Top 10 smartphone brands\")\n .format(precision=1)\n .background_gradient(cmap = 'vlag', axis = 1)\n .set_table_styles([\n    {'selector': 'td', 'props': 'text-align: center;'},\n     {'selector': 'caption','props': 'font-size:1.5em; font-weight:bold;'}\n     ,]\n )\n)\n\n\n\n\nKey descriptors of the Top 10 smartphone brands\n\n\nbrand_name\nTecno\nRealme\nApple\nVivo\nPoco\nSamsung\nOppo\nXiaomi\nMotorola\nOneplus\n\n\n\n\navg_rating\n7.4\n7.6\n7.7\n7.7\n7.9\n7.9\n7.9\n7.9\n8.0\n8.2\n\n\nprocessor_speed\n2.1\n2.3\n3.1\n2.4\n2.5\n2.4\n2.5\n2.4\n2.5\n2.7\n\n\nram_capacity\n5.4\n5.7\n5.3\n6.7\n6.1\n6.5\n7.5\n6.4\n6.1\n8.2\n\n\nscreen_size\n6.7\n6.5\n6.1\n6.5\n6.6\n6.6\n6.6\n6.6\n6.6\n6.6\n\n\nbattery_capacity\n5333.9\n4903.1\n3527.2\n4703.7\n5009.4\n4917.4\n4667.2\n4957.6\n4863.1\n4759.5\n\n\nprice\n14545.4\n17461.4\n95966.5\n26782.4\n18479.2\n36843.0\n29650.0\n27961.1\n24099.9\n35858.6\n\n\n\n\n\n\n\nTip #10: Use pd.pipe to include any functions in our chain\nOne of the most valuable lessons I learned from Effective Pandas is the importance of arranging my code in a chain. Although it may feel somewhat restrictive at first, once you overcome the initial hurdles, you‚Äôll realize that your code becomes more readable and easier to understand. The need to invent unique names for temporary variables is completely eliminated, making coding a much happier experience.\nIn the chaining world, you often find yourself wanting to use various functions that are not explicitly designed for chaining. However, there‚Äôs good news! You can still achieve this. The pd.pipe function comes to the rescue, allowing you to use any function as long as it returns a Series or DataFrame. It‚Äôs a flexible solution that empowers you to seamlessly integrate different functions into your chaining workflow, making your data manipulation more efficient and enjoyable.\nProblem statement: We aim to visualize the impact of RAM capacity on user satisfaction. To achieve this, we will utilize the sns.lmplot function, which plots the data and corresponding regression models for the Top 5 phone brands.\n\ntop5_brand_names_ordered = df.brand_name.value_counts().head().index\n\nwith sns.axes_style(\"darkgrid\"):\n    g = (df\n         .query(\"brand_name.isin(@top5_brand_names_ordered)\") # filter rows based on top10 brands\n         [['brand_name', 'avg_rating', 'ram_capacity']]\n         .assign(brand_name=lambda df: df.brand_name.str.capitalize())\n         .rename(columns={'brand_name':'Brand name'})\n         .pipe(lambda df: sns.lmplot(data=df,\n                                     x='ram_capacity',\n                                     y='avg_rating',\n                                     hue='Brand name',\n                                     # height=4,\n                                     # aspect=1.2\n                                    )\n              )\n        )\n\n    g.set(title='Customer satisfaction correlates with RAM capacity', \n          xlabel='RAM capacity',\n          ylabel='User rating'\n         )\nplt.tight_layout()\n\n\n\n\n\n\nTip #10 + 1: Use the ‚Äúmargin‚Äù parameter of pd.crosstab to easily calculate row/column subtotals\nDespite primarily using the pandas groupby function for data aggregation, the pd.crosstab function has an enticing feature: the margin parameter. This option enables us to effortlessly calculate subtotals across rows and columns. Moreover, by normalizing our data, we can gain even more intuition about the questions we want to answer.\nProblem statement: Our objective is to evaluate how RAM capacity impacts user satisfaction across the Top 5 brands. Additionally, we will normalize our data to compare values comprehensively across the entire dataset.\n\n(df\n .query(\"brand_name.isin(@top5_brand_names_ordered)\") # filter rows based on top10 brands\n .assign(brand_name=lambda df: df.brand_name.str.capitalize())\n .pipe(lambda df: pd.crosstab(index=df['ram_capacity'],\n                              columns=df['brand_name'],\n                              values=df['avg_rating'],\n                              aggfunc='mean',\n                              margins=True,\n                              normalize='all'\n                             )\n      )\n .mul(100)\n .round(1)\n)\n\n\n\n\n\n\n\nbrand_name\nOppo\nRealme\nSamsung\nVivo\nXiaomi\nAll\n\n\nram_capacity\n\n\n\n\n\n\n\n\n\n\n2\n0.0\n2.7\n2.8\n2.7\n2.7\n11.5\n\n\n3\n2.9\n2.8\n2.9\n2.9\n2.8\n12.1\n\n\n4\n3.1\n3.2\n3.2\n3.2\n3.2\n13.5\n\n\n6\n3.3\n3.5\n3.5\n3.4\n3.5\n14.8\n\n\n8\n3.7\n3.7\n3.8\n3.7\n3.8\n15.7\n\n\n12\n3.8\n3.9\n3.9\n3.8\n3.9\n16.3\n\n\n16\n3.8\n0.0\n0.0\n0.0\n0.0\n16.2\n\n\nAll\n20.2\n19.6\n20.2\n19.8\n20.2\n100.0\n\n\n\n\n\n\n\nI hope this article has convinced you to pick up Matt Harrison‚Äôs Effective Pandas! There are plenty more exciting ideas in the book beyond the Top 10 I‚Äôve shared here (I didn‚Äôt even get into the fascinating time series part!). I hope you found these insights helpful and inspiring.\nHappy coding üêºüíªüöÄ"
  }
]