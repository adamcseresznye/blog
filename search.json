[
  {
    "objectID": "scientific_output.html",
    "href": "scientific_output.html",
    "title": "Scientific Output",
    "section": "",
    "text": "Publications\n\nYin, S., McGrath, T. J., Cseresznye, A., Bombeke, J., Poma, G., & Covaci, A. (2023). Assessment of silicone wristbands for monitoring personal exposure to chlorinated paraffins (C8-36): A pilot study. Environmental Research, 224, 115526. https://doi.org/10.1016/j.envres.2023.115526\nHuguenard, C., Cseresznye, A., Darcey, T., Nkiliza, A., Evans, J. E., Hazen, S. L., Mullan, M., Crawford, F., & Abdullah, L. (2023). Age and APOE affect L-carnitine system metabolites in the brain in the APOE-TR model. Frontiers in Aging Neuroscience, 14. https://doi.org/10.3389/fnagi.2022.1059017\nHuguenard, C., Cseresznye, A., Evans, J. E., Darcey, T., Nkiliza, A., Keegan, A. P., Luis, C. A., Bennett, D. A., Arvanitakis, Z., Yassine, H. N., Mullan, M., Crawford, F., & Abdullah, L. (2023). APOE ε4 and Alzheimer’s disease diagnosis associated differences in L-carnitine, GBB, TMAO, and acylcarnitines in blood and brain. Current Research in Translational Medicine, 71(1), 103362. https://doi.org/10.1016/j.retram.2022.103362\nNkiliza, A., Parks, M., Cseresznye, A., Oberlin, S., Evans, J. E., Darcey, T., Aenlle, K., Niedospial, D., Mullan, M., Crawford, F., Klimas, N. G., & Abdullah, L. (2021). Sex-specific plasma lipid profiles of ME/CFS patients and their association with pain, fatigue, and cognitive symptoms. Journal of Translational Medicine, 19(1). https://doi.org/10.1186/s12967-021-03035-6\nJoshi, U., Evans, J. E., Pearson, A., Saltiel, N., Cseresznye, A., Darcey, T., Ojo, J., Keegan, A. P., Oberlin, S., Mouzon, B., Paris, D., Klimas, N. G., Sullivan, K., Mullan, M., Crawford, F., & Abdullah, L. (2020). Targeting sirtuin activity with nicotinamide riboside reduces neuroinflammation in a GWI mouse model. Neurotoxicology, 79, 84–94. https://doi.org/10.1016/j.neuro.2020.04.006\nHuguenard, C., Cseresznye, A., Evans, J. E., Oberlin, S., Langlois, H., Ferguson, S., Darcey, T., Nkiliza, A., Dretsch, M. N., Mullan, M., Crawford, F., & Abdullah, L. (2020). Plasma Lipidomic Analyses in Cohorts With mTBI and/or PTSD Reveal Lipids Differentially Associated With Diagnosis and APOE ε4 Carrier Status. Frontiers in Physiology, 11. https://doi.org/10.3389/fphys.2020.00012\nJoshi, U., Pearson, A., Evans, J. E., Langlois, H., Saltiel, N., Ojo, J., Klimas, N. G., Sullivan, K., Keegan, A. P., Oberlin, S., Darcey, T., Cseresznye, A., Raya, B., Paris, D., Hammock, B. D., Vasylieva, N., Hongsibsong, S., Stern, L. J., Crawford, F., . . . Abdullah, L. (2019). A permethrin metabolite is associated with adaptive immune responses in Gulf War Illness. Brain Behavior and Immunity, 81, 545–559. https://doi.org/10.1016/j.bbi.2019.07.015\nWang, M., Palavicini, J. P., Cseresznye, A., & Han, X. (2017). Strategy for Quantitative Analysis of Isomeric Bis(monoacylglycero)phosphate and Phosphatidylglycerol Species by Shotgun Lipidomics after One-Step Methylation. Analytical Chemistry, 89(16), 8490–8495. https://doi.org/10.1021/acs.analchem.7b02058\n\n\n\nConference talks\n\nAdam Cseresznye, Fatima den Ouden, Liesa Engelen, Jasper Bombeke, Giulia Poma, Thomas J McGrath, Roger Peró Gascon, Gwen Falony, Ellen De Paepe, Lieselot Y Hemeryck, Tim S Nawrot, Jeroen Raes, Lynn Vanhaecke, Sarah De Saeger, Marthe De Boevre, Adrian Covaci. Serum Levels of Persistent Organic Pollutants in the Flemish Gut Flora Project Cohort. Dioxin, Maastricht, the Netherlands, 2023.\n\nEmilie M. Hardy, Adam Cseresznye , Paul T.J. Scheepers, Maria Torres Toda, Yu Ait Bamai, Simo Porras , Susana Viegas, Tiina Santonen, Adrian Covaci, An van Nieuwenhuyse, Radu C. Duca and HBM4EU e-waste study team. Assessment of e-waste occupational exposure to PCBs and PBDEs using settled dust and silicone wristbands analysis. Dioxin, Maastricht, the Netherlands, 2023.\n\nS. Yin, T J. McGrath, A. Cseresznye, J. Bombeke, G. Poma, A. Covaci. Assessment of Silicone Wristbands for Monitoring Personal Exposure to Chlorinated Paraffins (C8-36): a Pilot Study. Dioxin, Maastricht, the Netherlands, 2023.\n\nPaul T.J. Scheepers, Susana Viegas, Radu-Corneliu Duca, Jelle Verdonck, Emilie Hardy, Adrian Covaci, Paulien Cleys, Adam Cseresznye, Thomas Goën, Karen S. Galea, Jelle Verdonck, Katrien Poels, Lode Godderis, Elizabeth Leese, Henriqueta Louro, Maria Silva, Sophie Ndaw, Simo Porras, Selma Mahiout, Tiina Santonen. The HBM4EU e-waste study: exploratory survey of worker’s exposure to toxic contaminants. ISES Annual Meeting, Chicago, IL, USA, 2023.\n\nPaulien Cleys, Emilie Hardy, Yu Ait Bamai, Giulia Poma, Adam Cseresznye, Govindan Malarvannan, Paul T.J. Scheepers, Susana Viegas, Simo P. Porras, Tiina Santonen, Lode Godderis, Jelle Verdonck, Katrien Poels, Carla Martins, Maria João Silva, Henriqueta Louro, Inese Martinsone, Lāsma Akūlova, An van Nieuwenhuyse, Martien Graumans, Selma Mahiout, Radu Corneliu Duca, Adrian Covaci, HBM4EU e-waste study: Occupational Exposure of Electronic Waste Workers to Phthalates and Alternative Plasticizers in Europe. ISES Annual Meeting, Chicago, IL, USA, 2023.\n\nAdam Cseresznye, P. Cleys, G. Poma, Y.A. Bamai, E. Hardy, P. Scheepers, S. Viegas, S. Porras, T. Santonen, R.C. Duca, A. Covaci, and HBM4EU E-Waste study group. HBM4EU E-waste study: persistent organic pollutants in e-waste recycling workers in the European Union. ISBM-12, Porto, Portugal, 2023.\n\n\n\nPoster presentations\n\nR. Pero-Gascon, E. Maris, A. Cseresznye, F. den Ouden, L. Engelen, L.Y. Hemeryck, E. De Paepe, A. Vich Vila, S. Moossavi, M. Derrien, G. Poma, M. De Boevre, T.S. Nawrot, J. Raes, A. Covaci, L. Vanhaecke & S. De Saeger. FLEXiGUT: investigating the importance of the exposome in the development and progression of chronic low-grade gut inflammation. Benelux Metabolomics Days, Utrecht, the Netherlands, 2023\nAlicia Macan Schönleben , Thomas McGrath, Shanshan Yin, Adam Cseresznye, Alexander van Nuijs, Giulia Poma, Adrian Covaci. Chlorinated paraffins in seaweed chips – First insights using High-Resolution Mass Spectrometry. Dioxin, Maastricht, the Netherlands, 2023\n\nAurore Nkiliza, Claire Huguenard, Adam Cseresznye, Teresa Darcey, Corbin Bachmeier, James E. Evans, Andrew P. Keegan, Michael Mullan, Fiona Crawford and Laila Abdullah. The APOE e4 allele differentially influences plasma bioactive lipids in soldiers with mild TBI and PTSD, AAIC, 2020.\nHuguenard C, Cseresznye A, Darcey T, Bachmeier C, Evans J E, Cheryl L, Keegan A. P, Mullan M,Crawford F, Abdullah L. APOE4-dependent and independent changes in acylcarnitines indicate lipid metabolic changes early on in Alzheimer’s disease. AAIC, 2020.\nCseresznye A, Huguenard C, Evans JE, Crawford F, Mullan M, Abdullah L. LC/MS/HRMS analysis of short to very long chain free fatty acids following charge reversal chemical derivatization [MP 364]. ASMS conference, 2020.\nRoderick G. Davies, Sarah Diane Oberlin, Adam Cseresznye, Laila Abdullah, Michael Mullan, Fiona Crawford, Megan C Schwarz, Marion Sourisseau, James E. Evans, Matthew J. Evans. Probing the Mechanism of Zika Infection/Replication Using Lipidomics And Proteomics Analyses [MP 364]. ASMS conference, 2020.\nCseresznye A, Huguenard C, Nkiliza A, Pearson A, Evans J, Crawford F, Mullan M, Abdullah L. Targeted analysis of microglia eicosanoid species by LC/MS/HRMS using high-resolution parallel- reaction monitoring. Bioactive Lipid Conference, 2019.\nNkiliza A, Huguenard C, Cseresznye A, Evans J, Raya B, Joshi U, Darcey T, Oberlin S, Crawford F, Mullan M, Abdullah L. Pharmacokinetic study of OEA for the treatment of Gulf war illness. Bioactive Lipid Conference, 2019.\nEvans J, Joshi U, Oberlin S, Cseresznye A, Oberlin S, Pearson A, Langlois H, Darcey T, Huguenard C, Davis R, Keegan A, Crawford F, Mullan M, Abdullah L. Nicotinamide riboside improves brain bioenergetics and reduces brain neuroinflammation in mouse model of Gulf war illness. Bioactive Lipid Conference, 2019.\nHuguenard C, Cseresznye A, Oberlin S, Langlois H, Darcey T, Dretsch M, Evans J, Crawford F, Mullan M, Abdullah L. Plasma lipidomic analysis in mild TBI and PTSD identify changes in ethanolamides, acylcarnitines and other circulating lipids. Bioactive Lipid Conference, 2019.\nHuguenard C, Cseresznye A, Oberlin S, Langlois H, Darcey T, Dretsch M, Evans J, Crawford F, Mullan M, Abdullah L. Plasma lipidomics analyses for diagnostic classification of mild TBI and PTSD. Keystone Conference, 2018.\nAbdullah L, Huguenard C, Cseresznye A, Evans J, Darcey T, Bachmeier C, Luis C, Crawford F, Mullan M. APOE4 and altered acylcarnitine profiles implicate mitochondria specific lipid metabolism dysfunction in preclinical Alzhemier’s disease. Keystone Conference, 2018.\nAbdullah L, Huguenard C, Cseresznye A, Evans J, Darcey T, Bachmeier C, Luis C, Keegan A, Crawford F, Mullan M. APOE4 and altered carnitine, TMAO acylcarnitines implicate mitochondrial dysfunction in preclinical Alzhemier’s disease. Keystone Conference, 2018.\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/Predicting Belgian Real Estate Prices_ Part 5_Initial_feature_selection/NB_5_ACs_Initial_feature_selection.html",
    "href": "projects/Predicting Belgian Real Estate Prices_ Part 5_Initial_feature_selection/NB_5_ACs_Initial_feature_selection.html",
    "title": "Predicting Belgian Real Estate Prices: Part 5: Initial feature selection",
    "section": "",
    "text": "Photo by Stephen Phillips - Hostreviews.co.uk on UnSplash"
  },
  {
    "objectID": "projects/Predicting Belgian Real Estate Prices_ Part 5_Initial_feature_selection/NB_5_ACs_Initial_feature_selection.html#read-in-dataframe",
    "href": "projects/Predicting Belgian Real Estate Prices_ Part 5_Initial_feature_selection/NB_5_ACs_Initial_feature_selection.html#read-in-dataframe",
    "title": "Predicting Belgian Real Estate Prices: Part 5: Initial feature selection",
    "section": "Read in dataframe",
    "text": "Read in dataframe\n\n\nCode\ndf = pd.read_parquet(\n    utils.Configuration.INTERIM_DATA_PATH.joinpath(\n        \"2023-10-01_Processed_dataset_for_NB_use.parquet.gzip\"\n    )\n)"
  },
  {
    "objectID": "projects/Predicting Belgian Real Estate Prices_ Part 5_Initial_feature_selection/NB_5_ACs_Initial_feature_selection.html#train-test-split",
    "href": "projects/Predicting Belgian Real Estate Prices_ Part 5_Initial_feature_selection/NB_5_ACs_Initial_feature_selection.html#train-test-split",
    "title": "Predicting Belgian Real Estate Prices: Part 5: Initial feature selection",
    "section": "Train-test split",
    "text": "Train-test split\nOur initial step involves executing a train-test split. This process divides the data into two distinct sets: a training set and a testing set. The training set is employed for model training, while the testing set is exclusively reserved for model evaluation. This methodology allows models to be trained on the training set and then assessed for accuracy using the unseen testing set. Ultimately, this approach enables an unbiased evaluation of our model’s performance, utilizing the test set that remained untouched during the model training phase.\n\n\nCode\ntrain, test = model_selection.train_test_split(\n    df, test_size=0.2, random_state=utils.Configuration.seed\n)\n\nprint(f\"Shape of train: {train.shape}\")\nprint(f\"Shape of test: {test.shape}\")\n\n\nShape of train: (2928, 56)\nShape of test: (732, 56)"
  },
  {
    "objectID": "projects/Predicting Belgian Real Estate Prices_ Part 5_Initial_feature_selection/NB_5_ACs_Initial_feature_selection.html#preprocess-dataframe-for-modelling",
    "href": "projects/Predicting Belgian Real Estate Prices_ Part 5_Initial_feature_selection/NB_5_ACs_Initial_feature_selection.html#preprocess-dataframe-for-modelling",
    "title": "Predicting Belgian Real Estate Prices: Part 5: Initial feature selection",
    "section": "Preprocess dataframe for modelling",
    "text": "Preprocess dataframe for modelling\nOur first step entails removing features that lack informative value for our model, including ‘external_reference’, ‘ad_url’, ‘day_of_retrieval’, ‘website’, ‘reference_number_of_the_epc_report’, and ‘housenumber’. After this, we’ll proceed to apply a log transformation to our target variable. Furthermore, we’ll address missing values in categorical features by replacing them with the label “missing value.” This step is crucial as CatBoost can handle missing values in numerical columns, but for categorical missing values, user intervention is needed.\n\n\nCode\nprocessed_train = (\n    train.reset_index(drop=True)\n    .assign(price=lambda df: np.log10(df.price))  # Log transformation of 'price' column\n    .drop(columns=utils.Configuration.features_to_drop)\n)\n\n# This step is needed since catboost cannot handle missing values when feature is categorical\nfor col in processed_train.columns:\n    if processed_train[col].dtype.name in (\"bool\", \"object\", \"category\"):\n        processed_train[col] = processed_train[col].fillna(\"missing value\")\n\nprocessed_train.shape\n\n\n(2928, 50)"
  },
  {
    "objectID": "projects/Predicting Belgian Real Estate Prices_ Part 3_Characterizing_the_data/NB_3_ACs_Characterizing_the_data.html",
    "href": "projects/Predicting Belgian Real Estate Prices_ Part 3_Characterizing_the_data/NB_3_ACs_Characterizing_the_data.html",
    "title": "Predicting Belgian Real Estate Prices: Part 3: Characterizing Belgian Real Estate Data Post-Scraping",
    "section": "",
    "text": "Photo by Stephen Phillips - Hostreviews.co.uk on UnSplash\nIn the second part, we delved into the intricacies of data processing necessary after scraping. Our discussion covered the treatment of numerical data, handling of categorical variables, and management of boolean values. Furthermore, we evaluated the data quality by scrutinizing the error log produced by the Immowebscraper class. In the upcoming Part 3, our focus will shift to getting a fundamental overview of our data by characterizing the cleaned scraped data. Additionally, we aim to assess feature cardinality, scrutinize distributions, and explore potential correlations between features and our target variable—property price."
  },
  {
    "objectID": "projects/Predicting Belgian Real Estate Prices_ Part 3_Characterizing_the_data/NB_3_ACs_Characterizing_the_data.html#distribution-of-features",
    "href": "projects/Predicting Belgian Real Estate Prices_ Part 3_Characterizing_the_data/NB_3_ACs_Characterizing_the_data.html#distribution-of-features",
    "title": "Predicting Belgian Real Estate Prices: Part 3: Characterizing Belgian Real Estate Data Post-Scraping",
    "section": "Distribution of features",
    "text": "Distribution of features\nNext, our focus will be on identifying low and high cardinality features. Subsequently, we will investigate how house prices vary when grouped according to these variables, using boxplots. Please take into account that the price values have undergone log transformation to address skewness.\n\n\nCode\nlow_cardinality_features = (\n    pd.DataFrame(number_unique_entries)\n    .query(\"unique_values_pct &lt;= 5\")\n    .column_name.to_list()\n)\n\n\n\n\nCode\nhigh_cardinality_features = (\n    pd.DataFrame(number_unique_entries)\n    .query(\"(unique_values_pct &gt;= 5)\")\n    .loc[lambda df: (df.column_dtype == \"float32\") | (df.column_dtype == \"float64\"), :]\n    .column_name.to_list()\n)\n\n\n\n\nCode\nplots = []\n\nfor idx, feature in enumerate(low_cardinality_features):\n    plot = (\n        df.melt(id_vars=[\"ad_url\", \"price\"])\n        .loc[lambda df: df.variable == feature, :]\n        .assign(price=lambda df: np.log10(df.price))\n        .pipe(\n            lambda df: ggplot(\n                df,\n                aes(as_discrete(\"value\"), \"price\"),\n            )\n            + facet_wrap(\"variable\")\n            + geom_boxplot(\n                show_legend=False,\n            )\n        )\n    )\n    plots.append(plot)\ngggrid(plots, ncol=4) + ggsize(900, 1600)\n\n\n\n   \n   \nFigure 3: Exploring Price Variations Across Different Variables"
  },
  {
    "objectID": "projects/Predicting Belgian Real Estate Prices_ Part 3_Characterizing_the_data/NB_3_ACs_Characterizing_the_data.html#distribution-of-target",
    "href": "projects/Predicting Belgian Real Estate Prices_ Part 3_Characterizing_the_data/NB_3_ACs_Characterizing_the_data.html#distribution-of-target",
    "title": "Predicting Belgian Real Estate Prices: Part 3: Characterizing Belgian Real Estate Data Post-Scraping",
    "section": "Distribution of target",
    "text": "Distribution of target\nUpon examining the distribution of our target variable, which is the price, it becomes evident that there is a notable skew. Our median price stands at 379,000 EUR, with the lowest at 350,000 EUR and the highest reaching 10 million EUR. To increase the accuracy of our predictions, it is worth considering a transformation of our target variable before proceeding with modeling. This transformation serves several beneficial purposes:\n\nNormalization: It has the potential to render the distribution of the target variable more symmetrical, resembling a normal distribution. Such a transformation can significantly enhance the performance of various regression models.\nEqualizing Variance: By stabilizing the variance of the target variable across different price ranges, this transformation becomes particularly valuable for ensuring the effectiveness of certain regression algorithms.\nMitigating Outliers: It is effective at diminishing the impact of extreme outliers, bolstering the model’s robustness against data anomalies.\nInterpretability: Notably, when interpreting model predictions, this transformation allows for straightforward back-transformation to the original scale. This can be achieved using a base 10 exponentiation, ensuring that predictions are easily interpretable in their origination task.\n\n\n\nCode\nbefore_transformation = df.pipe(\n    lambda df: ggplot(df, aes(\"price\")) + geom_histogram()\n) + labs(\n    title=\"Before Transformation\",\n)\nafter_transformation = df.assign(price=lambda df: np.log10(df.price)).pipe(\n    lambda df: ggplot(df, aes(\"price\"))\n    + geom_histogram()\n    + labs(\n        title=\"After log10 Transformation\",\n    )\n)\ngggrid([before_transformation, after_transformation], ncol=2) + ggsize(800, 500)\n\n\n\n   \n   \nFigure 4: Target distribution before and after log10 transformation"
  },
  {
    "objectID": "projects/Predicting Belgian Real Estate Prices_ Part 1_Feature Selection for Web Scraping/NB_1_ACs_Select_features_for_scraping.html",
    "href": "projects/Predicting Belgian Real Estate Prices_ Part 1_Feature Selection for Web Scraping/NB_1_ACs_Select_features_for_scraping.html",
    "title": "Predicting Belgian Real Estate Prices: Part 1: Feature Selection for Web Scraping",
    "section": "",
    "text": "Photo by Stephen Phillips - Hostreviews.co.uk on UnSplash\n\n\nWelcome to our project focusing on understanding the key factors that impact real estate property prices in Belgium. Our ultimate goal is to leverage data collected from immoweb.be, a prominent real estate platform in the country, to predict house prices in Belgium.\n\n\n\n\n\n\nNote\n\n\n\nYou can access the project’s app through its Streamlit website.\n\n\nThe app is divided into three sections:\n\nIntro: This section provides a basic overview of how the project is structured, how data is handled, and how the models are trained.\nExplore Data: In this section, you can explore the data interactively using boxplots and scatter plots. It allows you to visualize how specific variables impact property prices.\nMake Predictions: On the “Make Predictions” page, you can input certain variables and generate your own predictions based on the latest trained model.\n\nIt’s worth noting that we maintain the app’s accuracy by regularly updating the data through GitHub Actions, which scrapes and retrains the model every month. To test your skills against my base test RMSE score, you can download and use the dataset I uploaded to my Kaggle account through Kaggle Datasets.\nI’m excited to see what you can come up with using this tool. Feel free to explore and experiment with the app, and don’t hesitate to ask if you have any questions or need assistance with anything related to it.\nIn a series of blog posts, we will guide you through the thought process that led to the creation of the Streamlit application. Feel free to explore the topics that pique your interest or that you’d like to learn more about. We hope you’ll find this information valuable for your own projects. Let’s get started!\nNote: Although the data collection process is not described in detail here, you can find the complete workflow in the src/main.py file, specifically focusing on the relevant functions and methods in src/data/make_dataset.py. Feel free to explore it further. In summary, we utilized the request_html library to scrape all available data, which we will show you how to process in subsequent notebooks.\n\n\n\n\n\n\nHow to import your own module using a .pth file\n\n\n\nIn case you encounter difficulties importing your own modules, I found this Stack Overflow question to be quite helpful. To resolve this issue, you can follow these steps:\n\nCreate a .pth file that contains the path to the folder where your module is located. For example, prepare a .pth file with the content: C:\\Users\\myname\\house_price_prediction\\src.\nPlace this .pth file into the following folder: C:\\Users\\myname\\AppData\\Roaming\\Python\\Python310\\site-packages. This folder is already included in your PYTHONPATH, allowing Python to recognize your package directory.\nTo verify what folders are in your PYTHONPATH, you can check it using the import sys and sys.path commands.\n\nOnce you’ve completed these steps, you’ll be able to import the utils module with the following statement: `from data importach out.\n\n\n\nImport data\n\n\nCode\nimport time\nfrom pathlib import Path\n\nimport pandas as pd\nfrom data import utils\nfrom lets_plot import *\nfrom lets_plot.mapping import as_discrete\n\nLetsPlot.setup_html()\n\n\n\n            \n            \n            \n\n\n\n\nSelect Columns to Retain Based on the Quantity of Missing Values\nIn the realm of web scraping, managing the sheer volume of data is often the initial hurdle to conquer. It’s not so much about deciding what data to collect but rather what data to retain. As we delve into the data collected from the Imoweb website, we are met with a plethora of listings, each offering a unique set of information.\nFor many of these listings, there are commonalities – details like location and price tend to be constants. However, interspersed among them are those one-of-a-kind nuggets of information, such as the number of swimming pools available that obviously will be unique to certain listings. While these specific details can certainly be vital in assessing the value of certain listings, the downside is that they can lead to a sparse dataset.\nNow, let’s import our initial dataset to examine the features that are commonly shared among most ads, i.e., those that are filled in most frequently. After identifying these common attributes, we can optimize our data collection process by keeping these key characteristics and removing the less common ones.\n\n\nCode\ndf = pd.read_parquet(\n    utils.Configuration.RAW_DATA_PATH.joinpath(\n        \"complete_dataset_2023-09-27_for_NB2.parquet.gzip\"\n    )\n)\n\n\nAs depicted in Figure 1, the features ‘day of retrieval,’ ‘url,’ and ‘Energy Class’ demonstrate the highest completeness, with more than 90% of instances being present. In contrast, ‘dining room,’ ‘office,’ and ‘TV cable’ are among the least populated features, with roughly 10-20% of non-missing instances.\nThis information allows us to devise a strategy where we, for example, could retain features with a completeness of over 50%. We will delve deeper into this question in our subsequent notebooks.\n\n\nCode\n# Getting the column names with lowest missing values\nlowest_missing_value_columns = (\n    df.notna()\n    .sum()\n    .div(df.shape[0])\n    .mul(100)\n    .sort_values(ascending=False)\n    .head(50)\n    .round(1)\n)\nindexes_to_keep = lowest_missing_value_columns.index\n\n(\n    lowest_missing_value_columns.reset_index()\n    .rename(columns={\"index\": \"column\", 0: \"perc_values_present\"})\n    .assign(\n        Has_non_missing_values_above_50_pct=lambda df: df.perc_values_present.gt(50),\n        perc_values_present=lambda df: df.perc_values_present - 50,\n    )\n    .pipe(\n        lambda df: ggplot(\n            df,\n            aes(\n                \"perc_values_present\",\n                \"column\",\n                fill=\"Has_non_missing_values_above_50_pct\",\n            ),\n        )\n        + geom_bar(stat=\"identity\", orientation=\"y\", show_legend=False)\n        + ggsize(800, 1000)\n        + labs(\n            title=\"Top 50 Features with Non-Missing Values Above 50%\",\n            subtitle=\"\"\"The plot illustrates that the features 'day of retrieval,' 'url,' and 'Energy Class' exhibited the \n            highest completeness, with over 90% of instances present. Conversely, 'dining room','office,' and 'TV cable' \n            were among the least populated features, with approximately 10-20% of non-missing instances.\n            \"\"\",\n            x=\"Percentage of Instances Present with Reference Point at 50%\",\n            y=\"\",\n            caption=\"https://www.immoweb.be/\",\n        )\n        + theme(\n            plot_subtitle=element_text(\n                size=12, face=\"italic\"\n            ),  # Customize subtitle appearance\n            plot_title=element_text(size=15, face=\"bold\"),  # Customize title appearance\n        )\n    )\n)\n\n\n\n   \n   \nFigure 1: Top 50 Features with Non-Missing Values Above 50%\n\n\n\nThat’s all for now. In part 2, we will examine the downloaded raw data and investigate the error messages we encountered during the web scraping process with the goal of understanding how to overcome these challenges. See you in the next installment!"
  },
  {
    "objectID": "posts/spam/spam.html",
    "href": "posts/spam/spam.html",
    "title": "Tidy Tuesday: Spam E-mail",
    "section": "",
    "text": "Photo by Stephen Phillips - Hostreviews.co.uk on UnSplash\nWelcome to this week’s Tidy Tuesday! Today, we’re delving into the intriguing yet bothersome realm of email spams. You know, those unsolicited messages that flood our inboxes? They go by many names like junk email or spam mail, and they’re sent in bulk. The term “spam” got its name from a Monty Python sketch where the word “Spam” was everywhere, just like these emails. Starting from the early 1990s, these spam messages have been on a steady rise, making up around 90% of all email traffic by 2014.\nWe’ve got our data from the Tidy Tuesday treasure trove over at GitHub! This dataset, from Vincent Arel-Bundock’s Rdatasets package, was initially gathered at Hewlett-Packard Labs. They later kindly shared it with the UCI Machine Learning Repository.\nThis treasure trove of information consists of 4601 emaily sorted into spam and non-spam categorie"
  },
  {
    "objectID": "posts/spam/spam.html#visualize-all-the-principal-components",
    "href": "posts/spam/spam.html#visualize-all-the-principal-components",
    "title": "Tidy Tuesday: Spam E-mail",
    "section": "Visualize all the principal components",
    "text": "Visualize all the principal components\n\n\nCode\nX = df.select(pl.all().exclude(\"yesno\")).to_pandas()\ny = df.select(pl.col(\"yesno\")).to_pandas()\n\n\n\n\nCode\n# Before PCA we need to scale and transform our dataset\n\nscaler = preprocessing.PowerTransformer().set_output(transform=\"pandas\")\n\nX = scaler.fit_transform(X)\n\n\n\n\nCode\npca = decomposition.PCA()\ncomponents = pca.fit_transform(X)\nlabels = {\n    str(i): f\"PC {i+1} ({var:.1f}%)\"\n    for i, var in enumerate(pca.explained_variance_ratio_ * 100)\n}\n\nfig = px.scatter_matrix(\n    components,\n    opacity=0.2,\n    labels=labels,\n    dimensions=range(6),\n    color=y.squeeze().map({0: \"Not a Spam\", 1: \"Spam\"}),\n    width=1000,\n    height=1000,\n)\nfig.update_traces(diagonal_visible=False)\nfig.show()\n\n\n\n\n                                                \nFigure 4: Visualization of All Principal Components\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs evident, the first and second principal components manage to account for 62% of the variance within the data. A distinct separation of classes is noticeable, particularly with the spam group exhibiting a broader distribution, indicating greater diversity."
  },
  {
    "objectID": "posts/spam/spam.html#visualize-loadings",
    "href": "posts/spam/spam.html#visualize-loadings",
    "title": "Tidy Tuesday: Spam E-mail",
    "section": "Visualize Loadings",
    "text": "Visualize Loadings\nFor a deeper comprehension of how each characteristic influences our principal components, we can delve into examining the loadings.\n\n\nCode\npca = decomposition.PCA(n_components=2)\ncomponents = pca.fit_transform(X)\nloadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n\n\nfig = px.scatter(\n    components,\n    x=0,\n    y=1,\n    color=y.squeeze().map({0: \"Not a Spam\", 1: \"Spam\"}),\n    labels={\n        \"0\": f\"PC1 ({pca.explained_variance_ratio_[0]:.0%})\",\n        \"1\": f\"PC2 ({pca.explained_variance_ratio_[1]:.0%})\",\n    },\n    template=\"plotly_dark\",\n    color_discrete_sequence=[\n        \"red\",\n        \"green\",\n    ],\n    opacity=0.4,\n    width=700,\n    height=700,\n)\nfor i, feature in enumerate(X.columns):\n    fig.add_annotation(\n        ax=0,\n        ay=0,\n        axref=\"x\",\n        ayref=\"y\",\n        x=loadings[i, 0],\n        y=loadings[i, 1],\n        showarrow=True,\n        arrowsize=1,\n        arrowhead=2,\n        xanchor=\"right\",\n        yanchor=\"top\",\n    )\n    fig.add_annotation(\n        x=loadings[i, 0],\n        y=loadings[i, 1],\n        ax=0,\n        ay=0,\n        font=dict(\n            size=20,\n            # color='yellow'\n        ),\n        xanchor=\"left\",\n        yanchor=\"bottom\",\n        text=feature,\n        yshift=5,\n    )\nfig\n\n\n\n\n                                                \nFigure 5: Loadings Plot\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe loadings plot reveals a strong correlation among the variables “crl.tot,” “money,” and “n000”. If we were to construct a machine learning model for spam email recognition, we could opt for one of these variables to streamline our dataset. Furthermore, the disassociation of “bang” from this trio is evident, its vector positioned at a 90° angle.\n\n\nAlright folks, our journey into the intriguing realm of spam emails has left us with some valuable insights. It’s clear that words like “bang” and “n000” are key indicators to watch out for. Luckily, modern machine learning models are here to do the hard work on our behalf.\nStay vigilant out there and take care! See you in the next week’s adventure! 👋📧🛡️"
  },
  {
    "objectID": "posts/polars speed/polars_speed.html",
    "href": "posts/polars speed/polars_speed.html",
    "title": "The Ultimate Speed Test: Pandas vs Polars",
    "section": "",
    "text": "Once I shared my first article about “Getting Started with Polars” I started thinking about something exciting: comparing the speed of Polars and Pandas. This curiosity was sparked by all the buzz around the brand-new Pandas 2.0 release, promising lightning-fast performance. Pandas 2.0 was announced to come packed with cool features, including the addition of Apache Arrow (pyarrow) as its backing memory format. The big perk of Apache Arrow is that it makes operations speedier and more memory-friendly. Naturally, this got me wondering: how does Pandas 2.0 measure up against Polars? Let’s dive in and find out!"
  },
  {
    "objectID": "posts/polars speed/polars_speed.html#csv",
    "href": "posts/polars speed/polars_speed.html#csv",
    "title": "The Ultimate Speed Test: Pandas vs Polars",
    "section": "CSV",
    "text": "CSV\nReading CSV files from disk is a task that data scientists often find themselves doing. Now, let’s see how these two libraries compare for this particular job. To maximize the blazing-fast data handling capabilities of PyArrow, we’ll equip Pandas with the engine=\"pyarrow\" and dtype_backend=\"pyarrow\" arguments. Let’s see how these choices shape the performance!\n\n\nCode\ntest_dict = {\n    \"Pandas\": lambda: pd.read_csv(\n        \"sample.csv\", engine=\"pyarrow\", dtype_backend=\"pyarrow\"\n    ),\n    \"Polars\": lambda: pl.read_csv(\"sample.csv\"),\n}\nread_csv = run_test(test_dict, \"Read csv\")\n\n\n\n\n                                                \nFigure 1: Reading in data From a CSV File\n\n\n\nFor the sake of comparison, we’ll also demonstrate the timeit function invoked using Jupyter cell magic. You’ll notice that the numbers generated this way are quite closely aligned with ours.\n\n\nCode\n%%timeit\npd.read_csv(\"sample.csv\", engine=\"pyarrow\", dtype_backend=\"pyarrow\")\n\n\n6.33 ms ± 234 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\n\nCode\n%%timeit\npl.read_csv(\"sample.csv\")\n\n\n3.01 ms ± 456 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)"
  },
  {
    "objectID": "posts/polars speed/polars_speed.html#parquet",
    "href": "posts/polars speed/polars_speed.html#parquet",
    "title": "The Ultimate Speed Test: Pandas vs Polars",
    "section": "parquet",
    "text": "parquet\nNow, let’s read the data in Parquet format.\n\n\nCode\ntest_dict = {\n    \"Pandas\": lambda: pd.read_parquet(\n        \"sample.parquet\", engine=\"pyarrow\", dtype_backend=\"pyarrow\"\n    ),\n    \"Polars\": lambda: pl.read_parquet(\"sample.parquet\"),\n}\nread_parquet = run_test(test_dict, \"Read parquet\")\n\n\n\n\n                                                \nFigure 2: Reading in data From a parquet File\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nPolars unquestionably wins this round, it can boast a speed advantage of 2 to 4 times over Pandas."
  },
  {
    "objectID": "posts/polars speed/polars_speed.html#files-awaiting-in-memory-reading",
    "href": "posts/polars speed/polars_speed.html#files-awaiting-in-memory-reading",
    "title": "The Ultimate Speed Test: Pandas vs Polars",
    "section": "Files awaiting in-memory reading",
    "text": "Files awaiting in-memory reading\nA clever approach to conserve memory and enhance speed involves reading only the columns essential for operations. Consider a scenario where we’re interested in displaying just the names from this dataset. The big question now: how do these libraries measure up in terms of speed? Let’s find out!\n\n\nCode\ntest_dict = {\n    \"Pandas\": lambda: pd.read_csv(\n        \"sample.csv\", engine=\"pyarrow\", dtype_backend=\"pyarrow\", usecols=[\"name\"]\n    ),\n    \"Polars\": lambda: pl.scan_csv(\"sample.csv\").select(pl.col(\"name\")).collect(),\n}\nselect_col_not_in_memory = run_test(test_dict, \"Select column (not in memory)\")\n\n\n\n\n                                                \nFigure 3: Selecting Columns from a File Not Yet in Memory"
  },
  {
    "objectID": "posts/polars speed/polars_speed.html#file-is-in-memory",
    "href": "posts/polars speed/polars_speed.html#file-is-in-memory",
    "title": "The Ultimate Speed Test: Pandas vs Polars",
    "section": "File is in memory",
    "text": "File is in memory\nAs anticipated, Polars continues to showcase its swiftness. It’s worth highlighting the usage of the lazy and collect methods in Polars. These nifty tools grant us access to the library’s clever query optimization techniques, which play a pivotal role in significantly enhancing performance. OK, one step further: suppose our files are already loaded into memory. Would there still be a distinction in performance under this circumstance?\n\n\nCode\ndf_pandas = pd.read_csv(\"sample.csv\", engine=\"pyarrow\", dtype_backend=\"pyarrow\")\ndf_polars = pl.read_csv(\"sample.csv\")\n\n\n\n\nCode\ntest_dict = {\n    \"Pandas\": lambda: df_pandas.loc[:, \"name\"],\n    \"Polars\": lambda: df_polars.lazy().select(pl.col(\"name\")).collect(),\n}\nselect_col_in_memory = run_test(test_dict, \"Select column\")\n\n\n\n\n                                                \nFigure 4: Selecting Columns from a File Already in Memory\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhile Polars showed a significant speed advantage for tasks involving pre-read files, both libraries perform similarly when the files are already in memory."
  },
  {
    "objectID": "posts/polars speed/polars_speed.html#based-on-one-condition",
    "href": "posts/polars speed/polars_speed.html#based-on-one-condition",
    "title": "The Ultimate Speed Test: Pandas vs Polars",
    "section": "Based on one condition",
    "text": "Based on one condition\nFor our simple scenario, we’ll be narrowing down our focus to filter data based on individuals with the name “David”.\n\n\nCode\ntest_dict = {\n    \"Pandas\": lambda: (df_pandas.query(\"name=='David'\")),\n    \"Polars\": lambda: (df_polars.lazy().filter((pl.col(\"name\") == \"David\")).collect()),\n}\nfilter_row_one_condition = run_test(test_dict, \"Filter (simple)\")\n\n\n\n\n                                                \nFigure 5: Filtering Rows Based on One Condition"
  },
  {
    "objectID": "posts/polars speed/polars_speed.html#based-on-multiple-conditions",
    "href": "posts/polars speed/polars_speed.html#based-on-multiple-conditions",
    "title": "The Ultimate Speed Test: Pandas vs Polars",
    "section": "Based on multiple conditions",
    "text": "Based on multiple conditions\nNow, for a more intricate challenge, we’re going to dive into querying the data to extract individuals who meet specific criteria: those named David, born after 1980, residing in a city other than London, married, and with three children.\n\n\nCode\ntest_dict = {\n    \"Pandas\": lambda: (\n        df_pandas.query(\n            \"name=='David' and born&gt;1980 and city != 'London' or is_married == True and children &gt;= 3\"\n        )\n    ),\n    \"Polars\": lambda: (\n        df_polars.lazy()\n        .filter(\n            (pl.col(\"name\") == \"David\")\n            & (pl.col(\"born\") &gt; 1980)\n            & (pl.col(\"city\") != \"London\")\n            | (pl.col(\"is_married\") == True) & (pl.col(\"children\") &gt;= 3)\n        )\n        .collect()\n    ),\n}\nfilter_row_multiple_condition = run_test(test_dict, \"Filter (complex)\")\n\n\n\n\n                                                \nFigure 6: Filtering Rows Based on Multiple Condition\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nBoth libraries tackled this challenge quite well, yet Pandas struggled to keep pace with Polars. It’s intriguing to observe that while Pandas required nearly twice the time for the more intricate task, Polars managed to complete it in almost the same amount of time. Parallelization in action."
  },
  {
    "objectID": "posts/polars speed/polars_speed.html#single-operation",
    "href": "posts/polars speed/polars_speed.html#single-operation",
    "title": "The Ultimate Speed Test: Pandas vs Polars",
    "section": "Single operation",
    "text": "Single operation\nAs a single operation, we’ll simply calculate the century in which these individuals were born.\n\n\nCode\ntest_dict = {\n    \"Pandas\": lambda: (df_pandas.assign(born=lambda df: df.born.div(100).round())),\n    \"Polars\": lambda: (\n        df_polars.lazy().with_columns((pl.col(\"born\") / 100).round()).collect()\n    ),\n}\noperate_one_column = run_test(test_dict, \"Operate (one column)\")\n\n\n\n\n                                                \nFigure 7: Performing a Singme Operation on a Column"
  },
  {
    "objectID": "posts/polars speed/polars_speed.html#multiple-operations",
    "href": "posts/polars speed/polars_speed.html#multiple-operations",
    "title": "The Ultimate Speed Test: Pandas vs Polars",
    "section": "Multiple operations",
    "text": "Multiple operations\nLet’s also explore what happens when performing multiple operations on the columns. We’ll mix things up with some string operations, mapping, and math calculations to see how these libraries handle it!\n\n\nCode\ntest_dict = {\n    \"Pandas\": lambda: (\n        df_pandas.assign(\n            born=lambda df: df.born.div(100).round(),\n            name=lambda df: df.name.str.lower(),\n            city=lambda df: df.city.str.upper(),\n            zip_code=lambda df: df.zip_code.mul(2),\n            income=lambda df: df.income.div(10).astype(\"str\"),\n            is_married=lambda df: df.is_married.map({False: 0, True: 1}),\n            children=lambda df: df.children.astype(\"bool\"),\n            car=lambda df: df.car.str[0],\n        )\n    ),\n    \"Polars\": lambda: (\n        df_polars.lazy()\n        .with_columns(\n            [\n                (pl.col(\"born\") / 100).round(),\n                pl.col(\"name\").str.to_lowercase(),\n                pl.col(\"city\").str.to_uppercase(),\n                pl.col(\"zip_code\") * 2,\n                (pl.col(\"income\") / 10).cast(pl.Utf8),\n                pl.col(\"is_married\").map_dict({False: 0, True: 1}),\n                pl.col(\"children\").cast(pl.Boolean),\n                pl.col(\"car\").str.slice(0, length=1),\n            ]\n        )\n        .collect()\n    ),\n}\noperate_multiple_column = run_test(test_dict, \"Operate (more columns)\")\n\n\n\n\n                                                \nFigure 8: Perfrming a Multiple Operation on Columns\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nOnce again, Polars takes the lead. While both libraries required more time for the task involving multiple operations, Polars demonstrated superior scalability in this scenario."
  },
  {
    "objectID": "posts/polars speed/polars_speed.html#simple",
    "href": "posts/polars speed/polars_speed.html#simple",
    "title": "The Ultimate Speed Test: Pandas vs Polars",
    "section": "Simple",
    "text": "Simple\nTime to shift our attention to aggregation. First up, a simple task: let’s calculate the mean income based on names. Then, for a bit more complexity, we’ll dive into computing statistics involving the income, children, and car columns. Things are about to get interesting!\n\n\nCode\ntest_dict = {\n    \"Pandas\": lambda: (df_pandas.groupby(\"name\").income.mean()),\n    \"Polars\": lambda: (df_polars.lazy().groupby(\"name\").mean().collect()),\n}\naggregate_simple = run_test(test_dict, \"Aggregate (simple)\")\n\n\n\n\n                                                \nFigure 10: Performing a simple aggregation"
  },
  {
    "objectID": "posts/polars speed/polars_speed.html#more-complex",
    "href": "posts/polars speed/polars_speed.html#more-complex",
    "title": "The Ultimate Speed Test: Pandas vs Polars",
    "section": "More complex",
    "text": "More complex\n\n\nCode\ntest_dict = {\n    \"Pandas\": lambda: (\n        df_pandas.groupby([\"name\", \"car\", \"is_married\"]).agg(\n            born_min=(\"born\", min),\n            born_max=(\"born\", max),\n            income_mean=(\"income\", np.mean),\n            income_median=(\"income\", np.median),\n            children_mean=(\"children\", np.mean),\n            car_count=(\"car\", \"count\"),\n        )\n    ),\n    \"Polars\": lambda: (\n        df_polars.lazy()\n        .groupby([\"name\", \"car\", \"is_married\"])\n        .agg(\n            [\n                pl.col(\"born\").min().alias(\"born_min\"),\n                pl.col(\"born\").max().alias(\"born_max\"),\n                pl.col(\"income\").mean().alias(\"income_mean\"),\n                pl.col(\"income\").median().alias(\"income_median\"),\n                pl.col(\"children\").mean().alias(\"children_mean\"),\n                pl.col(\"car\").count().alias(\"car_count\"),\n            ]\n        )\n        .collect()\n    ),\n}\naggregate_complex = run_test(test_dict, \"Aggregate (complex)\")\n\n\n\n\n                                                \nFigure 11: Performing a complex aggregation\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhile Pandas showcased noteworthy speed for the simple aggregation, the more intricate task exposed significant disparities between the two libraries. Polars took a commanding lead in this scenario, presenting a considerably faster performance compared to Pandas."
  },
  {
    "objectID": "posts/polars speed/polars_speed.html#using-pl.read_csv",
    "href": "posts/polars speed/polars_speed.html#using-pl.read_csv",
    "title": "The Ultimate Speed Test: Pandas vs Polars",
    "section": "Using pl.read_csv",
    "text": "Using pl.read_csv\n\n\nCode\ntest_dict = {\n    \"Pandas\": lambda: (\n        df_pandas.loc[:, lambda df: ~df.columns.isin([\"is_married\"])]\n        .query(\"name=='David' and born&gt;1980 and city != 'London' and children &gt;= 3\")\n        .assign(\n            born=lambda df: df.born.div(100).round(),\n            name=lambda df: df.name.str.lower(),\n            city=lambda df: df.city.str.upper(),\n            zip_code=lambda df: df.zip_code.mul(2),\n            income=lambda df: df.income.div(10),\n            children=lambda df: df.children.astype(\"bool\"),\n            car=lambda df: df.car.str[0],\n        )\n        .groupby(\n            [\n                \"name\",\n                \"car\",\n            ]\n        )\n        .agg(\n            born_min=(\"born\", min),\n            born_max=(\"born\", max),\n            income_mean=(\"income\", np.mean),\n            income_median=(\"income\", np.median),\n            children_mean=(\"children\", np.mean),\n            car_count=(\"car\", \"count\"),\n        )\n    ),\n    \"Polars\": lambda: (\n        df_polars.lazy()\n        .select(cs.all() - cs.ends_with(\"married\"))\n        .filter(\n            (pl.col(\"name\") == \"David\")\n            & (pl.col(\"born\") &gt; 1980)\n            & (pl.col(\"city\") != \"London\")\n            & (pl.col(\"children\") &gt;= 3)\n        )\n        .with_columns(\n            [\n                (pl.col(\"born\") / 100).round(),\n                pl.col(\"name\").str.to_lowercase(),\n                pl.col(\"city\").str.to_uppercase(),\n                pl.col(\"zip_code\") * 2,\n                (pl.col(\"income\") / 10),\n                pl.col(\"children\").cast(pl.Boolean),\n                pl.col(\"car\").str.slice(0, length=1),\n            ]\n        )\n        .groupby(\n            [\n                \"name\",\n                \"car\",\n            ]\n        )\n        .agg(\n            [\n                pl.col(\"born\").min().alias(\"born_min\"),\n                pl.col(\"born\").max().alias(\"born_max\"),\n                pl.col(\"income\").mean().alias(\"income_mean\"),\n                pl.col(\"income\").median().alias(\"income_median\"),\n                pl.col(\"children\").mean().alias(\"children_mean\"),\n                pl.col(\"car\").count().alias(\"car_count\"),\n            ]\n        )\n        .collect()\n    ),\n}\nwhole_workflow_read_csv = run_test(test_dict, \"Whole workflow\")\n\n\n\n\n                                                \nFigure 12: Performing a representative workflow"
  },
  {
    "objectID": "posts/polars speed/polars_speed.html#using-pl.scan_csv",
    "href": "posts/polars speed/polars_speed.html#using-pl.scan_csv",
    "title": "The Ultimate Speed Test: Pandas vs Polars",
    "section": "Using pl.scan_csv",
    "text": "Using pl.scan_csv\n\n\nCode\ntest_dict = {\n    \"Pandas\": lambda: (\n        df_pandas.loc[:, lambda df: ~df.columns.isin([\"is_married\"])]\n        .query(\"name=='David' and born&gt;1980 and city != 'London' and children &gt;= 3\")\n        .assign(\n            born=lambda df: df.born.div(100).round(),\n            name=lambda df: df.name.str.lower(),\n            city=lambda df: df.city.str.upper(),\n            zip_code=lambda df: df.zip_code.mul(2),\n            income=lambda df: df.income.div(10),\n            children=lambda df: df.children.astype(\"bool\"),\n            car=lambda df: df.car.str[0],\n        )\n        .groupby(\n            [\n                \"name\",\n                \"car\",\n            ]\n        )\n        .agg(\n            born_min=(\"born\", min),\n            born_max=(\"born\", max),\n            income_mean=(\"income\", np.mean),\n            income_median=(\"income\", np.median),\n            children_mean=(\"children\", np.mean),\n            car_count=(\"car\", \"count\"),\n        )\n    ),\n    \"Polars\": lambda: (\n        pl.scan_csv(\"sample.csv\")\n        .select(cs.all() - cs.ends_with(\"married\"))\n        .filter(\n            (pl.col(\"name\") == \"David\")\n            & (pl.col(\"born\") &gt; 1980)\n            & (pl.col(\"city\") != \"London\")\n            & (pl.col(\"children\") &gt;= 3)\n        )\n        .with_columns(\n            [\n                (pl.col(\"born\") / 100).round(),\n                pl.col(\"name\").str.to_lowercase(),\n                pl.col(\"city\").str.to_uppercase(),\n                pl.col(\"zip_code\") * 2,\n                (pl.col(\"income\") / 10),\n                pl.col(\"children\").cast(pl.Boolean),\n                pl.col(\"car\").str.slice(0, length=1),\n            ]\n        )\n        .groupby(\n            [\n                \"name\",\n                \"car\",\n            ]\n        )\n        .agg(\n            [\n                pl.col(\"born\").min().alias(\"born_min\"),\n                pl.col(\"born\").max().alias(\"born_max\"),\n                pl.col(\"income\").mean().alias(\"income_mean\"),\n                pl.col(\"income\").median().alias(\"income_median\"),\n                pl.col(\"children\").mean().alias(\"children_mean\"),\n                pl.col(\"car\").count().alias(\"car_count\"),\n            ]\n        )\n        .collect()\n    ),\n}\nwhole_workflow_scan_csv = run_test(test_dict, \"Whole workflow (scan_csv)\")\n\n\n\n\n                                                \nFigure 13: Performing a representative workflow (using pl_scan_csv for Polars)\n\n\n\nAs evident, the utilization of scan_csv increased the required time by about 3-4 times. However, even with this increase, Polars still manages to maintain a substantial advantage of around 5 times faster than the entire workflow executed using Pandas.\n\n\n\n\n\n\nNote\n\n\n\nWhen we consider the entirety of the data processing pipeline, irrespective of the file reading approach, Polars emerges as the victor. It consistently exhibits a considerable speed advantage compared to Pandas."
  },
  {
    "objectID": "posts/pandas_styler/pandas_style.html",
    "href": "posts/pandas_styler/pandas_style.html",
    "title": "Mastering Pandas DataFrame Styling for a Stunning Presentation",
    "section": "",
    "text": "Image by Mika Baumeister\nAs you wrap up your data analysis journey, you face a fun decision: how to share your discoveries effectively. Tables and graphs both have their moments to shine. Tables work great when you want your audience to spot exact values and compare them. But, here’s the twist: tables can look overwhelming at first, especially if they’re crammed with data.\nBut don’t worry! Styling and formatting are here to help. Pandas comes to the rescue. It lets you turn your data into stylish tables effortlessly.\nIn this article, we’ll explore some tricks to make your Pandas DataFrames look awesome and tell a clear data story. For the demonstrations, we’ll dive into Seaborn’s built-in “tips” dataset. This dataset is a nifty data frame with 244 rows and 7 variables, offering a peek into the world of tipping. This comprehensive collection includes variables like the tip amount in dollars, the bill total in dollars, the gender of the bill payer, the presence of smokers in the party, the day of the week, the time of day, and the party size. Ready to roll? Let’s jump right in!"
  },
  {
    "objectID": "posts/pandas_styler/pandas_style.html#highlighting-minimum-and-maximum-values",
    "href": "posts/pandas_styler/pandas_style.html#highlighting-minimum-and-maximum-values",
    "title": "Mastering Pandas DataFrame Styling for a Stunning Presentation",
    "section": "Highlighting minimum and maximum values",
    "text": "Highlighting minimum and maximum values\nCheck out these handy helpers: highlight_min and highlight_max. They’re your go-to for spotting the lowest and highest values in each column. It’s a speedy way to emphasize the most important data points in each category.\nAlso, please note the format(precision=1, thousands=\",\", decimal=\".\") snippet, this is not exactly a built-in style feature but has everything to do with keeping those float numbers tidy. Pandas tends to display more decimal places than we often require, which can be a bit distracting. To tone it down a notch, we can lean on the format() and format_index() methods to fine-tune the precision. Trust me, it’s super useful!\n\n\nCode\n(\n    grouped.style.format(precision=1, thousands=\",\", decimal=\".\")\n    .highlight_max(\n        axis=0, props=\"color:white; font-weight:bold; background-color:green;\"\n    )\n    .highlight_min(axis=0, props=\"color:white; font-weight:bold; background-color:red;\")\n)\n\n\n\n\n\n\nTable 3: Highlighting Maximum and Minimum Values with Pandas Styler\n\n\n \n \n \ntotal_bill\ntip\nsize\n\n\nday\nsmoker\ntime\n \n \n \n\n\n\n\nFri\nNo\nDinner\n22.5\n3.2\n2.0\n\n\nLunch\n16.0\n3.0\n3.0\n\n\nYes\nDinner\n16.3\n3.0\n2.0\n\n\nLunch\n12.8\n2.1\n2.0\n\n\nSat\nNo\nDinner\n17.8\n2.8\n2.0\n\n\nYes\nDinner\n20.4\n2.7\n2.0\n\n\nSun\nNo\nDinner\n18.4\n3.0\n3.0\n\n\nYes\nDinner\n23.1\n3.5\n2.0\n\n\nThur\nNo\nDinner\n18.8\n3.0\n2.0\n\n\nLunch\n15.2\n2.1\n2.0\n\n\nYes\nLunch\n16.5\n2.6\n2.0"
  },
  {
    "objectID": "posts/pandas_styler/pandas_style.html#emphasizing-values-within-a-range",
    "href": "posts/pandas_styler/pandas_style.html#emphasizing-values-within-a-range",
    "title": "Mastering Pandas DataFrame Styling for a Stunning Presentation",
    "section": "Emphasizing Values Within a Range",
    "text": "Emphasizing Values Within a Range\nImagine you’re want to find the days when tips fell between 3 and 5 dollars. In this scenario, the highlight_between method comes to the rescue. Don’t forget to use the subset argument; it’s your trusty sidekick when you only want to work with selected columns.\n\n\nCode\n(\n    grouped.reset_index()\n    .style.format(precision=1, thousands=\",\", decimal=\".\")\n    .highlight_between(\n        left=3,\n        right=5,\n        subset=[\"tip\"],\n        axis=1,\n        props=\"color:white; font-weight:bold; background-color:purple;\",\n    )\n)\n\n\n\n\n\n\nTable 4: Highlighting Data within a Specified Range Using Pandas Styler\n\n\n \nday\nsmoker\ntime\ntotal_bill\ntip\nsize\n\n\n\n\n0\nFri\nNo\nDinner\n22.5\n3.2\n2.0\n\n\n1\nFri\nNo\nLunch\n16.0\n3.0\n3.0\n\n\n2\nFri\nYes\nDinner\n16.3\n3.0\n2.0\n\n\n3\nFri\nYes\nLunch\n12.8\n2.1\n2.0\n\n\n4\nSat\nNo\nDinner\n17.8\n2.8\n2.0\n\n\n5\nSat\nYes\nDinner\n20.4\n2.7\n2.0\n\n\n6\nSun\nNo\nDinner\n18.4\n3.0\n3.0\n\n\n7\nSun\nYes\nDinner\n23.1\n3.5\n2.0\n\n\n8\nThur\nNo\nDinner\n18.8\n3.0\n2.0\n\n\n9\nThur\nNo\nLunch\n15.2\n2.1\n2.0\n\n\n10\nThur\nYes\nLunch\n16.5\n2.6\n2.0"
  },
  {
    "objectID": "posts/pandas_styler/pandas_style.html#highlight-column-wise-outliers",
    "href": "posts/pandas_styler/pandas_style.html#highlight-column-wise-outliers",
    "title": "Mastering Pandas DataFrame Styling for a Stunning Presentation",
    "section": "Highlight Column-Wise Outliers",
    "text": "Highlight Column-Wise Outliers\nUtilizing parameters such as q_left=0.05, q_right=0.95, axis=0, and defining props='opacity: 10%;', we can highlight values residing outside the 5-95 percentile range.\n\n\nCode\n(\n    grouped.style.format(precision=1, thousands=\",\", decimal=\".\").highlight_quantile(\n        q_left=0.05, q_right=0.95, axis=0, props=\"opacity: 10%;\"\n    )\n)\n\n\n\n\n\n\nTable 5: Highlighting Outliers\n\n\n \n \n \ntotal_bill\ntip\nsize\n\n\nday\nsmoker\ntime\n \n \n \n\n\n\n\nFri\nNo\nDinner\n22.5\n3.2\n2.0\n\n\nLunch\n16.0\n3.0\n3.0\n\n\nYes\nDinner\n16.3\n3.0\n2.0\n\n\nLunch\n12.8\n2.1\n2.0\n\n\nSat\nNo\nDinner\n17.8\n2.8\n2.0\n\n\nYes\nDinner\n20.4\n2.7\n2.0\n\n\nSun\nNo\nDinner\n18.4\n3.0\n3.0\n\n\nYes\nDinner\n23.1\n3.5\n2.0\n\n\nThur\nNo\nDinner\n18.8\n3.0\n2.0\n\n\nLunch\n15.2\n2.1\n2.0\n\n\nYes\nLunch\n16.5\n2.6\n2.0"
  },
  {
    "objectID": "posts/pandas_styler/pandas_style.html#spot-trends-using-color-gradients",
    "href": "posts/pandas_styler/pandas_style.html#spot-trends-using-color-gradients",
    "title": "Mastering Pandas DataFrame Styling for a Stunning Presentation",
    "section": "Spot trends Using Color Gradients",
    "text": "Spot trends Using Color Gradients\nWe can show data trends using both the background_gradient and text_gradient methods. These methods introduce gradient-style background colors and text shading to our visualizations. To optimize their impact, it’s advisable to first arrange your data with the sort_values method before applying the background_gradient.\n\n\nCode\n(\n    grouped.reset_index()\n    .sort_values(by=\"total_bill\")\n    .style.format(precision=1, thousands=\",\", decimal=\".\")\n    .background_gradient(cmap=\"viridis\", axis=0)\n)\n\n\n\n\n\n\nTable 6: Unveiling Data Trends with Pandas Styler’s background_gradient\n\n\n \nday\nsmoker\ntime\ntotal_bill\ntip\nsize\n\n\n\n\n3\nFri\nYes\nLunch\n12.8\n2.1\n2.0\n\n\n9\nThur\nNo\nLunch\n15.2\n2.1\n2.0\n\n\n1\nFri\nNo\nLunch\n16.0\n3.0\n3.0\n\n\n2\nFri\nYes\nDinner\n16.3\n3.0\n2.0\n\n\n10\nThur\nYes\nLunch\n16.5\n2.6\n2.0\n\n\n4\nSat\nNo\nDinner\n17.8\n2.8\n2.0\n\n\n6\nSun\nNo\nDinner\n18.4\n3.0\n3.0\n\n\n8\nThur\nNo\nDinner\n18.8\n3.0\n2.0\n\n\n5\nSat\nYes\nDinner\n20.4\n2.7\n2.0\n\n\n0\nFri\nNo\nDinner\n22.5\n3.2\n2.0\n\n\n7\nSun\nYes\nDinner\n23.1\n3.5\n2.0\n\n\n\n\n\n\n\n\nCode\n(\n    grouped.reset_index()\n    .sort_values(by=\"total_bill\")\n    .style.format(precision=1, thousands=\",\", decimal=\".\")\n    .text_gradient(cmap=\"bwr\", axis=0)\n)\n\n\n\n\n\n\nTable 7: Unveiling Data Trends with Pandas Styler’s text_gradient\n\n\n \nday\nsmoker\ntime\ntotal_bill\ntip\nsize\n\n\n\n\n3\nFri\nYes\nLunch\n12.8\n2.1\n2.0\n\n\n9\nThur\nNo\nLunch\n15.2\n2.1\n2.0\n\n\n1\nFri\nNo\nLunch\n16.0\n3.0\n3.0\n\n\n2\nFri\nYes\nDinner\n16.3\n3.0\n2.0\n\n\n10\nThur\nYes\nLunch\n16.5\n2.6\n2.0\n\n\n4\nSat\nNo\nDinner\n17.8\n2.8\n2.0\n\n\n6\nSun\nNo\nDinner\n18.4\n3.0\n3.0\n\n\n8\nThur\nNo\nDinner\n18.8\n3.0\n2.0\n\n\n5\nSat\nYes\nDinner\n20.4\n2.7\n2.0\n\n\n0\nFri\nNo\nDinner\n22.5\n3.2\n2.0\n\n\n7\nSun\nYes\nDinner\n23.1\n3.5\n2.0"
  },
  {
    "objectID": "posts/pandas_styler/pandas_style.html#display-bar-charts-within-your-table",
    "href": "posts/pandas_styler/pandas_style.html#display-bar-charts-within-your-table",
    "title": "Mastering Pandas DataFrame Styling for a Stunning Presentation",
    "section": "Display Bar Charts within Your Table",
    "text": "Display Bar Charts within Your Table\nLet’s explore a technique for highlighting the significance of values by embedding bar charts right within the cells. The blend of bar heights and color gradients can pack a powerful punch in your data storytelling arsenal. Don’t forget to experiment with the ‘align’ option, a handy tool that helps you position these bars within the cells just right, giving your visuals a polished look. Feel free to play around with the settings and find what clicks best with your data tales.\n\n\nCode\n(\n    grouped.reset_index()\n    .sort_values(by=\"total_bill\")\n    .style.format(precision=1, thousands=\",\", decimal=\".\")\n    .bar(\n        align=\"mean\",\n        cmap=\"bwr\",\n        height=50,\n        width=60,\n        props=\"width: 120px; border-right: 1px solid black;\",\n    )\n)\n\n\n\n\n\n\nTable 8: Bar Charts in Your Table with Alignment Set to ‘Mean’\n\n\n \nday\nsmoker\ntime\ntotal_bill\ntip\nsize\n\n\n\n\n3\nFri\nYes\nLunch\n12.8\n2.1\n2.0\n\n\n9\nThur\nNo\nLunch\n15.2\n2.1\n2.0\n\n\n1\nFri\nNo\nLunch\n16.0\n3.0\n3.0\n\n\n2\nFri\nYes\nDinner\n16.3\n3.0\n2.0\n\n\n10\nThur\nYes\nLunch\n16.5\n2.6\n2.0\n\n\n4\nSat\nNo\nDinner\n17.8\n2.8\n2.0\n\n\n6\nSun\nNo\nDinner\n18.4\n3.0\n3.0\n\n\n8\nThur\nNo\nDinner\n18.8\n3.0\n2.0\n\n\n5\nSat\nYes\nDinner\n20.4\n2.7\n2.0\n\n\n0\nFri\nNo\nDinner\n22.5\n3.2\n2.0\n\n\n7\nSun\nYes\nDinner\n23.1\n3.5\n2.0\n\n\n\n\n\n\n\n\nCode\n(\n    grouped.reset_index()\n    .sort_values(by=\"total_bill\")\n    .style.format(precision=1, thousands=\",\", decimal=\".\")\n    .bar(\n        align=0,\n        cmap=\"bwr\",\n        height=50,\n        width=60,\n        props=\"width: 120px; border-right: 1px solid black;\",\n    )\n)\n\n\n\n\n\n\nTable 9: Bar Charts in Your Table with Alignment Set using a float number\n\n\n \nday\nsmoker\ntime\ntotal_bill\ntip\nsize\n\n\n\n\n3\nFri\nYes\nLunch\n12.8\n2.1\n2.0\n\n\n9\nThur\nNo\nLunch\n15.2\n2.1\n2.0\n\n\n1\nFri\nNo\nLunch\n16.0\n3.0\n3.0\n\n\n2\nFri\nYes\nDinner\n16.3\n3.0\n2.0\n\n\n10\nThur\nYes\nLunch\n16.5\n2.6\n2.0\n\n\n4\nSat\nNo\nDinner\n17.8\n2.8\n2.0\n\n\n6\nSun\nNo\nDinner\n18.4\n3.0\n3.0\n\n\n8\nThur\nNo\nDinner\n18.8\n3.0\n2.0\n\n\n5\nSat\nYes\nDinner\n20.4\n2.7\n2.0\n\n\n0\nFri\nNo\nDinner\n22.5\n3.2\n2.0\n\n\n7\nSun\nYes\nDinner\n23.1\n3.5\n2.0"
  },
  {
    "objectID": "posts/pandas_styler/pandas_style.html#formatting",
    "href": "posts/pandas_styler/pandas_style.html#formatting",
    "title": "Mastering Pandas DataFrame Styling for a Stunning Presentation",
    "section": "Formatting",
    "text": "Formatting\nBelow is an illustrative example of the myriad possibilities when it comes to fine-tuning the style of a DataFrame. In this code, we’ve undertaken various stylistic enhancements:\n\nBackground Gradient: We’ve applied a nice background color gradient to the columns along the vertical axis.\nCaption: We’ve set a descriptive caption for the DataFrame.\nVisual Styling: We’ve specified text alignment and introduced hovering options to make highlighted values pop even more.\nColumn Names Formatting: We’ve reformatted our column names for clarity and aesthetics.\nColumn Hiding: We’ve concealed the ‘smoker’ column, decluttering our view.\nIndex Hiding: We’ve also hidden the index labels for a cleaner look.\nNumerical Formatting: We’ve individually specified the numerical representation, including adding a dollar sign for currency values.\n\nThis demonstration underscores the virtually endless possibilities for customizing the appearance of your DataFrame. However, it’s important to note that the physical attributes set by the set_table_styles method won’t be exported to Excel, should you choose to do so. Just a handy tidbit to keep in mind.\n\n\nCode\n# Start by resetting the index and renaming columns with underscores\n# Replace underscores with spaces for better readability\n(\n    grouped.reset_index().rename(columns=lambda x: x.replace(\"_\", \" \"))\n    # Sort the DataFrame by the 'total bill' column\n    .sort_values(by=\"total bill\")\n    # Apply Pandas Styler to format the table\n    .style\n    # Apply background color gradient to columns along the vertical axis (axis=0)\n    .background_gradient(cmap=\"viridis\", axis=0)\n    # Set a caption for the table\n    .set_caption(\"Exploring Dining Trends: Bill Amounts, Tips, and Party Sizes\")\n    # Customize the table's visual styling\n    .set_table_styles(\n        [\n            {\n                \"selector\": \"th.col_heading\",\n                \"props\": \"text-align: center; font-size: 1.5em;\",\n            },\n            {\"selector\": \"td\", \"props\": \"text-align: center;\"},\n            {\n                \"selector\": \"td:hover\",\n                \"props\": \"font-style: italic; color: black; font-weight:bold; background-color : #ffffb3;\",\n            },\n        ],\n        overwrite=False,\n    )\n    # Apply custom formatting to the index labels (convert to uppercase)\n    .format_index(str.upper, axis=1)\n    # Hide the 'smoker' column from the table\n    .hide(subset=[\"smoker\"], axis=1)\n    # Hide the index label (row numbers)\n    .hide(axis=\"index\")\n    # Format specific columns with dollar signs and one decimal place\n    .format(\n        {\n            \"total bill\": \"$ {:.1f}\",\n            \"tip\": \"$ {:.1f}\",\n            \"size\": \"{:.0f}\",\n        }\n    )\n)\n\n\n\n\n\n\nTable 10: Formatting tables\n\n\nDAY\nTIME\nTOTAL BILL\nTIP\nSIZE\n\n\n\n\nFri\nLunch\n$ 12.8\n$ 2.1\n2\n\n\nThur\nLunch\n$ 15.2\n$ 2.1\n2\n\n\nFri\nLunch\n$ 16.0\n$ 3.0\n3\n\n\nFri\nDinner\n$ 16.3\n$ 3.0\n2\n\n\nThur\nLunch\n$ 16.5\n$ 2.6\n2\n\n\nSat\nDinner\n$ 17.8\n$ 2.8\n2\n\n\nSun\nDinner\n$ 18.4\n$ 3.0\n3\n\n\nThur\nDinner\n$ 18.8\n$ 3.0\n2\n\n\nSat\nDinner\n$ 20.4\n$ 2.7\n2\n\n\nFri\nDinner\n$ 22.5\n$ 3.2\n2\n\n\nSun\nDinner\n$ 23.1\n$ 3.5\n2"
  },
  {
    "objectID": "posts/pandas_styler/pandas_style.html#get-a-clearer-overview-with-set_sticky",
    "href": "posts/pandas_styler/pandas_style.html#get-a-clearer-overview-with-set_sticky",
    "title": "Mastering Pandas DataFrame Styling for a Stunning Presentation",
    "section": "Get a Clearer Overview with set_sticky",
    "text": "Get a Clearer Overview with set_sticky\nWhat if you find yourself facing a DataFrame with more columns than can comfortably fit on your screen, yet you still wish to inspect each column individually? In the past, you might have resorted to using pd.set_option('display.max_columns', xyz) to expand the display. However, there’s a much more elegant solution: set_sticky.\nset_sticky introduces a clever CSS trick that permanently pins the index or column headers within a scrolling frame. In our case, although the ‘tips’ DataFrame doesn’t have an excessive number of columns, we’ve concatenated 10 DataFrames together to showcase the remarkable utility of set_sticky. As you scroll horizontally, you’ll notice that you can now conveniently inspect all the columns while the index remains firmly in place, thanks to the magic of set_sticky. Let’s explore this feature below.\n\n\nCode\n(pd.concat([df for i in range(10)], axis=1).head().style.set_sticky(axis=\"index\"))\n\n\n\n\n\n\nTable 12: Demonstrating set_sticky\n\n\n \ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.990000\n1.010000\nFemale\nNo\nSun\nDinner\n2\n16.990000\n1.010000\nFemale\nNo\nSun\nDinner\n2\n16.990000\n1.010000\nFemale\nNo\nSun\nDinner\n2\n16.990000\n1.010000\nFemale\nNo\nSun\nDinner\n2\n16.990000\n1.010000\nFemale\nNo\nSun\nDinner\n2\n16.990000\n1.010000\nFemale\nNo\nSun\nDinner\n2\n16.990000\n1.010000\nFemale\nNo\nSun\nDinner\n2\n16.990000\n1.010000\nFemale\nNo\nSun\nDinner\n2\n16.990000\n1.010000\nFemale\nNo\nSun\nDinner\n2\n16.990000\n1.010000\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.340000\n1.660000\nMale\nNo\nSun\nDinner\n3\n10.340000\n1.660000\nMale\nNo\nSun\nDinner\n3\n10.340000\n1.660000\nMale\nNo\nSun\nDinner\n3\n10.340000\n1.660000\nMale\nNo\nSun\nDinner\n3\n10.340000\n1.660000\nMale\nNo\nSun\nDinner\n3\n10.340000\n1.660000\nMale\nNo\nSun\nDinner\n3\n10.340000\n1.660000\nMale\nNo\nSun\nDinner\n3\n10.340000\n1.660000\nMale\nNo\nSun\nDinner\n3\n10.340000\n1.660000\nMale\nNo\nSun\nDinner\n3\n10.340000\n1.660000\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.010000\n3.500000\nMale\nNo\nSun\nDinner\n3\n21.010000\n3.500000\nMale\nNo\nSun\nDinner\n3\n21.010000\n3.500000\nMale\nNo\nSun\nDinner\n3\n21.010000\n3.500000\nMale\nNo\nSun\nDinner\n3\n21.010000\n3.500000\nMale\nNo\nSun\nDinner\n3\n21.010000\n3.500000\nMale\nNo\nSun\nDinner\n3\n21.010000\n3.500000\nMale\nNo\nSun\nDinner\n3\n21.010000\n3.500000\nMale\nNo\nSun\nDinner\n3\n21.010000\n3.500000\nMale\nNo\nSun\nDinner\n3\n21.010000\n3.500000\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.680000\n3.310000\nMale\nNo\nSun\nDinner\n2\n23.680000\n3.310000\nMale\nNo\nSun\nDinner\n2\n23.680000\n3.310000\nMale\nNo\nSun\nDinner\n2\n23.680000\n3.310000\nMale\nNo\nSun\nDinner\n2\n23.680000\n3.310000\nMale\nNo\nSun\nDinner\n2\n23.680000\n3.310000\nMale\nNo\nSun\nDinner\n2\n23.680000\n3.310000\nMale\nNo\nSun\nDinner\n2\n23.680000\n3.310000\nMale\nNo\nSun\nDinner\n2\n23.680000\n3.310000\nMale\nNo\nSun\nDinner\n2\n23.680000\n3.310000\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.590000\n3.610000\nFemale\nNo\nSun\nDinner\n4\n24.590000\n3.610000\nFemale\nNo\nSun\nDinner\n4\n24.590000\n3.610000\nFemale\nNo\nSun\nDinner\n4\n24.590000\n3.610000\nFemale\nNo\nSun\nDinner\n4\n24.590000\n3.610000\nFemale\nNo\nSun\nDinner\n4\n24.590000\n3.610000\nFemale\nNo\nSun\nDinner\n4\n24.590000\n3.610000\nFemale\nNo\nSun\nDinner\n4\n24.590000\n3.610000\nFemale\nNo\nSun\nDinner\n4\n24.590000\n3.610000\nFemale\nNo\nSun\nDinner\n4\n24.590000\n3.610000\nFemale\nNo\nSun\nDinner\n4"
  },
  {
    "objectID": "posts/pandas_styler/pandas_style.html#concatenating-dataframe-outputs",
    "href": "posts/pandas_styler/pandas_style.html#concatenating-dataframe-outputs",
    "title": "Mastering Pandas DataFrame Styling for a Stunning Presentation",
    "section": "Concatenating DataFrame Outputs",
    "text": "Concatenating DataFrame Outputs\nYou can combine two or more Stylers if they have the same columns. This is particularly handy when presenting summary statistics for a DataFrame\n\n\nCode\n(\n    df.groupby(\"day\")[[\"total_bill\", \"tip\", \"size\"]]\n    .mean()\n    .style.format(precision=1)\n    .concat(\n        df[[\"total_bill\", \"tip\", \"size\"]]\n        .agg([\"mean\", \"median\", \"sum\"])\n        .style.format(precision=1)\n        .relabel_index([\"Average\", \"Median\", \"Sum\"])\n    )\n)\n\n\n\n\n\n\nTable 13: Demonstrating Easily Concatenating Different DataFrame Outputs\n\n\n \ntotal_bill\ntip\nsize\n\n\nday\n \n \n \n\n\n\n\nFri\n17.2\n2.7\n2.1\n\n\nSat\n20.4\n3.0\n2.5\n\n\nSun\n21.4\n3.3\n2.8\n\n\nThur\n17.7\n2.8\n2.5\n\n\nAverage\n19.8\n3.0\n2.6\n\n\nMedian\n17.8\n2.9\n2.0\n\n\nSum\n4827.8\n731.6\n627.0"
  },
  {
    "objectID": "posts/labour_day/labour_day.html",
    "href": "posts/labour_day/labour_day.html",
    "title": "Tidy Tuesday: Union Membership in the United States",
    "section": "",
    "text": "Photo by Claudio Schwarz\n\n\nHappy Labor Day! 🛠️ As we celebrate the achievements of workers and the contributions they’ve made to society, what better way to delve into the world of labor and employment than with a fresh Tidy Tuesday dataset? This week’s data comes from the comprehensive “Union Membership, Coverage, and Earnings from the CPS” dataset, courtesy of Barry Hirsch from Georgia State University, David Macpherson from Trinity University, and William Even from Miami University.\nThis dataset is a rich source of information, shedding light on the intricate dynamics of union membership, coverage, and earnings. So, grab your coffee, get comfortable, and let’s embark on this Tidy Tuesday journey together, exploring the intricate tapestry of labor data!\n\nImport data\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nimport geopandas\nfrom lets_plot.geo_data import *\n\nfrom lets_plot import *\nfrom lets_plot.mapping import as_discrete\n\nLetsPlot.setup_html()\n\n\n\n            \n            \n            \n\n\nThere are three distinct datasets to investigate: one focusing on demographic information, another centered on wage data, and the third associated with various states. Let’s read them all in.\n\n\nCode\ndemographic_df = pd.read_csv(\n    \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-09-05/demographics.csv\"\n)\nwages_df = pd.read_csv(\n    \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-09-05/wages.csv\"\n)\nstates_df = pd.read_csv(\n    \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-09-05/states.csv\"\n)\n\n\n\n\nData dictionaries\nHere are the corresponding data dictionaries:\nDemographics data: Data sources:\n\n1973-1981: May Current Population Survey (CPS)\n1982: No union questions available\n1983-2022: CPS Outgoing Rotation Group (ORG) Earnings Files\n\nThe definition of union membership was expanded in 1977 to include “employee associations similar to a union”.\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nyear\ndouble\nWhen the data was collected.\n\n\nsample_size\ndouble\nThe number of wage and salary workers ages 16 and over who were surveyed.\n\n\nemployment\ndouble\nWage and salary employment in thousands.\n\n\nmembers\ndouble\nEmployed workers who are union members in thousands.\n\n\ncovered\ndouble\nWorkers covered by a collective bargaining agreement in thousands.\n\n\np_members\ndouble\nPercent of employed workers who are union members.\n\n\np_covered\ndouble\nPercent of employed workers who are covered by a collective bargaining agreement.\n\n\nfacet\ncharacter\nThe sector or demographic group contained in this row of data.\n\n\n\nWages data: Data sources:\n\n1973-1981: May Current Population Survey (CPS)\n1982: No union questions available\n1983-2022: CPS Outgoing Rotation Group (ORG) Earnings Files\n\nThe definition of union membership was expanded in 1977 to include “employee associations similar to a union”.\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nyear\ndouble\nWhen the data was collected.\n\n\nsample_size\ndouble\nThe number of wage and salary workers ages 16 and over who were surveyed and provided earnings and hours worked information.\n\n\nwage\ndouble\nMean hourly earnings in nominal dollars.\n\n\nat_cap\ndouble\nPercent of workers with weekly earnings at the top code of $999 through 1988, $1923 in 1989-97, and $2885 beginning in 1998, with individuals assigned mean earnings above the cap based on annual estimates of the gender-specific Pareto distribution.\n\n\nunion_wage\ndouble\nMean wage among union members.\n\n\nnonunion_wage\ndouble\nMean wage among nonunion workers.\n\n\nunion_wage_premium_raw\ndouble\nThe percentage difference between the union and nonunion wage.\n\n\nunion_wage_premium_adjusted\ndouble\nEstimated as exp(b)-1 where b is the regression coefficient on a union membership variable (equal to 1 if union and 0 otherwise) from a semi-logarithmic wage equation, with controls included for worker/job characteristics. Included in the all-worker wage equation are the control variables: years of schooling, potential years of experience [proxied by age minus years of schooling minus 6] and its square [both interacted with gender], and categorical variables for marital status, race and ethnicity, gender, part-time, large metropolitan area, state, public sector, broad industry, and broad occupation. Controls are omitted, as appropriate, for estimates within sectors or by demographic group [i.e., by class, gender, race, or industry sector]. Workers who do not report earnings but instead have them imputed [i.e., assigned] by the Census are removed from the estimation samples in all years, except 1994 and 1995 when imputed earners cannot be identified. Inclusion of imputed earners causes union wages to be understated, nonunion wages overstated, and union-nonunion wage differences understated. For 1994-95, the sample includes imputed earners and estimates in those years have been adjusted to remove the bias from imputation.\n\n\nfacet\ncharacter\nThe sector or demographic group contained in this row of data.\n\n\n\nStates data:\nData source: Current Population Survey (CPS) Outgoing Rotation Group (ORG) Earnings Files\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nstate_census_code\ndouble\nCensus state code used in CPS\n\n\nstate\ncharacter\nState name.\n\n\nsector\ncharacter\nEmployment sector.\n\n\nobservations\ndouble\nCPS sample size.\n\n\nemployment\ndouble\nWage and salary employment in thousands.\n\n\nmembers\ndouble\nEmployed workers who are union members in thousands.\n\n\ncovered\ndouble\nWorkers covered by a collective bargaining agreement in thousands.\n\n\np_members\ndouble\nPercent of employed workers who are union members.\n\n\np_covered\ndouble\nPercent of employed workers who are covered by a collective bargaining agreement.\n\n\nstate_abbreviation\ncharacter\nState abbreviation.\n\n\nyear\ndouble\nYear of the survey.\n\n\n\nAs in our previous Tidy Tuesday blog, I believe we can address the following questions using this dataset:\n\nWhat are the overarching trends in the labor force, particularly regarding union memberships?\nDo specific demographic groups or occupations display a higher likelihood of union membership?\nDo union members experience any financial advantages or benefits compared to non-union workers?\nWhich states have the highest number of union members or affiliated unions?\n\n\n\nWhat are the overarching trends in the labor force, particularly regarding union memberships?\nThe figure below illustrates a consistent upward trajectory in the workforce’s growth, whereas union membership and the number of individuals covered by unions have been on a declining trend over the decades.\n\n\nCode\n(\n    demographic_df.query(\"facet.str.contains('all wage')\")\n    .drop(columns=[\"sample_size\", \"facet\", \"p_members\", \"p_covered\"])\n    .melt(id_vars=\"year\", var_name=\"Description\")\n    .pipe(\n        lambda df: ggplot(df)\n        + geom_line(aes(\"year\", \"value\", color=\"Description\"), size=1.5)\n        + labs(\n            title=\"Evolution of Workforce Size and Union Membership Across the Years\",\n            subtitle=\"While employment numbers have been on the rise, the proportion of employed workers \\nwho are union members and covered by unions has been steadily decreasing.\",\n            x=\"\",\n            y=\"Number of people in thousands\",\n            caption=\"© 2023 by Barry T. Hirsch, David A. Macpherson, and William E. Even.\",\n        )\n        + theme(\n            plot_subtitle=element_text(size=12, face=\"italic\"),\n            plot_title=element_text(size=15, face=\"bold\"),\n        )\n    )\n)\n\n\n\n   \n   \nFigure 1: Evolution of Workforce Size and Union Membership Across the Years\n\n\n\n\n\nDo specific demographic groups or occupations display a higher likelihood of union membership?\nNext, our investigation will focus on identifying potential demographic disparities among workforce members and uncovering the professions with the highest likelihood of union membership. We’ll utilize the “p_members” column, which represents the percentage of employed workers who are union members, to delve into these aspects.\nAs evident in Figure 2, a substantial decrease in union membership is observable across nearly all demographic groups over the decades.\n\n\nCode\n(\n    demographic_df.query(\"facet.str.contains('demographics')\")\n    .assign(facet=lambda df: df.facet.replace({\"demographics:\": \"\"}, regex=True))\n    .pipe(\n        lambda df: ggplot(df, aes(\"year\", \"p_members\", color=\"facet\"))\n        + geom_line(\n            size=1.5,\n        )\n        + labs(\n            title=\"Demographic Trends in Union Membership as a Percentage of the Total Workforce Over Time\",\n            subtitle=\"Almost all demographic groups exhibit a significant decline in union membership over the decades.\",\n            x=\"\",\n            y=\"Percent of employed workers who are union members\",\n            caption=\"© 2023 by Barry T. Hirsch, David A. Macpherson, and William E. Even.\",\n        )\n        + theme(\n            plot_subtitle=element_text(size=12, face=\"italic\"),\n            plot_title=element_text(size=15, face=\"bold\"),\n        )\n        + ggsize(800, 500)\n    )\n)\n\n\n\n   \n   \nFigure 2: Evolution of Workforce Size and Union Membership Across the Years\n\n\n\nWhile certain professions exhibit declining trends in union memberships, it’s noteworthy that some public sector occupations, such as postal service, police, and local government jobs, maintain the highest and most consistent levels of union participation (Figure 3).\n\n\nCode\n(\n    demographic_df.query(\n        \"(~facet.str.contains('demographics')) and (~facet.str.contains('all'))\"\n    ).pipe(\n        lambda df: ggplot(df, aes(\"year\", \"p_members\", color=\"facet\"))\n        + geom_line(\n            size=1.5,\n        )\n        + labs(\n            title=\"Workforce Sector-Specific Trends in Union Membership\",\n            subtitle=\"Public sector occupations demonstrate higher union participation rates.\",\n            x=\"\",\n            y=\"Percent of employed workers who are union members\",\n            caption=\"© 2023 by Barry T. Hirsch, David A. Macpherson, and William E. Even.\",\n        )\n        + theme(\n            plot_subtitle=element_text(size=12, face=\"italic\"),\n            plot_title=element_text(size=15, face=\"bold\"),\n        )\n        + ggsize(800, 500)\n    )\n)\n\n\n\n   \n   \nFigure 3: Workforce Sector-Specific Trends in Union Membership\n\n\n\n\n\nDo union members experience any financial advantages or benefits compared to non-union workers?\nThe next figure is both straightforward and highly impactful, providing a direct comparison of average wages for union members versus non-union members.\n\n\nCode\n(\n    wages_df.query(\"facet == 'all wage and salary workers'\")\n    .drop(\n        columns=[\n            \"wage\",\n            \"sample_size\",\n            \"at_cap\",\n            \"union_wage_premium_raw\",\n            \"union_wage_premium_adjusted\",\n            \"facet\",\n        ]\n    )\n    .melt(id_vars=\"year\", var_name=\"Description\")\n    .pipe(\n        lambda df: ggplot(df, aes(\"year\", \"value\", color=\"Description\"))\n        + geom_line(\n            size=1.5,\n        )\n        + labs(\n            title=\"Differences in Mean Hourly Earnings for Union and Non-Union Workers\",\n            subtitle=\"Union workers consistently display higher mean wages in comparison to non-union members.\",\n            x=\"\",\n            y=\"Mean hourly earnings in nominal dollars\",\n            caption=\"© 2023 by Barry T. Hirsch, David A. Macpherson, and William E. Even.\",\n        )\n        + theme(\n            plot_subtitle=element_text(size=12, face=\"italic\"),\n            plot_title=element_text(size=15, face=\"bold\"),\n        )\n    )\n)\n\n\n\n   \n   \nFigure 4: Differences in Mean Hourly Earnings for Union and Non-Union Workers\n\n\n\nAs with our previous figures, we will now delve into demographic variations and job sector disparities. To unveil these potential distinctions, we will use the “union_wage_premium_raw” column, which indicates the percentage difference between union and non-union wages.\nThe most significant disparities in hourly wages (Figure 5) between unionized and non-unionized workers are observed within the construction sector. These disparities have fluctuated over the decades, ranging from below 40% to as high as 80%. It’s important to note that almost all sectors exhibit higher earnings for union members, except for federal, manufacturing, and wholesale/retail workers (particularly in recent times).\n\n\nCode\n(\n    wages_df.query(\n        \"(~facet.str.contains('demographics')) and (~facet.str.contains('all'))\"\n    ).pipe(\n        lambda df: ggplot(df, aes(\"year\", \"union_wage_premium_raw\", color=\"facet\"))\n        + geom_line(\n            size=1.5,\n        )\n        + labs(\n            title=\"Sector-specific Differences in Mean Hourly Earnings for Union and Non-Union Workers\",\n            subtitle=\"The disparities between wages range from -20% to +80%.\",\n            x=\"\",\n            y=\"The percentage difference between the union and nonunion wage\",\n            caption=\"© 2023 by Barry T. Hirsch, David A. Macpherson, and William E. Even.\",\n        )\n        + theme(\n            plot_subtitle=element_text(size=12, face=\"italic\"),\n            plot_title=element_text(size=15, face=\"bold\"),\n        )\n        + ggsize(800, 500)\n        + geom_hline(yintercept=0, linetype=5, color=\"#6c6c6c\", size=1)\n    )\n)\n\n\n\n   \n   \nFigure 5: Sector-specific Differences in Mean Hourly Earnings for Union and Non-Union Workers\n\n\n\nIn addition to gender-specific differences, it’s intriguing to observe the relationship between education levels and wage disparities between unionized and non-unionized workers. Workers with education levels below college tend to benefit more from union membership, whereas, in general, individuals with higher education levels tend to have lower wages when affiliated with a union.\n\n\nCode\n(\n    wages_df.query(\"facet.str.contains('demographics')\")\n    .assign(facet=lambda df: df.facet.replace({\"demographics:\": \"\"}, regex=True))\n    .pipe(\n        lambda df: ggplot(df, aes(\"year\", \"union_wage_premium_raw\", color=\"facet\"))\n        + geom_line(\n            size=1.5,\n        )\n        + labs(\n            title=\"Demographics-specific Differences in Mean Hourly Earnings for Union and Non-Union Workers\",\n            subtitle=\"It appears that females tend to benefit more from union membership in comparison to males, \\nparticularly in terms of the percentage difference between union and non-union wages.\",\n            x=\"\",\n            y=\"The percentage difference between the union and nonunion wage\",\n            caption=\"© 2023 by Barry T. Hirsch, David A. Macpherson, and William E. Even.\",\n        )\n        + theme(\n            plot_subtitle=element_text(size=12, face=\"italic\"),\n            plot_title=element_text(size=15, face=\"bold\"),\n        )\n        + ggsize(800, 500)\n        + geom_hline(yintercept=0, linetype=5, color=\"#6c6c6c\", size=1)\n    )\n)\n\n\n\n   \n   \nFigure 6: Demographics-specific Differences in Mean Hourly Earnings for Union and Non-Union Workers\n\n\n\n\n\nWhich states have the highest number of union members or affiliated unions?\nTo illustrate variations in union membership across states, we will leverage Lets Plot’s capabilities for visualizing spatial data. Given the extensive dataset, our focus will be on highlighting a subset of the data, specifically the most recent trends in 2022 across all sectors.\nAs evident in Figure 7, certain states like New York, Alaska, Hawaii, Washington, California, and Oregon have a significant portion of their workforce as union members. Conversely, states like Utah, South Dakota, and the Carolinas display lower percentages of union membership among their workforce.\n\n\nCode\nfiltered_data = states_df.query(\"sector == 'Total' and year == 2022\")\nboundaries = (\n    geocode_states(filtered_data[\"state\"])\n    .scope(\"USA\")\n    .inc_res(4)\n    .get_boundaries(resolution=4)\n)\n\n(\n    ggplot()\n    + geom_livemap()\n    + geom_polygon(\n        aes(color=\"p_members\", fill=\"p_members\"),\n        data=states_df.query(\"sector == 'Total' and year == 2022\"),\n        map=boundaries,\n        alpha=0.9,\n        map_join=[[\"state\"], [\"state\"]],\n        show_legend=False,\n        size=0.01,\n    )\n    + theme(\n        axis_title=\"blank\", axis_text=\"blank\", axis_ticks=\"blank\", axis_line=\"blank\"\n    )\n    + scale_fill_brewer(palette=\"Greens\")\n    + ggsize(800, 500)\n)\n\n\n\n   \n   \nFigure 7: Percentage of Union Members Across States\n\n\n\nThere you have it! I hope you found this week’s Tidy Tuesday analysis insightful. I would encourage everyone to dive deeper into this dataset as I’ve only scratched the surface; there’s a wealth of knowledge waiting to be uncovered here.\nHappy coding, and until next time, see you in our next exploration!"
  },
  {
    "objectID": "posts/human_day/human_day.html",
    "href": "posts/human_day/human_day.html",
    "title": "Tidy Tuesday: The Global Human Day",
    "section": "",
    "text": "Photo by Filip Mroz\nThis week’s Tidy Tuesday takes us to The Human Chronome Project, an initiative based at McGill University in Montreal. We’re exploring insights from their paper titled “The global human day in PNAS” and the accompanying dataset on Zenodo.\nThe publication offers a broad view of our species’ activities, shedding light on how economic activities fit into the grand scheme of life. It also highlights activities with significant potential for change. For a deeper dive into our research methodology and to explore supplementary visualizations, I encourage you to peruse the supplementary materials provided with the publication."
  },
  {
    "objectID": "posts/human_day/human_day.html#regression-diagnostic",
    "href": "posts/human_day/human_day.html#regression-diagnostic",
    "title": "Tidy Tuesday: The Global Human Day",
    "section": "Regression diagnostic",
    "text": "Regression diagnostic\nNext, we’ll perform some diagnostic procedures for our regression model. These diagnostics will serve the purpose of evaluating how well our model aligns with its underlying assumptions. This step is crucial for ensuring the reliability and validity of our regression analysis.\nAs depicted in Figure 4, our residuals exhibit a relatively normal distribution. While there are a few data points that may have contributed to some erroneous predictions, we have opted to maintain the current state of the analysis as the results appear reasonable.\n\n\nCode\n# Create a histogram plot of residuals\nhist_plot = res.resid.to_frame(name=\"residuals\").pipe(\n    lambda df: ggplot(df, aes(\"residuals\"))\n    + geom_histogram(bins=15)\n    + labs(\n        title=\"Histogram of Residuals\",\n    )\n    + theme(plot_title=element_text(size=15, face=\"bold\"))\n)\n\n# Create a residuals plot\nresiduals_plot = (\n    res.resid.to_frame(name=\"residuals\")\n    .reset_index()  # Reset the index for plotting\n    .pipe(\n        lambda df: ggplot(df, aes(\"index\", \"residuals\"))\n        + geom_point(size=5, alpha=0.05)  # Plot residuals as points\n        + labs(\n            title=\"Residuals\",  # Title for the residuals plot\n        )\n        + theme(\n            plot_title=element_text(size=15, face=\"bold\")  # Customize title appearance\n        )\n    )\n)\n\n\n# Create a Q-Q plot of residuals\nqqplot = res.resid.to_frame(name=\"residuals\").pipe(\n    lambda df: qq_plot(data=df, sample=\"residuals\", size=5, alpha=0.05)\n    + labs(\n        title=\"Q-Q Plot\",\n    )\n    + theme(plot_title=element_text(size=15, face=\"bold\"))\n)\n\n# Create a fit plot comparing predicted values and log(GDP per population)\nfit_plot = (\n    pd.concat([res.fittedvalues, df.gdp_per_population_log], axis=1)\n    .rename(\n        columns={\n            0: \"Predicted value\",\n            \"gdp_per_population_log\": \"log(GDP per population)\",\n        }\n    )\n    .pipe(\n        lambda df: ggplot(df, aes(\"Predicted value\", \"log(GDP per population)\"))\n        + geom_point(size=5, alpha=0.05)\n        + labs(\n            title=\"Fit Plot\",\n        )\n        + theme(plot_title=element_text(size=15, face=\"bold\"))\n        + geom_abline(slope=1, size=1, linetype=\"dashed\", color=\"red\")\n    )\n)\n\n# Combine the three plots into a grid with two columns\n(gggrid([hist_plot, residuals_plot, qqplot, fit_plot], ncol=2) + ggsize(800, 600))\n\n\n\n   \n   \nFigure 4: Regression diagnostic\n\n\n\nAnd there you have it, folks! We hope you’ve found this week’s exploration insightful. Happy coding to all, and I look forward to seeing you next week for more exciting discoveries and learning opportunities. Until then, stay curious and keep exploring! 👩‍💻👨‍💻"
  },
  {
    "objectID": "posts/2023-08-08-HotOnes/2023_08_08_HotOnes.html",
    "href": "posts/2023-08-08-HotOnes/2023_08_08_HotOnes.html",
    "title": "Tidy Tuesday: Hot Ones Episodes",
    "section": "",
    "text": "I’ve decided to get more involved in the Tidy Tuesday movement. I think it’s a really enjoyable way to improve my skills in working with data, like analyzing, organizing, and visualizing it. The cool datasets they provide make it even more interesting. More information on Tidy Tuesday and their past datasets can be found here.\nThis week we have a dataset related to the show Hot Ones. Hot Ones is a unique web series that combines spicy food challenges with celebrity interviews. Hosted by Sean Evans, guests tackle increasingly hot chicken wings while answering questions, leading to candid and entertaining conversations. The show’s blend of heat and honesty has turned it into a global sensation, offering a fresh take on interviews and captivating audiences worldwide.\nLet’s see what we can learn from the data 🔬🕵️‍♂️."
  },
  {
    "objectID": "posts/2023-08-08-HotOnes/2023_08_08_HotOnes.html#what-differences-can-be-observed-in-the-spiciness-of-sauces-as-we-look-across-the-various-seasons",
    "href": "posts/2023-08-08-HotOnes/2023_08_08_HotOnes.html#what-differences-can-be-observed-in-the-spiciness-of-sauces-as-we-look-across-the-various-seasons",
    "title": "Tidy Tuesday: Hot Ones Episodes",
    "section": "What differences can be observed in the spiciness of sauces as we look across the various seasons?",
    "text": "What differences can be observed in the spiciness of sauces as we look across the various seasons?\n\n\nCode\n(sauces\n .groupby('season')\n .agg(AVG_scoville=('scoville', 'mean'),\n      median_scoville=('scoville', 'median'),\n     )\n .reset_index()\n .melt(id_vars='season',\n       var_name='statistics',\n      )\n .pipe(lambda df: (ggplot(df, aes('season', 'value',fill='statistics'))\n                   + geom_bar(stat='identity', show_legend= False)\n                   + facet_wrap('statistics',nrow=1,scales='free_y')\n                   + labs(x='Seasons',\n                          y='Scoville Units'\n                         )\n                   + theme(plot_title=element_text(size=20,face='bold'))\n                  + ggsize(1000,600)\n                  )\n      )\n \n)\n\n\n\n   \n   \nFigure 1: Average and median Scoville Units across the Hot Ones’ seasons\n\n\n\nThere seems to be a shift during the season 3-5 period as can be seen in Figure 1. Both indicators – mean and median Scoville Units – show a consistent upward trend over this time frame and later on they stabilize.\nWhat about the overall spread of the data? 🤔\n\n\nCode\n(sauces\n .loc[:, ['season', 'scoville']]\n .pipe(lambda df: (ggplot(df, aes('season', 'scoville'))\n                   + geom_boxplot()\n                   + scale_y_log10()\n                   + labs(x='Seasons',\n                          y='log(Scoville Units)'\n                         )\n                   + theme(plot_title=element_text(size=20,face='bold'))\n                   + ggsize(1000,600)\n                  )\n      )\n)\n\n\n\n   \n   \nFigure 2: Spread of Scoville Units across the Hot Ones’ seasons\n\n\n\nHere are some observations: Season 5 exhibits the widest range, featuring sauces with Scoville Units spanning from 450 to 2,000,000. In addition, starting from season 6 onwards, the averages, medians, and ranges of Scoville Units appear to even out."
  },
  {
    "objectID": "posts/2023-08-08-HotOnes/2023_08_08_HotOnes.html#has-every-individual-successfully-completed-all-the-episodes",
    "href": "posts/2023-08-08-HotOnes/2023_08_08_HotOnes.html#has-every-individual-successfully-completed-all-the-episodes",
    "title": "Tidy Tuesday: Hot Ones Episodes",
    "section": "Has every individual successfully completed all the episodes?",
    "text": "Has every individual successfully completed all the episodes?\nTo answer this question we will use the episodes dataframe. Keep in mind there are 300 episodes in this dataframe. The finished column can be useful here. Just by looking for entries where finished == False we will have our answer.\n\n\nCode\n(episodes\n .query(\"finished==False\")\n [['season', 'guest', 'guest_appearance_number']]\n)\n\n\n\n\n\n\n\n\n\nseason\nguest\nguest_appearance_number\n\n\n\n\n0\n1\nTony Yayo\n1\n\n\n7\n1\nDJ Khaled\n1\n\n\n19\n2\nMike Epps\n1\n\n\n20\n2\nJim Gaffigan\n1\n\n\n24\n2\nRob Corddry\n1\n\n\n51\n3\nRicky Gervais\n1\n\n\n90\n4\nMario Batali\n1\n\n\n96\n5\nTaraji P. Henson\n1\n\n\n129\n7\nLil Yachty\n1\n\n\n130\n7\nE-40\n1\n\n\n144\n8\nShaq\n1\n\n\n171\n10\nChance the Rapper\n1\n\n\n185\n12\nEric André\n2\n\n\n218\n15\nQuavo\n1\n\n\n251\n17\nPusha T\n1\n\n\n\n\n\n\n\nTaking a closer look, it seems that around 15 participants didn’t make it through the entire Hot Ones interview challenge.Not bad out of 300 shows. And guess what? Eric André popped up on the show not just once, but twice! Now, the big question: did he conquer the hot seat in at least one of those interviews? Let’s plot the data to make it more visual…\n\n\nCode\n(episodes\n .query(\"finished==False\")\n .groupby('season')\n .finished\n .count()\n .to_frame()\n .reset_index()\n .pipe(lambda df: (ggplot(df, aes('season', 'finished'))\n                   + geom_bar(stat='identity')\n                   + labs(x='Seasons',\n                          y='Number of incomplete interviews'\n                         )\n                   + theme(plot_title=element_text(size=20,face='bold'))\n                   + ggsize(600,400)\n                  )\n      )\n\n)\n\n\n\n   \n   \nFigure 3: Number of incomplete interviews per season\n\n\n\nInterestingly, a majority of these incomplete interviews belong to season 2 (Figure 3). This fact is quite surprising, especially when you consider that the maximum Scoville value for that season was only 550,000 – nearly a quarter of the following year’s value, where only one person faced difficulty finishing the challenge."
  },
  {
    "objectID": "posts/2023-08-08-HotOnes/2023_08_08_HotOnes.html#what-is-the-completion-rate-per-season",
    "href": "posts/2023-08-08-HotOnes/2023_08_08_HotOnes.html#what-is-the-completion-rate-per-season",
    "title": "Tidy Tuesday: Hot Ones Episodes",
    "section": "What is the completion rate per season?",
    "text": "What is the completion rate per season?\nTo get to the bottom of this question, let’s start by figuring out how many episodes were in each season. We can grab this info from the season dataset. Just a heads-up, in season 9 they seem to threw in an extra episode. So, keep this in mind! Otherwise, you might end up with percentages that go beyond 100%.\n\n\nCode\n# First we need to find out the total number of episodes per season\n\nepisodes_per_season = (season\n                       [['season', 'episodes', 'note']]\n                       .set_index('season')\n                       # we need to extract the one extra episode in season 9\n                       .assign(note=lambda df: df.note\n                               .str.extract(r'([0-9.]+)')\n                               .astype(float),\n                        # add the two column together\n                               episodes=lambda df: df.episodes\n                               .add(df.note, fill_value=0)\n                              )\n                       .drop(columns='note')\n                       .squeeze()\n                      )\n\n\n\n\nCode\ncompletion_rate = (episodes\n                   .query(\"finished==True\")\n                   .groupby('season')\n                   .finished\n                   .sum()\n                   .div(episodes_per_season)\n                   .mul(100)\n                   .to_frame().reset_index()\n                   .rename(columns={0:'completion_rate'})\n                  )\n                   \n(completion_rate                  \n .pipe(lambda df: (ggplot(df, aes('season', 'completion_rate'))\n                   + geom_line(stat='identity')\n                   + labs(x='Seasons',\n                          y='% successful participants'\n                         )\n                   + theme(plot_title=element_text(size=20,face='bold'))\n                   + ggsize(600,400)\n                  )\n      )\n)\n\n\n\n   \n   \nFigure 4: Completion rate per season\n\n\n\nTaking a peek at Figure 4, it seems like the normalized completion rate hits its lowest point in season 1, closely followed by season 7. However, even in those seasons, the rate remains surprisingly high."
  },
  {
    "objectID": "posts/2023-08-08-HotOnes/2023_08_08_HotOnes.html#is-there-a-correlation-between-scoville-units-and-completion-rate",
    "href": "posts/2023-08-08-HotOnes/2023_08_08_HotOnes.html#is-there-a-correlation-between-scoville-units-and-completion-rate",
    "title": "Tidy Tuesday: Hot Ones Episodes",
    "section": "Is there a correlation between Scoville Units and completion rate?",
    "text": "Is there a correlation between Scoville Units and completion rate?\nHere’s a curious thought: could there be a link between Scoville Units and the completion rate? I’m just wondering if the spiciness level affects how well participants handle the challenge. Exploring this connection might add a spicy twist to the Hot Ones experience – let’s see where the data takes us!\n\n\nCode\n# AVG_scoville code comes from a code snippet \n# 'What differences can be observed in the spiciness \n# of sauces as we look across the various seasons?'\n\nAVG_scoville = (sauces\n                .groupby('season')\n                .agg(AVG_scoville=('scoville', 'mean'))\n                .squeeze()\n               )\n\n# Let's calculate the Pearson correlation coefficient. We have to discard the last value\n# as the completion rate is not defined for that\n\nstats.pearsonr(AVG_scoville.values[:-1],completion_rate.completion_rate.values[:-1])\n\n\nPearsonRResult(statistic=0.5039021286250108, pvalue=0.02349225734224539)\n\n\nThe Pearson correlation coefficient has its say: there’s actually a moderate positive correlation(0.5, p&lt;0.05) between Scoville Units and completion rate. Quite intriguing, isn’t it? Honestly, I was expecting the opposite outcome myself! It seems like the higher the average spiciness, the more determined the guests become. Take a look at Figure 5.\n\n\nCode\n(pd.concat([AVG_scoville,completion_rate],axis=1)\n .rename(columns={0:'success_rate'})\n .pipe(lambda df: (ggplot(df, aes('AVG_scoville', 'completion_rate'))\n                   + geom_point(size=5, alpha=0.5)\n                   + geom_smooth()\n                   + labs(x='Average Scoville units',\n                          y='% successful participants'\n                         )\n                   + theme(plot_title=element_text(size=20,face='bold'))\n                   + ggsize(600,400)\n                  )\n      )\n)\n\n\n\n   \n   \nFigure 5: Correlation between Average Scoville units and Completion rates"
  },
  {
    "objectID": "posts/2023-08-08-HotOnes/2023_08_08_HotOnes.html#are-there-any-returning-guests",
    "href": "posts/2023-08-08-HotOnes/2023_08_08_HotOnes.html#are-there-any-returning-guests",
    "title": "Tidy Tuesday: Hot Ones Episodes",
    "section": "Are there any returning guests?",
    "text": "Are there any returning guests?\nHas there been a brave soul who dared to make a return to the show for a second time? The column guest_appearance_number holds the answers you’re looking for.\n\n\nCode\n(episodes\n .query(\"guest_appearance_number &gt; 1\")\n [['guest','season', 'episode_season', 'finished']]\n)\n\n\n\n\n\n\n\n\n\nguest\nseason\nepisode_season\nfinished\n\n\n\n\n124\nEddie Huang\n6\n13\nTrue\n\n\n161\nJay Pharoah\n9\n999\nTrue\n\n\n183\nTom Segura\n12\n1\nTrue\n\n\n185\nEric André\n12\n3\nFalse\n\n\n188\nT-Pain\n12\n6\nTrue\n\n\n189\nAdam Richman\n12\n7\nTrue\n\n\n190\nAction Bronson\n12\n8\nTrue\n\n\n203\nNaN\n13\n11\nTrue\n\n\n214\nRussell Brand\n14\n11\nTrue\n\n\n215\nSteve-O\n14\n12\nTrue\n\n\n241\nGordon Ramsay\n16\n14\nTrue\n\n\n254\nPost Malone\n18\n1\nTrue\n\n\n\n\n\n\n\nIt looks like a total of 12 individuals have taken on the challenge not once, but twice. Hats off to their courage! 🎩"
  },
  {
    "objectID": "articles.html",
    "href": "articles.html",
    "title": "Articles",
    "section": "",
    "text": "Tidy Tuesday: The Global Human Day\n\n\n\n\n\n\n\nTidy Tuesday\n\n\n\n\n\n\n\n\n\n\n\nSep 11, 2023\n\n\nAdam Cseresznye\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday: Union Membership in the United States\n\n\n\n\n\n\n\nTidy Tuesday\n\n\n\n\n\n\n\n\n\n\n\nSep 5, 2023\n\n\nAdam Cseresznye\n\n\n\n\n\n\n  \n\n\n\n\nMastering Pandas DataFrame Styling for a Stunning Presentation\n\n\n\n\n\n\n\nPandas\n\n\n\n\n\n\n\n\n\n\n\nAug 27, 2023\n\n\nAdam Cseresznye\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday: Refugees\n\n\n\n\n\n\n\nTidy Tuesday\n\n\n\n\n\n\n\n\n\n\n\nAug 22, 2023\n\n\nAdam Cseresznye\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday: Spam E-mail\n\n\n\n\n\n\n\nTidy Tuesday\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2023\n\n\nAdam Cseresznye\n\n\n\n\n\n\n  \n\n\n\n\nThe Ultimate Speed Test: Pandas vs Polars\n\n\n\n\n\n\n\nPolars\n\n\nPandas\n\n\n\n\n\n\n\n\n\n\n\nAug 13, 2023\n\n\nAdam Cseresznye\n\n\n\n\n\n\n  \n\n\n\n\nGetting Started with Polars\n\n\n\n\n\n\n\nPolars\n\n\n\n\n\n\n\n\n\n\n\nAug 11, 2023\n\n\nAdam Cseresznye\n\n\n\n\n\n\n  \n\n\n\n\n🥳🎉 Two IBM certificates and some geospatial data\n\n\n\n\n\n\n\ncelebration\n\n\ngeospatial data\n\n\n\n\n\n\n\n\n\n\n\nAug 10, 2023\n\n\nAdam Cseresznye\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday: Hot Ones Episodes\n\n\n\n\n\n\n\nTidy Tuesday\n\n\n\n\n\n\n\n\n\n\n\nAug 9, 2023\n\n\nAdam Cseresznye\n\n\n\n\n\n\n  \n\n\n\n\nHow to use the Lets-Plot library by JetBrains\n\n\n\n\n\n\n\nLets-Plot\n\n\nggplot2\n\n\n\n\n\n\n\n\n\n\n\nAug 5, 2023\n\n\nAdam Cseresznye\n\n\n\n\n\n\n  \n\n\n\n\nTop 10 things I learned from the book Effective Pandas by Matt Harrison\n\n\n\n\n\n\n\nPandas\n\n\n\n\n\n\n\n\n\n\n\nAug 2, 2023\n\n\nAdam Cseresznye\n\n\n\n\n\n\n  \n\n\n\n\n🏆🥉3rd place at Kaggle’s Classification with a Tabular Vector Borne Disease Dataset\n\n\n\n\n\n\n\ncelebration\n\n\nKaggle\n\n\n\n\n\n\n\n\n\n\n\nJun 4, 2023\n\n\nAdam Cseresznye\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\nHi there! Welcome to my website! \n",
    "section": "",
    "text": "Now is better than never…\n\n\n— Tim Peters, The Zen of Python\n\n\n\nHi there! Welcome to my website! \n\nI am Adam, a PhD candidate at the University of Antwerp focusing on the field of environmental toxicology. If you’re curious about my academic journey and accomplishments, feel free to explore my scientific output for more information.\nThis platform is mainly dedicated to my pursuit of mastering data science, and I would invite you to join me. Here, you’ll come across various projects, blog posts, and valuable tidbits that could inspire your own endeavors.\nLet’s embark on this data-driven journey together!"
  },
  {
    "objectID": "posts/folium/2023-07-09-Folium.html",
    "href": "posts/folium/2023-07-09-Folium.html",
    "title": "🥳🎉 Two IBM certificates and some geospatial data",
    "section": "",
    "text": "I’m happy to share that I’ve recently completed both IBM’s Data Analyst and Data Science Professional Certificates within the past month. The course content was well-structured, and I learned a great deal from these programs. For instance, I’ve always been interested in learning SQL, and this was the perfect chance to start exploring it.\nIf you’re curious about these certificates, you can find more information through the links provided below. But my learning journey doesn’t stop here—I’m planning to tackle most of the courses listed in the Data Science learning path on Coursera, so there’s more to come.\nWhile I’m at it, I wanted to introduce you to a neat library called Folium, which is fantastic for working with geospatial data. I came across Folium during the capstone project of the Data Science Specialization, where we had a fun task of predicting and visualizing the success of SpaceX rocket launches.\nIn this post, I’ll briefly share what I’ve learned about this library. I hope you’ll find it useful too. Let’s dive in!\n\n\nCode\nimport folium\nimport pandas as pd\nimport os\nfrom folium import plugins\n\n\nWe’ll be utilizing the dataset made available by https://open.toronto.ca/. This dataset includes the locations of bicycles installed on sidewalks and boulevards across the City of Toronto, wherever there’s a requirement for public bicycle parking facilities. By the way, I discovered this dataset through the Awesome Public Datasets repository on GitHub. If you haven’t already, I recommend checking them out.\n\n\nCode\n# Let's read in the file\n\nfor file in os.listdir():\n    if file.endswith(\".csv\"):\n        toronto_df = pd.read_csv(file)\n\n        print(f\"{file} read in as pandas dataframe\")\n\n\nStreet bicycle parking data - 4326.csv read in as pandas dataframe\n\n\nConsidering the original dataset has over 17,300 entries, we’ll keep things light by working with just 500 rows for now. It’s all for the sake of a demonstration, after all!\n\n\nCode\ntoronto_df = toronto_df.sample(n=500)\ntoronto_df.head()\n\n\n\n\n\n\n\n\n\n_id\nOBJECTID\nID\nADDRESSNUMBERTEXT\nADDRESSSTREET\nFRONTINGSTREET\nSIDE\nFROMSTREET\nDIRECTION\nSITEID\nWARD\nBIA\nASSETTYPE\nSTATUS\nSDE_STATE_ID\nX\nY\nLONGITUDE\nLATITUDE\ngeometry\n\n\n\n\n784\n4481427\n10424\nBP-05283\n15\nDundonald St\nNaN\nNaN\nDundonald St\nNaN\nNaN\n13.0\nNaN\nRing\nTemporarily Removed\n0\nNaN\nNaN\nNaN\nNaN\n{'type': 'MultiPoint', 'coordinates': [[-79.38...\n\n\n3297\n4483940\n15253\nBP-35603\n49\nHarbour Sq\nQueens Quay W\nSouth\nHarbour Sq\nWest\nNaN\n10.0\nThe Waterfront\nRing\nExisting\n0\nNaN\nNaN\nNaN\nNaN\n{'type': 'MultiPoint', 'coordinates': [[-79.37...\n\n\n13971\n4494614\n31121\nBP-22492\n200\nElizabeth St\nElizabeth St\nWest\nLa Plante Ave\nWest\nNaN\n11.0\nNaN\nRing\nExisting\n0\nNaN\nNaN\nNaN\nNaN\n{'type': 'MultiPoint', 'coordinates': [[-79.38...\n\n\n5139\n4485782\n17465\nBP-40070\n70\nPeter St\nKing St W\nNorth\nPeter St\nWest\nNaN\n10.0\nToronto Downtown West\nRing\nExisting\n0\nNaN\nNaN\nNaN\nNaN\n{'type': 'MultiPoint', 'coordinates': [[-79.39...\n\n\n7635\n4488278\n20375\nBP-27153\n39\nPrince Arthur Ave\nPrince Arthur Ave\nSouth\nBedford Rd\nEast\nNaN\n11.0\nNaN\nRing\nExisting\n0\nNaN\nNaN\nNaN\nNaN\n{'type': 'MultiPoint', 'coordinates': [[-79.39...\n\n\n\n\n\n\n\nThe geometry column holds the longitude and latitude information, but before we dive in, we need to extract the valuable details. No worries – we’ll make use of pandas’ str.extract for this task.\n\n\nCode\npattern = r\"(-?\\d+\\.\\d+),\\s*(-?\\d+\\.\\d+)\"\n\ntoronto_df_processed = toronto_df.assign(\n    LONGITUDE=lambda df: df.geometry.str.extract(pattern)[0],\n    LATITUDE=lambda df: df.geometry.str.extract(pattern)[1],\n).loc[:, [\"ASSETTYPE\", \"STATUS\", \"LONGITUDE\", \"LATITUDE\"]]\ntoronto_df_processed.head()\n\n\n\n\n\n\n\n\n\nASSETTYPE\nSTATUS\nLONGITUDE\nLATITUDE\n\n\n\n\n784\nRing\nTemporarily Removed\n-79.38378423783222\n43.6660359833018\n\n\n3297\nRing\nExisting\n-79.3774934493851\n43.6407633657936\n\n\n13971\nRing\nExisting\n-79.386799735149\n43.6589303889453\n\n\n5139\nRing\nExisting\n-79.3926661761316\n43.6460273003346\n\n\n7635\nRing\nExisting\n-79.3973838724551\n43.6693038734947\n\n\n\n\n\n\n\n\nCreating the map and displaying it\nHere’s an example of how to create a map without any overlaid data points.\n\n\nCode\ntoronto_map = folium.Map(\n    location=[43.651070, -79.347015], zoom_start=11, tiles=\"OpenStreetMap\"\n)\ntoronto_map\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\nFigure 1: The City of Toronto\n\n\n\n\n\nSuperimposing bike locations on the map with FeatureGroup\nAfter instantiating FeatureGroup, we can easily add the bike locations using the add_child method. It is really easy!\n\n\nCode\n# let's start with a clean copy of the map of Toronto\ntoronto_map = folium.Map(\n    location=[43.651070, -79.347015], zoom_start=11, tiles=\"OpenStreetMap\"\n)\n\n# instantiate a feature group \nbike_stations = folium.map.FeatureGroup()\n\n# loop through the bike stations\nfor lat, long in zip(toronto_df_processed.LATITUDE, toronto_df_processed.LONGITUDE):\n    bike_stations.add_child(\n        folium.features.CircleMarker(\n            [lat, long],\n            radius=5,\n            color=\"red\",\n            fill=True,\n            fill_color=\"yellow\",\n            fill_opacity=1,\n        )\n    )\n# add bike stations to the map\ntoronto_map.add_child(bike_stations)\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\nFigure 2: The City of Toronto with available bike locations\n\n\n\n\n\nAdding pop-up text with relevant information\nWe can also enhance this by adding a pop-up box that displays custom text of our choice.\n\n\nCode\n# let's start with a clean copy of the map of Toronto\ntoronto_map = folium.Map(\n    location=[43.651070, -79.347015], zoom_start=11, tiles=\"OpenStreetMap\"\n)\n\n# instantiate a feature group \nbike_stations = folium.map.FeatureGroup()\n\n# loop through the bike stations\nfor lat, long in zip(toronto_df_processed.LATITUDE, toronto_df_processed.LONGITUDE):\n    bike_stations.add_child(\n        folium.features.CircleMarker(\n            [lat, long],\n            radius=5,\n            color=\"grey\",\n            fill=True,\n            fill_color=\"white\",\n            fill_opacity=1,\n        )\n    )\n\n# add pop-up text to each marker on the map\nlatitudes = list(toronto_df_processed.LATITUDE)\nlongitudes = list(toronto_df_processed.LONGITUDE)\nlabels = list(toronto_df_processed.STATUS)\n\nfor lat, lng, label in zip(latitudes, longitudes, labels):\n    folium.Marker([lat, lng], popup=label).add_to(toronto_map)\n\n# add bike stations to map\ntoronto_map.add_child(bike_stations)\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\nFigure 3: The City of Toronto with available bike locations\n\n\n\n\n\nClustering the rental locations with MarkerCluster\nAnd the best part, which happens to be my favorite, is that we can also integrate a MarkerCluster. This comes in handy when we’re dealing with numerous data points clustered closely together on the map. With a MarkerCluster, you get to see their combined values instead of each one individually. It’s a fantastic feature!\n\n\nCode\n# let's start with a clean copy of the map of Toronto\ntoronto_map = folium.Map(\n    location=[43.651070, -79.347015], zoom_start=11, tiles=\"OpenStreetMap\"\n)\n\n# instantiate a mark cluster object \nbike_stations_cluster = plugins.MarkerCluster().add_to(toronto_map)\n\n# loop through the dataframe and add each data point to the mark cluster\nfor lat, lng, label, in zip(\n    toronto_df_processed.LATITUDE,\n    toronto_df_processed.LONGITUDE,\n    toronto_df_processed.STATUS,\n):\n    folium.Marker(\n        location=[lat, lng],\n        icon=None,\n        popup=label,\n    ).add_to(bike_stations_cluster)\n\n# display map\ntoronto_map\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\nFigure 4: Aggregated Bike Locations in the City of Toronto\n\n\n\nThat’s a wrap! I hope these examples have been helpful. Feel free to use these techniques in your next data science or geospatial project. Until next time, happy exploring!"
  },
  {
    "objectID": "posts/Kaggle/Kaggle.html",
    "href": "posts/Kaggle/Kaggle.html",
    "title": "🏆🥉3rd place at Kaggle’s Classification with a Tabular Vector Borne Disease Dataset",
    "section": "",
    "text": "Greetings everyone! I am thrilled to announce that I have achieved a significant milestone in my data science journey. I am delighted to share that I secured the 3rd position in Kaggle’s “Classification with a Tabular Vector Borne Disease Dataset” competition, surpassing 931 talented contenders! If you’re interested in learning more about my approach to the competition, I invite you to check out my detailed solution on the Kaggle discussion page.\n\n\n\n\n\nIn this exciting competition, the aim was to predict the three most probable diseases out of a total of 11 vector-borne diseases, including well-known names such as Zika, Yellow Fever, Malaria, and more. Our task involved analyzing the provided symptoms and using the MPA@3 evaluation metric for accurate predictions.\nMy journey in the world of Kaggle has been a relatively short one, with this competition marking my third participation. To give you some context, let me share a bit about my previous placements:\n\nReaching this point has been a long and rewarding endeavor. Over the past two years, I have been studying this topic in my free time while also pursuing a full-time Ph.D. research. It has been an exciting balancing act!\nParticipating in Kaggle competitions has been an invaluable learning experience for me. It has allowed me to apply my knowledge, explore new techniques, and connect with fellow data enthusiasts. I am deeply grateful to the organizers of Kaggle for providing a platform that fosters growth and healthy competition.\nWhile this achievement brings a sense of accomplishment, I am already diving into new competitions and expanding my skills. Currently, I am particularly interested in reproducible data and developing robust architectures for machine learning projects to enhance code reproducibility. In fact, I am considering writing a blog on this often overlooked topic.\nStay tuned for more :)\n#DataScience #KaggleCompetition #VectorBorneDiseases #MachineLearning #ReproducibleData"
  },
  {
    "objectID": "posts/Lets-Plot/2023_08_05_How_to_use_the_Lets_Plot_library_by_JetBrains.html",
    "href": "posts/Lets-Plot/2023_08_05_How_to_use_the_Lets_Plot_library_by_JetBrains.html",
    "title": "How to use the Lets-Plot library by JetBrains",
    "section": "",
    "text": "When I embarked on my data science journey, due to my academics background I quickly gravitated towards the R programming language. Like many R novices, I began with Hadley Wickham’s R for Data Science book which introduced me to the wonderful ggplot2 library. As my interest in machine learning grew, I made the switch to Python. Nowadays, for most of my data plotting needs, I rely mainly on matplotlib or seaborn. Though I love these libraries, their multiple ways of accomplishing the same tasks can be a bit cumbersome and challenging to learn at first.\nThat’s why in this article, I’m excited to introduce you to the Lets-Plot library by JetBrains. It is the closest you can get to ggplot’s syntax while using Python. While some traditional Python users might find the syntax a bit unfamiliar initially, I’m here to make a case for this fantastic library and hopefully inspire you to incorporate it into your day-to-day data science activities.\nTo showcase (some of) the key features of Lets-Plot, we will be utilizing the penguins dataset 🐧 from Github.\nWithout further ado, let’s dive right in and discover the power and beauty of Lets-Plot! 🐍📊\n\n\nCode\n# Installation\n# pip install lets-plot \n\n\n\n\nCode\n# Import libraries\nimport numpy as np\nimport pandas as pd\nfrom lets_plot import *\nfrom lets_plot.mapping import as_discrete\nLetsPlot.setup_html()\n\n\n\n            \n            \n            \n\n\n\n\nCode\naddress='https://gist.githubusercontent.com/slopp/ce3b90b9168f2f921784de84fa445651/raw/4ecf3041f0ed4913e7c230758733948bc561f434/penguins.csv'\ndf=pd.read_csv(address)\ndf.head()\n\n\n\n\n\n\n\n\n\nrowid\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\n1\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\n2\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\n3\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\n4\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\n5\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n\n\n\n\n\n\nSyntax Similarities: Lets-Plot and ggplot2\nFor our first exercise, I thought it would be beneficial to replicate a basic plot inspired by Hadley’s book. When comparing my code here with the one presented by him, you’ll notice that there is very little difference between the two. The syntax is nearly identical, making it a smooth transition from ggplot to Lets-Plot.\nNow, let’s take a closer look at the code. In the ggplot function, we define the DataFrame we’ll be working with, and the aesthetic mappings are set at the global level. We assign the values for the x and y axes, as well as the color argument, which groups the data based on the categorical variable representing the three different penguin species: Adelie, Gentoo, and Chinstrap. This color parameter is quite similar to seaborn’s hue, making it easy for those familiar with seaborn to adapt to Lets-Plot seamlessly.\nAfter the ggplot() function sets the global aesthetic mappings, the geom_point() function comes into play and draws the points defined by the x and y parameters, effectively creating a scatter plot. These points represent the data points from the penguins dataset, with x and y coordinates corresponding to the specified variables.\nAdditionally, we enhance the plot by using geom_smooth(method=‘lm’), which adds a smoothed conditional mean. The lm method stands for ‘linear model,’ indicating that the smoothing is based on a linear regression. This smoothed line helps reveal trends and patterns in the data, making it easier to observe any overall relationships between the variables.\nLet’s continue exploring more of what Lets-Plot has in store for us! 📊🐧🌈\n\n\nCode\n(ggplot(df,\n        aes(x='flipper_length_mm',\n            y = 'body_mass_g',\n            color='species'\n           )\n       )\n + geom_point() # Draw points defined by an x and y coordinate, as for a scatter plot.\n + geom_smooth(method='lm') # Add a smoothed conditional mean. ‘lm’ stands for 'linear model' as Smoothing method\n) \n\n\n   \n   \n\n\nIn the previous example, we highlighted the importance of placing the color parameter at the global level, which grouped the data by the three penguin species and showed separate regression lines for each group. However, if we prefer to depict the regression line for the entire dataset, regardless of the group association, we can do so just as easily. All we need to do is remove the color parameter from the aesthetics of the ggplot function and place it solely in the geom_point.\nAdditionally, to enhance the plot further, we can properly label the x and y axes, add a title and subtitle. With these simple adjustments, we can achieve the same output as Hadley’s original code, with little to no modification.\n\n\nCode\n(ggplot(df, \n        aes(x='flipper_length_mm',\n            y = 'body_mass_g',\n           )\n       )\n + geom_point(aes(color='species', shape='species'))\n + geom_smooth(method='lm')\n + labs(\n    title = \"Body mass and flipper length\",\n    subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n    x = \"Flipper length (mm)\", y = \"Body mass (g)\",\n  ) \n + scale_color_viridis() # lets-plots equivalent of the scale_color_colorblind()\n) \n\n\n   \n   \n\n\n\n\nVisualizing Data Based on Categorical Variables\nLets-Plot provides numerous options to showcase our data using categorical variables. From bar plots, box plots, and violin plots to pie charts, the possibilities are diverse. You can check out their API for reference. Let’s explore some examples to demonstrate the versatility of Lets-Plot in visualizing categorical data.\n\n\nCode\npenguin_bar = (ggplot(df,aes(x='species'))\n               + geom_bar()\n              )\n\npenguin_box = (ggplot(df,aes(x = 'species', y = 'body_mass_g'))\n               + geom_boxplot()\n              )\n\npenguin_density = (ggplot(df,aes('body_mass_g', color='species', fill='species'))\n                   + geom_density(alpha=0.5)\n                  )\n\npenguin_rel_frequency = (ggplot(df,aes(x = 'island', fill = 'species'))\n                         + geom_bar(position='fill')\n                        )\ngggrid([penguin_bar, penguin_box, penguin_density, penguin_rel_frequency], ncol=2)\n\n\n   \n   \n\n\n\n\nIncorporate Multiple Variables with facet_wrap\nSo far we’ve discovered how easy it is to plot data based on a single categorical variable. However, what if we want to depict relationships involving two or more categorical variables? That’s where facet_wrap comes in handy. This versatile function bears resemblance to similar functions found in seaborn or ggplot2 libraries.\nTo unlock the potential of facet_wrap, we simply need to define aesthetics, which can either be global or local to the mapping function. Then, we can use facet_wrap with the relevant categorical variable we want to visualize. It’s as simple as that!\n\n\nCode\n(ggplot(df, aes(x = 'flipper_length_mm', y = 'body_mass_g'))  \n + geom_point(aes(color = 'species', shape = 'species'), size = 4) \n + facet_wrap('island', nrow=1)\n + labs(title = \"Body mass and flipper length based on island\",\n        subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n        x = \"Flipper length (mm)\", y = \"Body mass (g)\",\n       )\n + theme(plot_title=element_text(size=20,face='bold'))\n + ggsize(1000,500)\n)\n\n\n   \n   \n\n\n\n\n\n\nReordering Categorical Variables Based On Statistics\nWhen visualizing data, a task I frequently encounter is ordering categorical variables in either ascending or descending order, based on statistics like median or mean. In my previous point on “Visualizing Data Based on Categorical Variables,” you noticed that the boxplot displayed categories in an unordered manner. However, consider how we can present them in an ascending order, determined by the median. This not only enhances the aesthetics of the plot but also provides valuable insights into the relationships among the categories.\n\n\nCode\n(ggplot(df,aes(as_discrete('species', order=1, order_by='..middle..'), \n               y = 'body_mass_g'))\n + geom_boxplot()\n)\n\n\n   \n   \n\n\nBy incorporating the as_discrete function, specifying the column, the ordering direction (1 for ascending, -1 for descending), and setting the order_by variable to middle (representing the median), the plot has become significantly more informative. This simple addition has allowed us to organize the categorical variables in a meaningful manner, improving the visualization’s clarity and aiding in the interpretation of relationships among the categories.\n\n\nChaining Pandas Code with Lets Plot Visualization\nOne of the best features of the pandas library is its remarkable customizability. With the help of the pd.pipe function, we can seamlessly integrate any of our own functions into method chains, as long as they return a DataFrame or Series. This opens up exciting possibilities to fully incorporate Lets-Plot into our code, just like pandas’ own built-in plotting functionality.\nWhile Lets-Plot may be slightly more verbose than pandas plotting, it offers significantly more flexibility and freedom for customization. Not to mention that some may consider it visually more appealing. With Lets-Plot integrated into our pandas code, we can effortlessly create stunning and informative plots, making data analysis an even more enjoyable experience.\n\n\nCode\n(df\n .groupby('species')\n [['body_mass_g', 'flipper_length_mm']]\n .mean()\n .reset_index()\n .pipe(lambda df: (ggplot(df)\n                   + geom_pie(aes(slice='body_mass_g', fill='species'), \n                              stat='identity',size=30, hole=0.2, stroke=1.0,\n                              labels=layer_labels().line('@body_mass_g').format('@body_mass_g', '{.0f}').size(20)\n                             )\n                   + labs(title = \"Body mass based on species\",\n                          subtitle = \"Representing how Lets-Plot can be used with pd. pipe\",\n                          x = \"\", y = \"\",\n                         )\n                   + theme(axis='blank',\n                          plot_title=element_text(size=20,face='bold'))\n                   + ggsize(500,400)\n                  )\n )\n)\n\n\n   \n   \n\n\nThat’s a wrap on the Lets-Plot library! There’s so much more to explore and learn about this powerful tool. I hope you found this introduction helpful and consider integrating Lets-Plot into your daily data analysis routine.\nHappy coding 🐍🖥️🔍🚀"
  },
  {
    "objectID": "posts/polars/Polars_revised.html",
    "href": "posts/polars/Polars_revised.html",
    "title": "Getting Started with Polars",
    "section": "",
    "text": "In the ever-evolving field of data science, effective data manipulation tools are essential. Enter Polars, a Rust-based library garnering attention within the data community. Boasting impressive speed and versatile capabilities, Polars is redefining our data management practices. In this blog, we delve into Polars’ core functions and practical applications, shedding light on how it empowers data professionals to efficiently tackle complex tasks.\nFor those well-versed in Pandas, Polars offers a blend of familiarity and innovation. Although this document is not designed to substitute the official documentation, it serves as a tool to provide you with insights into the capabilities that polars offers. Our goal is to ensure the continuous updating of this document.\nOur exploration of Polars is guided by insights from the Polars User Guide, Kevin Heavey’s perspectives in Modern Polars and Matt Harrison’s engaging tutorial on Polars at PyCon. We kickstart our exploration with Matt Harrison’s shared dataset, the US Department of Energy’s Automobile Fuel Economy data. Let’s begin this journey!"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#use-take-for-series-or-expressions",
    "href": "posts/polars/Polars_revised.html#use-take-for-series-or-expressions",
    "title": "Getting Started with Polars",
    "section": "Use take for Series or Expressions",
    "text": "Use take for Series or Expressions\nAn interesting approach is through Polars’ take method. This method can be used with Expr, Series, or np.ndarray. However, it’s worth noting that if you intend to utilize it with DataFrames, a preliminary conversion to Series is necessary.\n\n\nCode\ndf.select(pl.col(\"make\")).to_series().take(list(range(0, 5)))\n\n\n\n\nshape: (5,)\n\n\n\nmake\n\n\ncat\n\n\n\n\n\"Alfa Romeo\"\n\n\n…\n\n\n\"Subaru\""
  },
  {
    "objectID": "posts/polars/Polars_revised.html#using-filter",
    "href": "posts/polars/Polars_revised.html#using-filter",
    "title": "Getting Started with Polars",
    "section": "Using filter",
    "text": "Using filter\nSimilar to Pandas, Polars features a filter method that assists in the process of selectively filtering rows based on one or multiple conditions.\n\nEvaluate a single condition\n\n\nCode\ndf.filter(pl.col(\"model\") == \"Testarossa\")\n\n\n\nshape: (14, 11)\n\n\n\nmake\nmodel\ndrive\ncylinders\ndispl\nfuelType\nmpgData\ncity08\nhighway08\ncomb08\nyear\n\n\ncat\ncat\ncat\ni8\nf32\ncat\nbool\ni8\ni8\ni8\ndate\n\n\n\n\n\"Ferrari\"\n\"Testarossa\"\n\"Rear-Wheel Dri…\n12\n4.9\n\"Regular\"\nfalse\n9\n14\n11\n1985-01-01\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Ferrari\"\n\"Testarossa\"\n\"Rear-Wheel Dri…\n12\n4.9\n\"Premium\"\nfalse\n10\n15\n11\n1992-01-01\n\n\n\n\n\n\n\n\nCombine multiple conditions\n\n\nCode\ndf.filter((pl.col(\"model\") == \"Testarossa\") & (pl.col(\"make\") == \"Ferrari\"))\n\n\n\nshape: (9, 11)\n\n\n\nmake\nmodel\ndrive\ncylinders\ndispl\nfuelType\nmpgData\ncity08\nhighway08\ncomb08\nyear\n\n\ncat\ncat\ncat\ni8\nf32\ncat\nbool\ni8\ni8\ni8\ndate\n\n\n\n\n\"Ferrari\"\n\"Testarossa\"\n\"Rear-Wheel Dri…\n12\n4.9\n\"Regular\"\nfalse\n9\n14\n11\n1985-01-01\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Ferrari\"\n\"Testarossa\"\n\"Rear-Wheel Dri…\n12\n4.9\n\"Premium\"\nfalse\n10\n15\n11\n1992-01-01"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#imitate-pandas-index-using-with_row_count",
    "href": "posts/polars/Polars_revised.html#imitate-pandas-index-using-with_row_count",
    "title": "Getting Started with Polars",
    "section": "Imitate pandas’ index using with_row_count",
    "text": "Imitate pandas’ index using with_row_count\nEven in the absence of a traditional index, you can make use of the with_row_count method in Polars. This clever method comes to the rescue for tasks like indexing and filtering, providing an alternative approach.\n\n\nCode\n(df.with_row_count().filter(pl.col(\"row_nr\") == 5))\n\n\n\nshape: (1, 12)\n\n\n\nrow_nr\nmake\nmodel\ndrive\ncylinders\ndispl\nfuelType\nmpgData\ncity08\nhighway08\ncomb08\nyear\n\n\nu32\ncat\ncat\ncat\ni8\nf32\ncat\nbool\ni8\ni8\ni8\ndate\n\n\n\n\n5\n\"Subaru\"\n\"Loyale\"\n\"Front-Wheel Dr…\n4\n1.8\n\"Regular\"\nfalse\n21\n24\n22\n1993-01-01\n\n\n\n\n\n\n\nUse is_in\nThe is_in function can be employed to verify whether elements of a given expression are present within another Series. We can use this to filter our rows.\n\n\nCode\ndf.filter(pl.col(\"cylinders\").is_in([i for i in range(6, 8)]))\n\n\n\nshape: (14_284, 11)\n\n\n\nmake\nmodel\ndrive\ncylinders\ndispl\nfuelType\nmpgData\ncity08\nhighway08\ncomb08\nyear\n\n\ncat\ncat\ncat\ni8\nf32\ncat\nbool\ni8\ni8\ni8\ndate\n\n\n\n\n\"Audi\"\n\"100\"\n\"Front-Wheel Dr…\n6\n2.8\n\"Premium\"\ntrue\n17\n22\n19\n1993-01-01\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Pontiac\"\n\"Grand Am\"\n\"Front-Wheel Dr…\n6\n3.3\n\"Regular\"\nfalse\n18\n26\n21\n1993-01-01"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#difference-between-select-and-with_columns",
    "href": "posts/polars/Polars_revised.html#difference-between-select-and-with_columns",
    "title": "Getting Started with Polars",
    "section": "Difference between select and with_columns`",
    "text": "Difference between select and with_columns`\nAs evident from the examples below, both select and with_column can be utilized for both column selection and column assignment. However, there’s a crucial distinction between the two. After performing column operations, the select method drops the unselected columns and retains only the columns that underwent operations. Conversely, the with_column method appends the new columns to the dataframe, preserving the original ones.\n\n\nCode\n# everything else is dropped\n\n(\n    df.select(\n        (pl.col(\"city08\") + 10).alias(\"added\"),\n        (pl.col(\"city08\") - 10).alias(\"substracted\"),\n    )\n)\n\n\n\n\nshape: (41_144, 2)\n\n\n\nadded\nsubstracted\n\n\ni8\ni8\n\n\n\n\n29\n9\n\n\n…\n…\n\n\n26\n6\n\n\n\n\n\n\n\nCode\n# columns are kept\n\n(\n    df.with_columns(\n        (pl.col(\"city08\") + 10).alias(\"added\"),\n        (pl.col(\"city08\") - 10).alias(\"substracted\"),\n    )\n)\n\n\n\nshape: (41_144, 13)\n\n\n\nmake\nmodel\ndrive\ncylinders\ndispl\nfuelType\nmpgData\ncity08\nhighway08\ncomb08\nyear\nadded\nsubstracted\n\n\ncat\ncat\ncat\ni8\nf32\ncat\nbool\ni8\ni8\ni8\ndate\ni8\ni8\n\n\n\n\n\"Alfa Romeo\"\n\"Spider Veloce …\n\"Rear-Wheel Dri…\n4\n2.0\n\"Regular\"\ntrue\n19\n25\n21\n1985-01-01\n29\n9\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Subaru\"\n\"Legacy AWD Tur…\n\"4-Wheel or All…\n4\n2.2\n\"Premium\"\nfalse\n16\n21\n18\n1993-01-01\n26\n6\n\n\n\n\n\n\nNow that we’ve clarified this point, let’s proceed to explore the fundamental methods for selecting columns."
  },
  {
    "objectID": "posts/polars/Polars_revised.html#select-all-columns",
    "href": "posts/polars/Polars_revised.html#select-all-columns",
    "title": "Getting Started with Polars",
    "section": "Select all columns",
    "text": "Select all columns\nA particularly handy tool is pl.all(), which provides the current state of the dataframe—similar to pd.assign(foo=lambda df) in Pandas. This can prove useful, particularly when dealing with operations like groupby and aggregation.\n\n\nCode\n# this is analogous to df.select(pl.col(\"*\")), where * represents the wildcard component\ndf.select(pl.all())\n\n\n\nshape: (41_144, 11)\n\n\n\nmake\nmodel\ndrive\ncylinders\ndispl\nfuelType\nmpgData\ncity08\nhighway08\ncomb08\nyear\n\n\ncat\ncat\ncat\ni8\nf32\ncat\nbool\ni8\ni8\ni8\ndate\n\n\n\n\n\"Alfa Romeo\"\n\"Spider Veloce …\n\"Rear-Wheel Dri…\n4\n2.0\n\"Regular\"\ntrue\n19\n25\n21\n1985-01-01\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Subaru\"\n\"Legacy AWD Tur…\n\"4-Wheel or All…\n4\n2.2\n\"Premium\"\nfalse\n16\n21\n18\n1993-01-01"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#select-all-columns-except-for",
    "href": "posts/polars/Polars_revised.html#select-all-columns-except-for",
    "title": "Getting Started with Polars",
    "section": "Select all columns except for…",
    "text": "Select all columns except for…\n\n\nCode\ndf.select(pl.col(\"*\").exclude(\"year\", \"comb08\", \"highway08\"))\n\n\n\nshape: (41_144, 8)\n\n\n\nmake\nmodel\ndrive\ncylinders\ndispl\nfuelType\nmpgData\ncity08\n\n\ncat\ncat\ncat\ni8\nf32\ncat\nbool\ni8\n\n\n\n\n\"Alfa Romeo\"\n\"Spider Veloce …\n\"Rear-Wheel Dri…\n4\n2.0\n\"Regular\"\ntrue\n19\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Subaru\"\n\"Legacy AWD Tur…\n\"4-Wheel or All…\n4\n2.2\n\"Premium\"\nfalse\n16"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#select-a-subset-of-columns",
    "href": "posts/polars/Polars_revised.html#select-a-subset-of-columns",
    "title": "Getting Started with Polars",
    "section": "Select a subset of columns",
    "text": "Select a subset of columns\n\n\nCode\ndf.select(pl.col(\"year\", \"comb08\", \"highway08\"))\n\n\n\n\nshape: (41_144, 3)\n\n\n\nyear\ncomb08\nhighway08\n\n\ndate\ni8\ni8\n\n\n\n\n1985-01-01\n21\n25\n\n\n…\n…\n…\n\n\n1993-01-01\n18\n21"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#select-columns-based-on-regular-expression",
    "href": "posts/polars/Polars_revised.html#select-columns-based-on-regular-expression",
    "title": "Getting Started with Polars",
    "section": "Select columns based on regular expression",
    "text": "Select columns based on regular expression\n\n\nCode\ndf.select(pl.col(\"^.*(mo|ma).*$\"))\n\n\n\n\nshape: (41_144, 2)\n\n\n\nmake\nmodel\n\n\ncat\ncat\n\n\n\n\n\"Alfa Romeo\"\n\"Spider Veloce …\n\n\n…\n…\n\n\n\"Subaru\"\n\"Legacy AWD Tur…"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#select-columns-based-on-dtypes",
    "href": "posts/polars/Polars_revised.html#select-columns-based-on-dtypes",
    "title": "Getting Started with Polars",
    "section": "Select columns based on dtypes",
    "text": "Select columns based on dtypes\nThis may remind you of pandas’ select_dtypes method.\n\n\nCode\ndf.select(pl.col(pl.Int8, pl.Boolean))\n\n\n\nshape: (41_144, 5)\n\n\n\ncylinders\nmpgData\ncity08\nhighway08\ncomb08\n\n\ni8\nbool\ni8\ni8\ni8\n\n\n\n\n4\ntrue\n19\n25\n21\n\n\n…\n…\n…\n…\n…\n\n\n4\nfalse\n16\n21\n18"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#using-selectors",
    "href": "posts/polars/Polars_revised.html#using-selectors",
    "title": "Getting Started with Polars",
    "section": "Using selectors",
    "text": "Using selectors\nSelectors offer a more intuitive approach to selecting columns from DataFrame or LazyFrame objects, taking into account factors like column names, data types, and other properties. They consolidate and enhance the functionality that’s accessible through the col() expression. More on them here.\n\nBy dtypes\n\n\nCode\ndf.select(cs.integer(), cs.float(), cs.temporal())\n\n\n\nshape: (41_144, 6)\n\n\n\ncylinders\ncity08\nhighway08\ncomb08\ndispl\nyear\n\n\ni8\ni8\ni8\ni8\nf32\ndate\n\n\n\n\n4\n19\n25\n21\n2.0\n1985-01-01\n\n\n…\n…\n…\n…\n…\n…\n\n\n4\n16\n21\n18\n2.2\n1993-01-01\n\n\n\n\n\n\n\n\nCode\n# all columns except for the ones containing float\ndf.select(cs.all() - cs.numeric())\n\n\n\nshape: (41_144, 6)\n\n\n\nmake\nmodel\ndrive\nfuelType\nmpgData\nyear\n\n\ncat\ncat\ncat\ncat\nbool\ndate\n\n\n\n\n\"Alfa Romeo\"\n\"Spider Veloce …\n\"Rear-Wheel Dri…\n\"Regular\"\ntrue\n1985-01-01\n\n\n…\n…\n…\n…\n…\n…\n\n\n\"Subaru\"\n\"Legacy AWD Tur…\n\"4-Wheel or All…\n\"Premium\"\nfalse\n1993-01-01\n\n\n\n\n\n\n\n\nCode\n# same as the one above but it uses the tilde\ndf.select(~cs.numeric())\n\n\n\nshape: (41_144, 6)\n\n\n\nmake\nmodel\ndrive\nfuelType\nmpgData\nyear\n\n\ncat\ncat\ncat\ncat\nbool\ndate\n\n\n\n\n\"Alfa Romeo\"\n\"Spider Veloce …\n\"Rear-Wheel Dri…\n\"Regular\"\ntrue\n1985-01-01\n\n\n…\n…\n…\n…\n…\n…\n\n\n\"Subaru\"\n\"Legacy AWD Tur…\n\"4-Wheel or All…\n\"Premium\"\nfalse\n1993-01-01\n\n\n\n\n\n\n\n\nBy column names\n\n\nCode\ndf.select(cs.contains(\"08\"))\n\n\n\n\nshape: (41_144, 3)\n\n\n\ncity08\nhighway08\ncomb08\n\n\ni8\ni8\ni8\n\n\n\n\n19\n25\n21\n\n\n…\n…\n…\n\n\n16\n21\n18\n\n\n\n\n\n\n\nCode\ndf.select(cs.starts_with(\"d\"))\n\n\n\n\nshape: (41_144, 2)\n\n\n\ndrive\ndispl\n\n\ncat\nf32\n\n\n\n\n\"Rear-Wheel Dri…\n2.0\n\n\n…\n…\n\n\n\"4-Wheel or All…\n2.2\n\n\n\n\n\nThe possibilities here are incredibly vast! I’m pretty sure you’ll find a function that suits your needs just right."
  },
  {
    "objectID": "posts/polars/Polars_revised.html#basic-operations",
    "href": "posts/polars/Polars_revised.html#basic-operations",
    "title": "Getting Started with Polars",
    "section": "Basic operations",
    "text": "Basic operations\n\n\nCode\n(\n    df.select(\n        pl.col(\"make\").cast(pl.Utf8).str.contains(\"Hyundai|Kia\").alias(\"contains\"),\n        pl.col(\"make\").cast(pl.Utf8).str.starts_with(\"Alfa\").alias(\"starts_with\"),\n        pl.col(\"make\").cast(pl.Utf8).str.ends_with(\"ari\").alias(\"ends_with\"),\n    )\n)\n\n\n\nshape: (41_144, 3)\n\n\n\ncontains\nstarts_with\nends_with\n\n\nbool\nbool\nbool\n\n\n\n\nfalse\ntrue\nfalse\n\n\n…\n…\n…\n\n\nfalse\nfalse\nfalse\n\n\n\n\n\n\nWe can extract the numbers from the different car models:\n\n\nCode\n(\n    df.select(\n        pl.col(\"model\")\n        .cast(pl.Utf8)\n        .str.extract(r\"(\\d+)\", group_index=1)\n        .cast(pl.Int32)\n        .alias(\"extracted_number\"),\n    )\n)\n\n\n\n\nshape: (41_144, 1)\n\n\n\nextracted_number\n\n\ni32\n\n\n\n\n2000\n\n\n…\n\n\nnull\n\n\n\n\n\nAs per usual, we can replace values in a given column:\n\n\nCode\n(\n    df.select(\n        pl.col(\"make\").cast(pl.Utf8).str.replace(\"r\", 0).alias(\"replaced\"),\n    )\n)\n\n\n\n\nshape: (41_144, 1)\n\n\n\nreplaced\n\n\nstr\n\n\n\n\n\"Alfa Romeo\"\n\n\n…\n\n\n\"Suba0u\""
  },
  {
    "objectID": "posts/polars/Polars_revised.html#changing-column-names",
    "href": "posts/polars/Polars_revised.html#changing-column-names",
    "title": "Getting Started with Polars",
    "section": "Changing column names",
    "text": "Changing column names\nAltering column names is quite reminiscent of the process in Pandas:\n\n\nCode\ndf.rename({\"make\": \"car maker\", \"model\": \"car model\"})\n\n\n\nshape: (41_144, 11)\n\n\n\ncar maker\ncar model\ndrive\ncylinders\ndispl\nfuelType\nmpgData\ncity08\nhighway08\ncomb08\nyear\n\n\ncat\ncat\ncat\ni8\nf32\ncat\nbool\ni8\ni8\ni8\ndate\n\n\n\n\n\"Alfa Romeo\"\n\"Spider Veloce …\n\"Rear-Wheel Dri…\n4\n2.0\n\"Regular\"\ntrue\n19\n25\n21\n1985-01-01\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Subaru\"\n\"Legacy AWD Tur…\n\"4-Wheel or All…\n4\n2.2\n\"Premium\"\nfalse\n16\n21\n18\n1993-01-01\n\n\n\n\n\n\nIn case you would like to alter multiple column names all at once:\n\n\nCode\ndf.select(pl.all().map_alias(lambda name: name.upper().replace(\"I\", \"0000\")))\n\n\n\nshape: (41_144, 11)\n\n\n\nMAKE\nMODEL\nDR0000VE\nCYL0000NDERS\nD0000SPL\nFUELTYPE\nMPGDATA\nC0000TY08\nH0000GHWAY08\nCOMB08\nYEAR\n\n\ncat\ncat\ncat\ni8\nf32\ncat\nbool\ni8\ni8\ni8\ndate\n\n\n\n\n\"Alfa Romeo\"\n\"Spider Veloce …\n\"Rear-Wheel Dri…\n4\n2.0\n\"Regular\"\ntrue\n19\n25\n21\n1985-01-01\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Subaru\"\n\"Legacy AWD Tur…\n\"4-Wheel or All…\n4\n2.2\n\"Premium\"\nfalse\n16\n21\n18\n1993-01-01\n\n\n\n\n\n\n\nstr.split with expand='all'\nI often find myself using the str.split method, and it caught me off guard that there isn’t a direct equivalent in Polars. Fingers crossed, we might come across an implementation like expand=True from Pandas, which would be a real game-changer here too!\n\n\nCode\n(\n    df.select(\n        pl.col(\"drive\")\n        .cast(pl.Utf8)\n        .str.split_exact(\"-\", n=1)\n        .struct.rename_fields([\"first_part\", \"second_part\"])\n        .alias(\"fields\"),\n    ).unnest(\"fields\")\n)\n\n\n\n\nshape: (41_144, 2)\n\n\n\nfirst_part\nsecond_part\n\n\nstr\nstr\n\n\n\n\n\"Rear\"\n\"Wheel Drive\"\n\n\n…\n…\n\n\n\"4\"\n\"Wheel or All\""
  },
  {
    "objectID": "posts/polars/Polars_revised.html#a-simple-split-apply-combine-operation",
    "href": "posts/polars/Polars_revised.html#a-simple-split-apply-combine-operation",
    "title": "Getting Started with Polars",
    "section": "A simple split-apply-combine operation",
    "text": "A simple split-apply-combine operation\n\n\nCode\n(df.groupby(\"make\").count().sort(by=\"count\", descending=True).limit(5))\n\n\n\n\nshape: (5, 2)\n\n\n\nmake\ncount\n\n\ncat\nu32\n\n\n\n\n\"Chevrolet\"\n4003\n\n\n…\n…\n\n\n\"Toyota\"\n2071"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#using-agg",
    "href": "posts/polars/Polars_revised.html#using-agg",
    "title": "Getting Started with Polars",
    "section": "Using agg",
    "text": "Using agg\nYou can use agg to calculate statistics over either a single or multiple columns all at once.\n\n\nCode\n(df.groupby(\"make\").agg(pl.count(), pl.col(\"model\")))\n\n\n\nshape: (136, 3)\n\n\n\nmake\ncount\nmodel\n\n\ncat\nu32\nlist[cat]\n\n\n\n\n\"TVR Engineerin…\n4\n[\"TVR 280i/350i Convertible\", \"TVR 280i/350i Coupe\", … \"TVR 280i Coupe\"]\n\n\n…\n…\n…\n\n\n\"Grumman Allied…\n1\n[\"LLV\"]\n\n\n\n\n\n\nHere’s an illustration of how to utilize the is_not_nan method. In Polars, unknown values are represented with floating-point precision, similar to how pandas handles them. It’s important not to mix this up with missing values, which are indicated by Null in Polars. Additionally, take note of the use of limit instead of head, a concept quite akin to SQL.\n\n\nCode\n(\n    df.groupby(\"make\")\n    .agg(\n        pl.col(\"city08\").mean().alias(\"mean_cyl\"),\n        pl.col(\"displ\").mean().alias(\"mean_disp\"),\n    )\n    .filter(pl.col(\"mean_cyl\").is_not_nan())\n    .sort(by=[\"mean_cyl\", \"mean_disp\"], descending=[True, True])\n    .limit(5)\n)\n\n\n\nshape: (5, 3)\n\n\n\nmake\nmean_cyl\nmean_disp\n\n\ncat\nf64\nf64\n\n\n\n\n\"Tesla\"\n94.925373\nNaN\n\n\n…\n…\n…\n\n\n\"Azure Dynamics…\n62.0\nNaN\n\n\n\n\n\n\nAggregated columns can be renamed immediately with the alias method.\n\n\nCode\n(\n    df.groupby(\"make\", \"fuelType\")\n    .agg(pl.col(\"comb08\").mean().alias(\"filtered\"))\n    .filter((pl.col(\"fuelType\") == \"CNG\") | (pl.col(\"fuelType\") == \"Diesel\"))\n)\n\n\n\nshape: (38, 3)\n\n\n\nmake\nfuelType\nfiltered\n\n\ncat\ncat\nf64\n\n\n\n\n\"Mazda\"\n\"Diesel\"\n29.714286\n\n\n…\n…\n…\n\n\n\"Ford\"\n\"Diesel\"\n29.542857\n\n\n\n\n\n\nHere’s a scenario for performing aggregation based on the year. In this case, the “year” column holds the year-related data, which can be extracted using the dt.year attribute. Keep in mind that Polars doesn’t have native plotting capabilities like pandas. To visualize the data, as a final step, you can convert the dataframe to pandas and make use of its .plot method.\n\n\nCode\n(\n    df.groupby(\n        [\n            pl.col(\"year\").dt.year().alias(\"year\"),\n        ]\n    )\n    .agg([pl.col(\"comb08\").mean(), pl.col(\"city08\").mean()])\n    .sort(\n        [\n            \"year\",\n        ]\n    )\n    .to_pandas()\n    .set_index(\"year\")\n    .plot()\n)\n\n\n&lt;Axes: xlabel='year'&gt;\n\n\n\n\n\nHere’s another fantastic option for you: the groupby_dynamic feature. It’s really impressive, by the way. You can employ this when grouping based on years or any other time-related information. Additionally, you can make use of the every parameter for resampling, which closely resembles what you can do with pandas.\n\n\nCode\n(\n    df.sort(by=\"year\")\n    .groupby_dynamic(index_column=\"year\", every=\"2y\")\n    .agg([pl.col(\"comb08\").mean(), pl.col(\"city08\").mean()])\n    .sort(\n        [\n            \"year\",\n        ]\n    )\n    .to_pandas()\n    .set_index(\"year\")\n    .plot()\n)\n\n\n&lt;Axes: xlabel='year'&gt;"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#pivot",
    "href": "posts/polars/Polars_revised.html#pivot",
    "title": "Getting Started with Polars",
    "section": "Pivot",
    "text": "Pivot\nYou can also accomplish split-apply-combine tasks seamlessly using the pivot function. It’s remarkably straightforward.\n\n\nCode\n(\n    df.with_columns(\n        pl.col(\"fuelType\").cast(pl.Utf8)\n    )  # conversion to str is needed for the next step\n    .filter(pl.col(\"fuelType\").is_in([\"Regular\", \"Premium\"]))\n    .pivot(values=\"city08\", index=\"make\", columns=\"fuelType\", aggregate_function=\"mean\")\n)\n\n\n\nshape: (126, 3)\n\n\n\nmake\nRegular\nPremium\n\n\ncat\nf64\nf64\n\n\n\n\n\"Alfa Romeo\"\n17.142857\n18.902439\n\n\n…\n…\n…\n\n\n\"PAS Inc - GMC\"\nnull\n14.0"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#getting-missing-value-count-per-column",
    "href": "posts/polars/Polars_revised.html#getting-missing-value-count-per-column",
    "title": "Getting Started with Polars",
    "section": "Getting missing value count per column",
    "text": "Getting missing value count per column\nThe null_count function showcases the count of missing values in the dataframe. Remember, these are not the same as np.nans; it’s important to be aware of this distinction. This functionality is akin to the pandas df.isna().sum() operation if you’re familiar with pandas.\n\n\nCode\ndf.null_count()\n\n\n\nshape: (1, 11)\n\n\n\nmake\nmodel\ndrive\ncylinders\ndispl\nfuelType\nmpgData\ncity08\nhighway08\ncomb08\nyear\n\n\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\n\n\n\n\n0\n0\n1189\n206\n204\n0\n0\n27\n0\n7\n0"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#getting-boolean-array-of-missing-values",
    "href": "posts/polars/Polars_revised.html#getting-boolean-array-of-missing-values",
    "title": "Getting Started with Polars",
    "section": "Getting boolean array of missing values",
    "text": "Getting boolean array of missing values\nIf you ever need to manipulate a series based on its null values, you can also obtain a boolean mask using the is_null function. This can be really handy for targeted data manipulation.\n\n\nCode\ndf.select(pl.col(\"city08\").is_null())\n\n\n\n\nshape: (41_144, 1)\n\n\n\ncity08\n\n\nbool\n\n\n\n\nfalse\n\n\n…\n\n\nfalse"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#filling-missing-values",
    "href": "posts/polars/Polars_revised.html#filling-missing-values",
    "title": "Getting Started with Polars",
    "section": "Filling missing values",
    "text": "Filling missing values\nYou have a variety of options at your disposal for filling in missing values. Here’s a list of some of the most common ones, although it’s not an exhaustive rundown.\n\nWith literals\n\n\nCode\n(df.with_columns(pl.col(\"cylinders\").fill_null(pl.lit(2))))\n\n\n\nshape: (41_144, 11)\n\n\n\nmake\nmodel\ndrive\ncylinders\ndispl\nfuelType\nmpgData\ncity08\nhighway08\ncomb08\nyear\n\n\ncat\ncat\ncat\ni8\nf32\ncat\nbool\ni8\ni8\ni8\ndate\n\n\n\n\n\"Alfa Romeo\"\n\"Spider Veloce …\n\"Rear-Wheel Dri…\n4\n2.0\n\"Regular\"\ntrue\n19\n25\n21\n1985-01-01\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Subaru\"\n\"Legacy AWD Tur…\n\"4-Wheel or All…\n4\n2.2\n\"Premium\"\nfalse\n16\n21\n18\n1993-01-01\n\n\n\n\n\n\n\n\nCode\n(df.with_columns(pl.col(\"cylinders\").fill_null(strategy=\"zero\")))\n\n\n\nshape: (41_144, 11)\n\n\n\nmake\nmodel\ndrive\ncylinders\ndispl\nfuelType\nmpgData\ncity08\nhighway08\ncomb08\nyear\n\n\ncat\ncat\ncat\ni8\nf32\ncat\nbool\ni8\ni8\ni8\ndate\n\n\n\n\n\"Alfa Romeo\"\n\"Spider Veloce …\n\"Rear-Wheel Dri…\n4\n2.0\n\"Regular\"\ntrue\n19\n25\n21\n1985-01-01\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Subaru\"\n\"Legacy AWD Tur…\n\"4-Wheel or All…\n4\n2.2\n\"Premium\"\nfalse\n16\n21\n18\n1993-01-01\n\n\n\n\n\n\n\n\nCode\n(df.with_columns(pl.col(\"cylinders\").fill_null(strategy=\"forward\")))\n\n\n\nshape: (41_144, 11)\n\n\n\nmake\nmodel\ndrive\ncylinders\ndispl\nfuelType\nmpgData\ncity08\nhighway08\ncomb08\nyear\n\n\ncat\ncat\ncat\ni8\nf32\ncat\nbool\ni8\ni8\ni8\ndate\n\n\n\n\n\"Alfa Romeo\"\n\"Spider Veloce …\n\"Rear-Wheel Dri…\n4\n2.0\n\"Regular\"\ntrue\n19\n25\n21\n1985-01-01\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Subaru\"\n\"Legacy AWD Tur…\n\"4-Wheel or All…\n4\n2.2\n\"Premium\"\nfalse\n16\n21\n18\n1993-01-01\n\n\n\n\n\n\n\n\nWith an expression\nThis can be very useful for machine learning pipelines. When addressing missing values, you can set them to the mode, median, mean, or any other value that suits your needs.\n\n\nCode\n(df.with_columns(pl.col(\"cylinders\").fill_null(pl.col(\"cylinders\").mode())))\n\n\n\nshape: (41_144, 11)\n\n\n\nmake\nmodel\ndrive\ncylinders\ndispl\nfuelType\nmpgData\ncity08\nhighway08\ncomb08\nyear\n\n\ncat\ncat\ncat\ni8\nf32\ncat\nbool\ni8\ni8\ni8\ndate\n\n\n\n\n\"Alfa Romeo\"\n\"Spider Veloce …\n\"Rear-Wheel Dri…\n4\n2.0\n\"Regular\"\ntrue\n19\n25\n21\n1985-01-01\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Subaru\"\n\"Legacy AWD Tur…\n\"4-Wheel or All…\n4\n2.2\n\"Premium\"\nfalse\n16\n21\n18\n1993-01-01\n\n\n\n\n\n\n\n\nUsing the over function\nThe over function in Polars is used to perform window operations, akin to what you’d achieve with window functions in SQL or the transform function in pandas. This function enables you to compute aggregations over a specified window or range of rows defined by a window specification. It’s perfect for tasks like calculating rolling averages, cumulative sums, and other operations that involve working with a specific subset of rows within the dataset.\nLet’s begin by filtering the car makes to encompass only the top 5 car brands. Following that, we can utilize the over function to derive several statistics.\n\n\nCode\ntop5 = (\n    df.select(pl.col(\"make\"))\n    .to_series()\n    .value_counts()\n    .sort(by=\"counts\", descending=True)\n    .limit(3)\n    .select(pl.col(\"make\"))\n    .to_series()\n)\n\n(\n    df.filter(pl.col(\"make\").is_in(top5)).select(\n        \"make\",\n        \"model\",\n        pl.col(\"city08\").mean().over(\"make\").alias(\"avg_city_mpg_by_make\"),\n        pl.col(\"city08\").mean().over(\"model\").alias(\"avg_city_mpg_by_model\"),\n        pl.col(\"comb08\").mean().over([\"make\", \"model\"]).alias(\"avg_comb_mpg_by_model\"),\n    )\n)\n\n\n\nshape: (9_957, 5)\n\n\n\nmake\nmodel\navg_city_mpg_by_make\navg_city_mpg_by_model\navg_comb_mpg_by_model\n\n\ncat\ncat\nf64\nf64\nf64\n\n\n\n\n\"Dodge\"\n\"Charger\"\n15.462253\n18.197531\n21.320988\n\n\n…\n…\n…\n…\n…\n\n\n\"Dodge\"\n\"B150/B250 Wago…\n15.462253\n11.654321\n12.925926"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#operating-on-multiple-columns-and-renaming-them",
    "href": "posts/polars/Polars_revised.html#operating-on-multiple-columns-and-renaming-them",
    "title": "Getting Started with Polars",
    "section": "Operating on multiple columns and renaming them",
    "text": "Operating on multiple columns and renaming them\nYou have the flexibility to conduct column operations and then easily rename them. Additionally, you can make use of the prefix and suffix functions to streamline your workflow.\n\n\nCode\n(df.with_columns((cs.numeric() * 2).prefix(\"changed_\")))\n\n\n\nshape: (41_144, 16)\n\n\n\nmake\nmodel\ndrive\ncylinders\ndispl\nfuelType\nmpgData\ncity08\nhighway08\ncomb08\nyear\nchanged_cylinders\nchanged_displ\nchanged_city08\nchanged_highway08\nchanged_comb08\n\n\ncat\ncat\ncat\ni8\nf32\ncat\nbool\ni8\ni8\ni8\ndate\ni8\nf32\ni8\ni8\ni8\n\n\n\n\n\"Alfa Romeo\"\n\"Spider Veloce …\n\"Rear-Wheel Dri…\n4\n2.0\n\"Regular\"\ntrue\n19\n25\n21\n1985-01-01\n8\n4.0\n38\n50\n42\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Subaru\"\n\"Legacy AWD Tur…\n\"4-Wheel or All…\n4\n2.2\n\"Premium\"\nfalse\n16\n21\n18\n1993-01-01\n8\n4.4\n32\n42\n36"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#count-unique-values",
    "href": "posts/polars/Polars_revised.html#count-unique-values",
    "title": "Getting Started with Polars",
    "section": "Count unique values",
    "text": "Count unique values\nAbsolutely, you can also count the unique values within a series in Polars, much like you would with the unique function in pandas. This can be really useful for understanding the distribution and diversity of data within a specific column.\n\n\nCode\ndf.select(pl.col(\"make\")).n_unique()\n\n\n136"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#value_counts-to-get-number-of-occurrences-per-item",
    "href": "posts/polars/Polars_revised.html#value_counts-to-get-number-of-occurrences-per-item",
    "title": "Getting Started with Polars",
    "section": "Value_counts to get number of occurrences per item",
    "text": "Value_counts to get number of occurrences per item\n\nOn a single column\n\n\nCode\n(df.select(pl.col(\"make\")).to_series().value_counts(sort=True))\n\n\n\n\nshape: (136, 2)\n\n\n\nmake\ncounts\n\n\ncat\nu32\n\n\n\n\n\"Chevrolet\"\n4003\n\n\n…\n…\n\n\n\"Isis Imports L…\n1\n\n\n\n\n\n\n\nCode\n(df.select(pl.col(\"make\")).groupby(\"make\").count().sort(by=\"count\", descending=True))\n\n\n\n\nshape: (136, 2)\n\n\n\nmake\ncount\n\n\ncat\nu32\n\n\n\n\n\"Chevrolet\"\n4003\n\n\n…\n…\n\n\n\"Panoz Auto-Dev…\n1\n\n\n\n\n\n\n\nOn multiple columns\nI noticed that Polars lacks the capability to perform value counts on multiple columns, unlike pandas’ value_counts function which operates only on series. However, I’ve discovered that combining a groupby operation with a count function can effectively achieve the same outcome for multiple columns. It’s all about finding alternative approaches that get the job done!\n\n\nCode\n(\n    df.select(pl.col(\"make\", \"model\"))\n    .groupby([\"make\", \"model\"])\n    .count()\n    .sort(by=\"count\", descending=True)\n)\n\n\n\n\nshape: (4_127, 3)\n\n\n\nmake\nmodel\ncount\n\n\ncat\ncat\nu32\n\n\n\n\n\"Ford\"\n\"F150 Pickup 2W…\n214\n\n\n…\n…\n…\n\n\n\"Mercedes-Benz\"\n\"400SE\"\n1"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#conditionals-if-else-statements",
    "href": "posts/polars/Polars_revised.html#conditionals-if-else-statements",
    "title": "Getting Started with Polars",
    "section": "Conditionals/ if-else statements",
    "text": "Conditionals/ if-else statements\nIf you’re looking to integrate conditional if-else statements into your Polars chain, you can make use of the when, then, and otherwise functions. However, keep in mind that a series of chained when, then statements should be interpreted as if, elif, … elif, rather than if, if, … if. In other words, the first condition that evaluates to True will be selected. It’s crucial to understand this distinction when crafting your conditions.\n\n\nCode\n(\n    df.select(\n        pl.col(\"make\"),\n        pl.when(pl.col(\"comb08\") &lt; 15)\n        .then(\"bad\")\n        .when(pl.col(\"comb08\") &lt; 30)\n        .then(\"good\")\n        .otherwise(\"very good\")\n        .alias(\"fuel_economy\"),\n    )\n)\n\n\n\n\nshape: (41_144, 2)\n\n\n\nmake\nfuel_economy\n\n\ncat\nstr\n\n\n\n\n\"Alfa Romeo\"\n\"good\"\n\n\n…\n…\n\n\n\"Subaru\"\n\"good\"\n\n\n\n\n\nAlright, folks, that’s it for this introduction to Polars. I know it wasn’t exactly short, but I hope it was informative! I’ll be adding more content to this article in the future, so stay tuned. Until then, happy coding 😎💻🔎"
  },
  {
    "objectID": "posts/refugee/refugee.html",
    "href": "posts/refugee/refugee.html",
    "title": "Tidy Tuesday: Refugees",
    "section": "",
    "text": "Paolo Bernasconi – Refugiarte 2018\n\n\nWelcome to our blog dedicated to Tidy Tuesday. This week, we venture into the sobering realm of refugee statistics using the {refugees} R package. This tool grants access to the United Nations High Commissioner for Refugees (UNHCR) Refugee Data Finder, providing critical insights into forcibly displaced populations spanning over seven decades. With data from UNHCR, UNRWA, and the Internal Displacement Monitoring Centre, we’ll explore a subset of this information, focusing on population statistics from 2010 to 2022.\nJoin us as we explore the landscape of refugee data.\n\nSetup\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nimport numpy as np\n\nfrom pathlib import Path\nimport json\nfrom urllib.request import urlopen\n\n\nfrom lets_plot import *\nfrom lets_plot.mapping import as_discrete\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\nLetsPlot.setup_html()\n\nimport plotly.io as pio\nimport plotly.express as px\n\npio.templates.default = \"presentation\"\n\n\n\n            \n            \n            \n\n\n\n\nCode\ntry:\n    df = pd.read_csv(\n        \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-08-22/population.csv\"\n    )\nexcept:\n    df = pd.read_csv(\n        Path.cwd().joinpath(\n            \"raw.githubusercontent.com_rfordatascience_tidytuesday_master_data_2023_2023-08-22_population.csv\"\n        )\n    )\n\ndf = (\n    df\n    # .assign(year=lambda df: pd.to_datetime(df.year, format='%Y'))\n)\n\ndf.sample(5)\n\n\n\n\n\n\n\n\n\nyear\ncoo_name\ncoo\ncoo_iso\ncoa_name\ncoa\ncoa_iso\nrefugees\nasylum_seekers\nreturned_refugees\nidps\nreturned_idps\nstateless\nooc\noip\nhst\n\n\n\n\n14314\n2013\nBulgaria\nBUL\nBGR\nUnited Kingdom of Great Britain and Northern I...\nGBR\nGBR\n9\n5\n0\n0\n0\n0\n0\nNaN\nNaN\n\n\n56951\n2021\nBurundi\nBDI\nBDI\nNamibia\nNAM\nNAM\n310\n135\n10\n0\n0\n0\n0\nNaN\nNaN\n\n\n64238\n2022\nLebanon\nLEB\nLBN\nSyrian Arab Rep.\nSYR\nSYR\n25\n18\n0\n0\n0\n0\n0\nNaN\n0.0\n\n\n50029\n2020\nViet Nam\nSRV\nVNM\nGreece\nGRE\nGRC\n5\n12\n0\n0\n0\n0\n0\nNaN\nNaN\n\n\n59984\n2022\nBelarus\nBLR\nBLR\nChina\nCHI\nCHN\n0\n5\n0\n0\n0\n0\n0\nNaN\n0.0\n\n\n\n\n\n\n\nHere is the data dictionary:\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nyear\ndouble\nThe year.\n\n\ncoo_name\ncharacter\nCountry of origin name.\n\n\ncoo\ncharacter\nCountry of origin UNHCR code.\n\n\ncoo_iso\ncharacter\nCountry of origin ISO code.\n\n\ncoa_name\ncharacter\nCountry of asylum name.\n\n\ncoa\ncharacter\nCountry of asylum UNHCR code.\n\n\ncoa_iso\ncharacter\nCountry of asylum ISO code.\n\n\nrefugees\ndouble\nThe number of refugees.\n\n\nasylum_seekers\ndouble\nThe number of asylum-seekers.\n\n\nreturned_refugees\ndouble\nThe number of returned refugees.\n\n\nidps\ndouble\nThe number of internally displaced persons.\n\n\nreturned_idps\ndouble\nThe number of returned internally displaced persons.\n\n\nstateless\ndouble\nThe number of stateless persons.\n\n\nooc\ndouble\nThe number of others of concern to UNHCR.\n\n\noip\ndouble\nThe number of other people in need of international protection.\n\n\nhst\ndouble\nThe number of host community members.\n\n\n\nLooking at the data dictionary, here are the questions we will be exploring in this blog post:\n\nHow has the overall refugee count evolved over the years?\nWhat countries experience the highest annual refugee outflows?\nWhat countries receive the most refugees annually?\nWhere do most refugees from the country with the highest numbers go?\nWhich countries have the highest numbers of internally displaced persons?\nWhat countries have the highest stateless populations?\n\nIn this blog, to visualize the topological data, we will be utilizing Plotly’s choropleth_mapbox function as well as scatter_geo. For a quick overview about Plotly check out this website. In addition, our GeoJSON file, which represents simple geographical features along with their non-spatial attributes, is sourced from GitHub curated by Rufus Pollock.\n\n\nCode\ntry:\n    with urlopen(\n        \"https://raw.githubusercontent.com/datasets/geo-boundaries-world-110m/master/countries.geojson\"\n    ) as response:\n        countries = json.load((response))\nexcept:\n    countries = json.load(open(\"countries.geojson\"))\n\n\n\n\nHow has the overall refugee count evolved over the years?\nAs you can see, the number of stateless persons has not changed significantly over the period studied; on the other hand, the number of internally displaced persons has dramatically increased from over 14 million in 2010 to just shy of 60 million by 2022\n\n\nCode\nfig = (\n    df.groupby(\"year\")[[\"refugees\", \"asylum_seekers\", \"stateless\", \"idps\"]]\n    .sum()\n    .reset_index()\n    .melt(id_vars=\"year\")\n    .pipe(\n        lambda df: px.line(\n            df,\n            x=df.year,\n            y=df.value,\n            color=df.variable,\n            width=750,\n            height=600,\n            # markers=True,\n            labels={\n                \"year\": \"Year\",\n                \"value\": \"Number of individuals\",\n            },\n        )\n    )\n)\nfig.update_layout(legend_title_text=\"Categories\")\n\n\n\n\n                                                \nFigure 1: Overall statistics\n\n\n\n\n\nWhat countries experience the highest annual refugee outflows?\nAmong all the countries under examination, the highlighted ones represent the top 5 nations with the highest registered refugee populations. It’s evident that the Syrian Arab Republic consistently maintained the highest number of registered refugees from 2013 to 2022, followed closely by Afghanistan\n\n\nCode\nfig = go.Figure()\n\ntop10_countries = (\n    df.groupby(\"coo_name\")\n    .refugees.sum()\n    .to_frame()\n    .reset_index()\n    .sort_values(by=\"refugees\", ascending=False)\n    .head(5)\n    .coo_name.values\n)\n\ntest = (\n    df.groupby([\"year\", \"coo_name\"])\n    .refugees.sum()\n    .to_frame()\n    .reset_index()\n    .assign(\n        color=lambda df: df.coo_name.mask(\n            df.coo_name == (top10_countries[0]), \"#6eb14a\"\n        )\n        .mask(df.coo_name == (top10_countries[1]), \"#2d4526\")\n        .mask(df.coo_name == (top10_countries[2]), \"#f6d19b\")\n        .mask(df.coo_name == (top10_countries[3]), \"#ca3780\")\n        .mask(df.coo_name == (top10_countries[4]), \"#e9b4cd\")\n        .mask(~df.coo_name.isin(top10_countries), \"#dadadc\")\n    )\n)\n\n# Loop over each unique 'coo_name'\nfor coo_name in test[\"coo_name\"].unique():\n    df_subset = test[test[\"coo_name\"] == coo_name]\n    if coo_name in top10_countries:\n        fig.add_trace(\n            go.Scatter(\n                x=df_subset[\"year\"],\n                y=df_subset[\"refugees\"],\n                mode=\"lines+markers\",\n                name=coo_name,\n                showlegend=True,\n                line=dict(\n                    color=df_subset[\"color\"].iloc[0], dash=\"dash\"\n                ),  # Use the first color from the subset\n            )\n        )\n    else:\n        fig.add_trace(\n            go.Scatter(\n                x=df_subset[\"year\"],\n                y=df_subset[\"refugees\"],\n                mode=\"lines\",\n                name=coo_name,\n                showlegend=False,\n                line=dict(\n                    color=df_subset[\"color\"].iloc[0], dash=\"dot\"\n                ),  # Use the first color from the subset\n            )\n        )\n\n\nfig.update_layout(\n    autosize=False, width=750, height=600, yaxis_title=\"Sum Annual Refugee Outflows\"\n)\nfig.show()\n\n\n\n\n                                                \nFigure 2: Top 5 countries with highest number of sum annual refugee outflows\n\n\n\nI have also prepared a short animation that depicts the year-by-year pattern of refugee outflows.\n\n\nCode\n(\n    df.assign(year=lambda df: df.year.astype(str).str.split(\"-\", expand=True)[0])\n    .groupby([\"year\", \"coo_iso\", \"coo_name\"])[\"refugees\"]\n    .sum()\n    .to_frame()\n    .reset_index()\n    .pipe(\n        lambda df: px.choropleth_mapbox(\n            data_frame=df,\n            locations=\"coo_iso\",\n            geojson=countries,\n            color=\"refugees\",\n            animation_frame=\"year\",\n            animation_group=\"coo_iso\",\n            hover_name=\"coo_name\",\n            hover_data={\n                \"coo_iso\": \"\",\n            },\n            range_color=(0, 8_000_000),\n            color_continuous_scale=px.colors.sequential.Oranges,\n            featureidkey=\"properties.iso_a3\",\n            zoom=1.2,\n            mapbox_style=\"carto-positron\",\n            opacity=0.6,\n            width=1200,\n            height=700\n            # **fig_dict\n        )\n    )\n)\n\n\n\n\n                                                \nFigure 3: Patterns of Annual Refugee Outflows\n\n\n\n\n\nWhat countries receive the most refugees annually?\nAfter examining which countries have the most refugees leaving, we will shift our focus to the countries with the highest number of refugees received.\n\n\nCode\nfig = go.Figure()\n\ntop5_countries_accept = (\n    df.groupby(\"coa_name\")[[\"refugees\"]]\n    .sum()\n    .sort_values(by=\"refugees\", ascending=False)\n    .head(5)\n    .index\n)\n\ntest = (\n    df.groupby([\"year\", \"coa_name\"])\n    .refugees.sum()\n    .to_frame()\n    .reset_index()\n    .assign(\n        color=lambda df: df.coa_name.mask(\n            df.coa_name == (top5_countries_accept[0]), \"#6eb14a\"\n        )\n        .mask(df.coa_name == (top5_countries_accept[1]), \"#2d4526\")\n        .mask(df.coa_name == (top5_countries_accept[2]), \"#f6d19b\")\n        .mask(df.coa_name == (top5_countries_accept[3]), \"#ca3780\")\n        .mask(df.coa_name == (top5_countries_accept[4]), \"#e9b4cd\")\n        .mask(~df.coa_name.isin(top5_countries_accept), \"#dadadc\")\n    )\n)\n\n# Loop over each unique 'coo_name'\nfor coa_name in test[\"coa_name\"].unique():\n    df_subset = test[test[\"coa_name\"] == coa_name]\n    if coa_name in top5_countries_accept:\n        fig.add_trace(\n            go.Scatter(\n                x=df_subset[\"year\"],\n                y=df_subset[\"refugees\"],\n                mode=\"lines+markers\",\n                name=coa_name,\n                showlegend=True,\n                line=dict(\n                    color=df_subset[\"color\"].iloc[0], dash=\"dash\"\n                ),  # Use the first color from the subset\n            )\n        )\n    else:\n        fig.add_trace(\n            go.Scatter(\n                x=df_subset[\"year\"],\n                y=df_subset[\"refugees\"],\n                mode=\"lines\",\n                name=coa_name,\n                showlegend=False,\n                line=dict(\n                    color=df_subset[\"color\"].iloc[0], dash=\"dot\"\n                ),  # Use the first color from the subset\n            )\n        )\n\n\nfig.update_layout(\n    autosize=False, width=750, height=600, yaxis_title=\"Total Annual Refugee Arrivals\"\n)\nfig.show()\n\n\n\n\n                                                \nFigure 4: Top 5 countries with highest number of refugees accepted\n\n\n\n\n\nWhere do most refugees from the country with the highest numbers go?\nNext, we wanted to find out where Syrian refugees have found welcomes for their new homes.\n\n\nCode\nfig = (\n    df.query(\"coo_name == 'Syrian Arab Rep.'\")\n    .groupby(\"coa_name\")\n    .refugees.sum()\n    .pipe(lambda x: x / x.sum())\n    .mul(100)\n    .sort_values(ascending=False)\n    .head(10)\n    .to_frame()\n    .reset_index()\n    .pipe(\n        lambda df: px.bar(\n            df,\n            y=\"coa_name\",\n            x=\"refugees\",\n            labels={\n                \"refugees\": \"Acceptance Rate of Syrian Refugees (%)\",\n                \"coa_name\": \"\",\n            },\n            height=500,\n            width=1000,\n        )\n    )\n)\nfig.update_yaxes(tickangle=0, automargin=True)\nfig\n\n\n\n\n                                                \nFigure 5: Welcoming Destinations for Syrian Refugees\n\n\n\n\n\nWhich countries have the highest numbers of internally displaced persons?\nNext, we’ll explore the count of internally displaced persons (IDPs). An IDP is someone who has had to leave their home or usual residence because of reasons like armed conflict, violence, human rights violations, natural disasters, or humanitarian emergencies. Unlike refugees, IDPs haven’t crossed international borders to find safety; they stay within their own country’s borders\n\n\nCode\nfig = (\n    df.groupby([\"coo_iso\", \"coo_name\"])\n    .idps.sum()\n    .sort_values(ascending=False)\n    .to_frame()\n    .reset_index()\n    .head(50)\n    .pipe(\n        lambda df: px.scatter_geo(\n            df,\n            locations=\"coo_iso\",\n            geojson=countries,\n            hover_name=\"coo_name\",\n            # color='coo_name',\n            featureidkey=\"properties.iso_a3\",\n            size=\"idps\",\n            # opacity = 0.6,\n            width=1200,\n            height=700,\n        )\n    )\n)\nfig\n\n\n\n\n                                                \nFigure 6: Top 50 nations hosting the largest internally displaced populations\n\n\n\n\n\nWhat countries have received the highest stateless populations?\nNext, we focus on countries that have provided refuge to a substantial number of stateless individuals. Stateless persons are individuals who don’t have the legal status of citizenship in any country. They lack the rights and protection typically granted to citizens. Stateless people often encounter significant difficulties in accessing education, healthcare, jobs, and the freedom to travel.\n\n\nCode\nfig = (\n    df.groupby([\"coa_iso\", \"coa_name\"])\n    .stateless.sum()\n    .pipe(lambda x: x / 49036122)\n    .sort_values(ascending=False)\n    .mul(100)\n    .to_frame()\n    .reset_index()\n    .head(10)\n    .pipe(\n        lambda df: px.scatter(\n            df,\n            y=\"coa_name\",\n            x=\"stateless\",\n            labels={\n                \"stateless\": \"Acceptance Rate of Stateless Populations (%)\",\n                \"coa_name\": \"\",\n            },\n            height=500,\n            width=800,\n        )\n    )\n)\nfig.update_yaxes(tickangle=0, automargin=True)\nfig.update_layout(xaxis_range=[0, 100])\n\nfig\n\n\n\n\n                                                \nFigure 7: Top 10 nations sheltering the largest stateless populations\n\n\n\nAnd there you have it, everyone. That wraps up this week’s data exploration. I’m cant’t wait for what Tidy Tuesday has in store for us next week. Until then, take good care of yourselves, and I’ll catch you soon!"
  },
  {
    "objectID": "posts/Top 10 things I learned from Effective Pandas/2023_08_02-Top_10_things_I_learned_from_Effective_Pandas.html",
    "href": "posts/Top 10 things I learned from Effective Pandas/2023_08_02-Top_10_things_I_learned_from_Effective_Pandas.html",
    "title": "Top 10 things I learned from the book Effective Pandas by Matt Harrison",
    "section": "",
    "text": "Effective Pandas by Matt Harrison is a guide to the Pandas library, a powerful Python tool for data manipulation and analysis. The book covers a wide range of topics, from basic data structures to advanced techniques for data cleaning, transformation, and visualization.\nI have found Effective Pandas to be a captivating and thought-provoking read. The book offers a genuinely unique take on data wrangling, putting a great emphasis on the utility of chaining methods and utilizing the lambda function. I have found these ideas to be so useful and practical that I have revisited the book multiple times just to make sure I keep them fresh in my memory. I must have read the book from back to back at least 3-4 times.\nIn this article, I will share the top 10 (+1) things I learned from Effective Pandas. These are the concepts and techniques that I found to be the most useful and practical.\nWe will use the Real World Smartphone’s Dataset by Abhijit Dahatonde from Kaggle. Let’s get to it.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\npd.set_option('display.max_columns', None)\n\n\n\nLoad the dataset\n\n\nCode\ndf=pd.read_csv('smartphones.csv')\n# some info about the dataframe, such as dimensions and dtypes of columns\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 980 entries, 0 to 979\nData columns (total 22 columns):\n #   Column                     Non-Null Count  Dtype  \n---  ------                     --------------  -----  \n 0   brand_name                 980 non-null    object \n 1   model                      980 non-null    object \n 2   price                      980 non-null    int64  \n 3   avg_rating                 879 non-null    float64\n 4   5G_or_not                  980 non-null    int64  \n 5   processor_brand            960 non-null    object \n 6   num_cores                  974 non-null    float64\n 7   processor_speed            938 non-null    float64\n 8   battery_capacity           969 non-null    float64\n 9   fast_charging_available    980 non-null    int64  \n 10  fast_charging              769 non-null    float64\n 11  ram_capacity               980 non-null    int64  \n 12  internal_memory            980 non-null    int64  \n 13  screen_size                980 non-null    float64\n 14  refresh_rate               980 non-null    int64  \n 15  num_rear_cameras           980 non-null    int64  \n 16  os                         966 non-null    object \n 17  primary_camera_rear        980 non-null    float64\n 18  primary_camera_front       975 non-null    float64\n 19  extended_memory_available  980 non-null    int64  \n 20  resolution_height          980 non-null    int64  \n 21  resolution_width           980 non-null    int64  \ndtypes: float64(8), int64(10), object(4)\nmemory usage: 168.6+ KB\n\n\n\n\nTip #1: Use pd.assign more extensively\nThe pd.assign method in Pandas is a very powerful tool that can be used to create new columns, modify existing columns, or both. It is a very versatile method that can be used in a variety of ways.\nOne of the most important benefits of using the assign method is that it can be incorporated into method chaining. This means that you can chain multiple assign methods together to create a more concise and readable code. Another benefit of using the assign method is that it completely sidesteps the infamous SettingWithCopyWarning. This warning is often triggered when you try to modify an existing column in a DataFrame. However, the assign method creates a new DataFrame, so there is no need to worry about this warning.\nProblem statement: Let’s say we would like to capitalize the brand names located in the brand_name column as well as calculate the Pixels Per Inch (PPI). PPI can be calculated following the equation described by the Pixel density page on Wikipedia.\n\n\nCode\n(df\n .assign(brand_name=lambda df: df.brand_name.str.capitalize(), # capitalizes the brand names \n         PPI=lambda df: (np.sqrt(np.square(df.resolution_height) + np.square(df.resolution_width))\n                         .div(df.screen_size)\n                         .round(1)\n                        )\n        )\n .loc[:, ['brand_name','model','PPI']]\n .sort_values(by='PPI',ascending=False)\n .head(5)\n)\n\n\n\n\n\n\n\n\n\nbrand_name\nmodel\nPPI\n\n\n\n\n689\nSony\nSony Xperia 1 IV (12GB RAM + 512GB)\n642.6\n\n\n696\nSony\nSony Xperia Pro-I\n642.6\n\n\n688\nSony\nSony Xperia 1 II\n642.6\n\n\n655\nSamsung\nSamsung Galaxy S20\n566.0\n\n\n656\nSamsung\nSamsung Galaxy S20 5G\n566.0\n\n\n\n\n\n\n\n\n\nTip #2: Simplify the management of multiple if-else conditions using np.select\nIf our goal is to incorporate if-else logic seamlessly into our code, we can effortlessly achieve this using either pd.mask or pd.where. Yet, what approach should we adopt when we need to evaluate multiple conditions instead of just two? In such situations, we have two options: we can either employ successive pd.mask or pd.where calls, or we can take advantage of the np.select function as an alternative solution.\nProblem statement: We want to identify the top 3 and top 5 most popular processor brands in smartphones. To do this, we will first create two lists, one for the top 3 brands and one for the top 5 brands. Any processor brand that is not in either of these lists will be categorized as “Other”.\n\n\nCode\n# Let's create the two lists that contain the top3 and top5 brand names\ntop3=df.processor_brand.value_counts().head(3).index\ntop5=df.processor_brand.value_counts().head(5).index\nprint(f'Top 3 most popular processors: {top3.tolist()}')\nprint(f'Top 5 most popular processors: {top5.tolist()}')\n\n\nTop 3 most popular processors: ['snapdragon', 'helio', 'dimensity']\nTop 5 most popular processors: ['snapdragon', 'helio', 'dimensity', 'exynos', 'bionic']\n\n\n\n\nCode\n'''\nHere's an example that employs two successive pd.where calls:\nIn the first pd.where call, it checks whether the brand is in the top 3; if not, it assigns the label \"Top5\" to it.\nThen, in the second call, it checks if the value is in the top 5; if not, it appends the category \"Other\".\nAs you can see, the logic can become intricate and difficult to grasp, especially when dealing with numerous conditions, \nmaking the code cumbersome and hard to manage.\n'''\n(df\n .assign(frequency=lambda df: df.processor_brand\n         .where(df.processor_brand.isin(top3), other = 'Top5')\n         .where(df.processor_brand.isin(top5), other = 'Other')\n        )\n .loc[:, 'frequency']\n .value_counts()\n)\n\n\nsnapdragon    413\nhelio         201\ndimensity     177\nTop5           95\nOther          94\nName: frequency, dtype: int64\n\n\n\n\nCode\n'''\n Now Let's see np.select!\n It simplifies the process significantly. By providing a list of conditions we want to evaluate and their \n corresponding values if the condition evaluates to True, we can handle multiple conditions effortlessly. \n Additionally, we can specify a default value if none of the conditions evaluates to True, making the code \n much more straightforward and easier to manage. \n'''\n(df\n .assign(frequency=lambda df: np.select(condlist=[df.processor_brand.isin(top3), df.processor_brand.isin(top5)],\n                                        choicelist=[df.processor_brand,'Top5'],\n                                        default='Other'\n                                       )\n        )\n .loc[:, 'frequency']\n .value_counts()\n)\n\n\nsnapdragon    413\nhelio         201\ndimensity     177\nTop5           95\nOther          94\nName: frequency, dtype: int64\n\n\n\n\nTip #3 Filter rows or columns with the combination of pd.loc and lambda\nSome experienced Pandas users might consider the following concept trivial, but it was an eye-opener for me after reading the book. It turns out that combining pd.loc and lambda (or any custom functions) allows us to filter both rows and columns, depending on our specific needs.\nProblem statement: We are interested in identifying phones with a battery capacity greater than 5000mAh.\n\n\nCode\n(df\n .loc[lambda df: df.battery_capacity.gt(5000),['model', 'battery_capacity']] # here we use pd.gt() to select values greater than 5000\n .sort_values(by='battery_capacity')\n)\n\n\n\n\n\n\n\n\n\nmodel\nbattery_capacity\n\n\n\n\n70\nGoogle Pixel 6 Pro (12GB RAM + 256GB)\n5003.0\n\n\n69\nGoogle Pixel 6 Pro\n5003.0\n\n\n977\nXiaomi Redmi Note 9 Pro Max\n5020.0\n\n\n922\nXiaomi Redmi Note 10 Lite\n5020.0\n\n\n923\nXiaomi Redmi Note 10 Lite (4GB RAM + 128GB)\n5020.0\n\n\n...\n...\n...\n\n\n624\nSamsung Galaxy F63\n7000.0\n\n\n411\nOukitel WP9\n8000.0\n\n\n410\nOukitel WP21\n9800.0\n\n\n409\nOukitel WP19\n21000.0\n\n\n58\nDoogee V Max\n22000.0\n\n\n\n\n113 rows × 2 columns\n\n\n\n\n\nTip #4 Rename multiple columns effortlessly with rename and replace\nOK. This is a big one for me. I used this one multiple times already. The title pretty much says it all. Let’s say our column names contain spaces, which makes column selection by attribute access pretty much impossible. Now, what do we do? Well…Let’s see.\nProblem statement: We would like to remove all the underscores from our column names.\n\n\nCode\ndf.columns # original column names for reference\n\n\nIndex(['brand_name', 'model', 'price', 'avg_rating', '5G_or_not',\n       'processor_brand', 'num_cores', 'processor_speed', 'battery_capacity',\n       'fast_charging_available', 'fast_charging', 'ram_capacity',\n       'internal_memory', 'screen_size', 'refresh_rate', 'num_rear_cameras',\n       'os', 'primary_camera_rear', 'primary_camera_front',\n       'extended_memory_available', 'resolution_height', 'resolution_width'],\n      dtype='object')\n\n\n\n\nCode\n# column names after replacing underscores\n(df\n .rename(columns = lambda x: x.replace('_', ''))\n .columns\n)\n\n\nIndex(['brandname', 'model', 'price', 'avgrating', '5Gornot', 'processorbrand',\n       'numcores', 'processorspeed', 'batterycapacity',\n       'fastchargingavailable', 'fastcharging', 'ramcapacity',\n       'internalmemory', 'screensize', 'refreshrate', 'numrearcameras', 'os',\n       'primarycamerarear', 'primarycamerafront', 'extendedmemoryavailable',\n       'resolutionheight', 'resolutionwidth'],\n      dtype='object')\n\n\n\n\nTip #5: Use pd.clip to easily remove outliers\n\n\nCode\n# First, we'll identify the phone brands with the most number of handsets present in our dataset.\"\ntop10_brand_names = (df\n                     .brand_name\n                     .value_counts()\n                     .head(10)\n                     .index\n                     .tolist()\n                    )\nprint(top10_brand_names)\n\n# Then we will sort them based on median price\ntop10_brand_names_ordered = (df\n                             .loc[lambda x: x.brand_name.isin(top10_brand_names),['brand_name', 'price']]\n                             .groupby('brand_name')\n                             .median()\n                             .sort_values(by='price')\n                             .index\n                             .to_list()\n                            )\nprint(top10_brand_names_ordered)\n\n\n['xiaomi', 'samsung', 'vivo', 'realme', 'oppo', 'motorola', 'apple', 'oneplus', 'poco', 'tecno']\n['tecno', 'poco', 'realme', 'xiaomi', 'motorola', 'vivo', 'oppo', 'samsung', 'oneplus', 'apple']\n\n\n\n\nCode\nfig, axs = plt.subplots(1,2, figsize=(10,5))\n\n# For reference, this is what our box plot looks if we leave in the outlier values\n(df\n .loc[lambda x: x.brand_name.isin(top10_brand_names),['brand_name', 'price']] # filter rows based on top10_brand_names and select columns\n .pivot(columns='brand_name',values='price') # pivot to get the brand names on the x axis later on\n .loc[:, top10_brand_names_ordered] # order the columns based on median price\n .plot\n .box(title='Top 10 most popular smartphone brands \\n -outlier values included-', \n      rot=90,\n      ax=axs[0]\n     )\n)\n\n(df\n .loc[lambda x: x['brand_name'].isin(top10_brand_names), ['brand_name', 'price']]\n .pivot(columns='brand_name', values='price')\n .loc[:, top10_brand_names_ordered]\n .pipe(lambda df: df.assign(**{col : df[col].clip(lower=df[col].quantile(0.05), # this is called dictionary unpacking\n                                                  upper=df[col].quantile(0.95))\n                              for col in df.columns}))\n .plot\n .box(title='Top 10 most popular smartphone brands \\n -only values between 5th and 95th percentiles included-',\n      rot=90,\n      ax=axs[1]\n     )\n)\naxs[0].set(ylabel='Price in local currency')\nplt.tight_layout()\n\n\n\n\n\n\n\nTip #6: Find corrupted entries with str.extract\nHow often have you encountered the situation where, for some reason, a column that is expected to only contain numerical values displays object as its dtype? This often indicates the presence of some string values mixed within the column. It would be beneficial to promptly identify all the erroneous values, correct them, and proceed with our analysis smoothly. Let’s explore what we can do in such scenarios.\nProblem statement: We would like to identify any cells in a specific column that contain non-numerical values.\n\n\nCode\ndf_bad_values = df.copy(deep=True) # let's prepare a copy of the original dataframe \n\n# let's modify some of the values in the price column randomly:\ndf_bad_values.loc[np.random.randint(0, high=df_bad_values.shape[0], size=10), 'price'] = '.'\ndf_bad_values.loc[np.random.randint(0, high=df_bad_values.shape[0], size=10), 'price'] = '-'\ndf_bad_values.loc[np.random.randint(0, high=df_bad_values.shape[0], size=10), 'price'] = '*'\ndf_bad_values.info() # the modified dataframe's price column now returns object as dtype\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 980 entries, 0 to 979\nData columns (total 22 columns):\n #   Column                     Non-Null Count  Dtype  \n---  ------                     --------------  -----  \n 0   brand_name                 980 non-null    object \n 1   model                      980 non-null    object \n 2   price                      980 non-null    object \n 3   avg_rating                 879 non-null    float64\n 4   5G_or_not                  980 non-null    int64  \n 5   processor_brand            960 non-null    object \n 6   num_cores                  974 non-null    float64\n 7   processor_speed            938 non-null    float64\n 8   battery_capacity           969 non-null    float64\n 9   fast_charging_available    980 non-null    int64  \n 10  fast_charging              769 non-null    float64\n 11  ram_capacity               980 non-null    int64  \n 12  internal_memory            980 non-null    int64  \n 13  screen_size                980 non-null    float64\n 14  refresh_rate               980 non-null    int64  \n 15  num_rear_cameras           980 non-null    int64  \n 16  os                         966 non-null    object \n 17  primary_camera_rear        980 non-null    float64\n 18  primary_camera_front       975 non-null    float64\n 19  extended_memory_available  980 non-null    int64  \n 20  resolution_height          980 non-null    int64  \n 21  resolution_width           980 non-null    int64  \ndtypes: float64(8), int64(9), object(5)\nmemory usage: 168.6+ KB\n\n\n\n\nCode\n# let's find the corrupted values easily:\n(df_bad_values\n .price\n .str.extract(r'([^a-zA-Z])') # returns NON-matching alphabetical characters\n .value_counts()\n)\n\n\n*    10\n-    10\n.    10\ndtype: int64\n\n\n\n\nTip #7: Sort values based on the key parameter\nThe pd.sort_values function surprised me with its versatility. Previously, I had only used its by, axis, and ascending parameters for sorting. However, Matt’s book introduced me to its key parameter, which allows us to apply any function to sort the values. The only constraint is that the function must return a Series.\nProblem statement: For whatever reason, we would like to sort the phone model names in ascending order based on their second letter.\n\n\nCode\n(df\n .iloc[:, 1:3]\n .sort_values(by='model',\n              key = lambda x: x.str[1],\n              ascending = True\n             )\n .head(10)\n)\n\n\n\n\n\n\n\n\n\nmodel\nprice\n\n\n\n\n55\nCAT S22 Flip\n14999\n\n\n697\nTCL Ion X\n8990\n\n\n195\nLG V60 ThinQ\n79990\n\n\n196\nLG Velvet 5G\n54999\n\n\n197\nLG Wing 5G\n54999\n\n\n108\niKall Z19 Pro\n8099\n\n\n107\niKall Z19\n7999\n\n\n106\niKall Z18\n6799\n\n\n54\nBLU F91 5G\n14990\n\n\n413\nPOCO C31 (4GB RAM + 64GB)\n7499\n\n\n\n\n\n\n\n\n\nTip #8: Reference an existing variable inside pd.query with @\nThis resembles Tip #4, as it’s a technique I frequently use. Since reading Matt’s book, I have started using pd.query extensively to filter rows based on values, instead of relying on .loc or .iloc. In case you choose to adopt pd.query as well, it’s essential to be aware of its capability to use “@” to reference variables in the environment. This feature enhances its flexibility and makes it even more convenient to apply in various data filtering scenarios.\nProblem statement: Our objective is to identify phones that meet three specific criteria: being priced below the average market price, having more processor cores than the average, and possessing a battery capacity greater than the average.\n\n\nCode\naverage_price=df.price.mean()\naverage_cores=df.num_cores.mean()\naverage_battery=df.battery_capacity.mean()\n\n(df\n .query(\"(price &lt;= @average_price) and (num_cores &gt;= @average_cores) and (battery_capacity &gt;= @average_battery)\")\n .iloc[:,:3]\n .sort_values(by='price')\n .head()\n)\n\n\n\n\n\n\n\n\n\nbrand_name\nmodel\nprice\n\n\n\n\n498\nrealme\nRealme C30\n5299\n\n\n179\nitel\nitel Vision 3 (2GB RAM + 32GB)\n5785\n\n\n202\nmicromax\nMicromax IN 2C\n5999\n\n\n729\ntecno\nTecno Spark Go 2022\n6249\n\n\n499\nrealme\nRealme C30 (3GB RAM + 32GB)\n6299\n\n\n\n\n\n\n\n\n\nTip #9: Gain more insights by using style.background_gradient\nBeing a visual creature, I often struggle to comprehend data quickly just by examining the raw table alone. Fortunately, with the help of style.background_gradient, similar to the heatmap function in the seaborn library, we can represent cells, in terms of color gradient, based on their values. This enables us to identify trends and patterns in our data swiftly, making data analysis more intuitive and insightful.\nProblem statement: Our goal is to identify the overall trends related to key descriptors found among the Top 10 smartphone brands, aiming to determine which brand offers the most value for our money.\n\n\nCode\n(df\n .query(\"brand_name.isin(@top10_brand_names_ordered)\") # filter rows based on top10 brands\n .groupby(['brand_name',])\n [['avg_rating', 'processor_speed', 'ram_capacity', \n   'screen_size', 'battery_capacity', 'price']]\n .mean()\n .sort_values(by='avg_rating')\n .transpose()\n .rename(columns=str.capitalize) # capitalize brand names\n .style\n .set_caption(\"Key descriptors of the Top 10 smartphone brands\")\n .format(precision=1)\n .background_gradient(cmap = 'vlag', axis = 1)\n .set_table_styles([\n    {'selector': 'td', 'props': 'text-align: center;'},\n     {'selector': 'caption','props': 'font-size:1.5em; font-weight:bold;'}\n     ,]\n )\n)\n\n\n\n\n\nKey descriptors of the Top 10 smartphone brands\n\n\nbrand_name\nTecno\nRealme\nApple\nVivo\nPoco\nSamsung\nOppo\nXiaomi\nMotorola\nOneplus\n\n\n\n\navg_rating\n7.4\n7.6\n7.7\n7.7\n7.9\n7.9\n7.9\n7.9\n8.0\n8.2\n\n\nprocessor_speed\n2.1\n2.3\n3.1\n2.4\n2.5\n2.4\n2.5\n2.4\n2.5\n2.7\n\n\nram_capacity\n5.4\n5.7\n5.3\n6.7\n6.1\n6.5\n7.5\n6.4\n6.1\n8.2\n\n\nscreen_size\n6.7\n6.5\n6.1\n6.5\n6.6\n6.6\n6.6\n6.6\n6.6\n6.6\n\n\nbattery_capacity\n5333.9\n4903.1\n3527.2\n4703.7\n5009.4\n4917.4\n4667.2\n4957.6\n4863.1\n4759.5\n\n\nprice\n14545.4\n17461.4\n95966.5\n26782.4\n18479.2\n36843.0\n29650.0\n27961.1\n24099.9\n35858.6\n\n\n\n\n\n\n\nTip #10: Use pd.pipe to include any functions in our chain\nOne of the most valuable lessons I learned from Effective Pandas is the importance of arranging my code in a chain. Although it may feel somewhat restrictive at first, once you overcome the initial hurdles, you’ll realize that your code becomes more readable and easier to understand. The need to invent unique names for temporary variables is completely eliminated, making coding a much happier experience.\nIn the chaining world, you often find yourself wanting to use various functions that are not explicitly designed for chaining. However, there’s good news! You can still achieve this. The pd.pipe function comes to the rescue, allowing you to use any function as long as it returns a Series or DataFrame. It’s a flexible solution that empowers you to seamlessly integrate different functions into your chaining workflow, making your data manipulation more efficient and enjoyable.\nProblem statement: We aim to visualize the impact of RAM capacity on user satisfaction. To achieve this, we will utilize the sns.lmplot function, which plots the data and corresponding regression models for the Top 5 phone brands.\n\n\nCode\ntop5_brand_names_ordered = df.brand_name.value_counts().head().index\n\nwith sns.axes_style(\"darkgrid\"):\n    g = (df\n         .query(\"brand_name.isin(@top5_brand_names_ordered)\") # filter rows based on top10 brands\n         [['brand_name', 'avg_rating', 'ram_capacity']]\n         .assign(brand_name=lambda df: df.brand_name.str.capitalize())\n         .rename(columns={'brand_name':'Brand name'})\n         .pipe(lambda df: sns.lmplot(data=df,\n                                     x='ram_capacity',\n                                     y='avg_rating',\n                                     hue='Brand name',\n                                     # height=4,\n                                     # aspect=1.2\n                                    )\n              )\n        )\n\n    g.set(title='Customer satisfaction correlates with RAM capacity', \n          xlabel='RAM capacity',\n          ylabel='User rating'\n         )\nplt.tight_layout()\n\n\n\n\n\n\n\nTip #10 + 1: Use the “margin” parameter of pd.crosstab to easily calculate row/column subtotals\nDespite primarily using the pandas groupby function for data aggregation, the pd.crosstab function has an enticing feature: the margin parameter. This option enables us to effortlessly calculate subtotals across rows and columns. Moreover, by normalizing our data, we can gain even more intuition about the questions we want to answer.\nProblem statement: Our objective is to evaluate how RAM capacity impacts user satisfaction across the Top 5 brands. Additionally, we will normalize our data to compare values comprehensively across the entire dataset.\n\n\nCode\n(df\n .query(\"brand_name.isin(@top5_brand_names_ordered)\") # filter rows based on top10 brands\n .assign(brand_name=lambda df: df.brand_name.str.capitalize())\n .pipe(lambda df: pd.crosstab(index=df['ram_capacity'],\n                              columns=df['brand_name'],\n                              values=df['avg_rating'],\n                              aggfunc='mean',\n                              margins=True,\n                              normalize='all'\n                             )\n      )\n .mul(100)\n .round(1)\n)\n\n\n\n\n\n\n\n\nbrand_name\nOppo\nRealme\nSamsung\nVivo\nXiaomi\nAll\n\n\nram_capacity\n\n\n\n\n\n\n\n\n\n\n2\n0.0\n2.7\n2.8\n2.7\n2.7\n11.5\n\n\n3\n2.9\n2.8\n2.9\n2.9\n2.8\n12.1\n\n\n4\n3.1\n3.2\n3.2\n3.2\n3.2\n13.5\n\n\n6\n3.3\n3.5\n3.5\n3.4\n3.5\n14.8\n\n\n8\n3.7\n3.7\n3.8\n3.7\n3.8\n15.7\n\n\n12\n3.8\n3.9\n3.9\n3.8\n3.9\n16.3\n\n\n16\n3.8\n0.0\n0.0\n0.0\n0.0\n16.2\n\n\nAll\n20.2\n19.6\n20.2\n19.8\n20.2\n100.0\n\n\n\n\n\n\n\nI hope this article has convinced you to pick up Matt Harrison’s Effective Pandas! There are plenty more exciting ideas in the book beyond the Top 10 I’ve shared here (I didn’t even get into the fascinating time series part!). I hope you found these insights helpful and inspiring.\nHappy coding 🐼💻🚀"
  },
  {
    "objectID": "projects/Predicting Belgian Real Estate Prices_ Part 2_Basic_clean_up_after_scraping/NB_2_ACs_Basic_clean_up_after_scraping.html",
    "href": "projects/Predicting Belgian Real Estate Prices_ Part 2_Basic_clean_up_after_scraping/NB_2_ACs_Basic_clean_up_after_scraping.html",
    "title": "Predicting Belgian Real Estate Prices: Part 2: Data Cleanup After Web Scraping",
    "section": "",
    "text": "Photo by Stephen Phillips - Hostreviews.co.uk on UnSplash\nIn part 1, we provided a brief introduction to the project’s purpose. Now, in part 2, we will dive deeper into the data processing steps required after scraping. We will discuss the handling of numerical data, categorical variables, and boolean values. Additionally, we’ll assess the data quality by examining the error log generated by the Immowebscraper class. Let’s get to it!"
  },
  {
    "objectID": "projects/Predicting Belgian Real Estate Prices_ Part 2_Basic_clean_up_after_scraping/NB_2_ACs_Basic_clean_up_after_scraping.html#reading-in-and-inspecting-the-log-file",
    "href": "projects/Predicting Belgian Real Estate Prices_ Part 2_Basic_clean_up_after_scraping/NB_2_ACs_Basic_clean_up_after_scraping.html#reading-in-and-inspecting-the-log-file",
    "title": "Predicting Belgian Real Estate Prices: Part 2: Data Cleanup After Web Scraping",
    "section": "Reading in and inspecting the log file",
    "text": "Reading in and inspecting the log file\nAnalyzing the error log file, we’ve identified a total of 3,515 errors encountered during the web scraping process on the Immoweb website. Let’s delve into these errors to pinpoint the most common issues and address them accordingly.\n\n\nCode\nerror_log = pd.read_table(\n    utils.Configuration.RAW_DATA_PATH.joinpath(\"make_dataset_error_for_NB2.log\"),\n    header=None,\n).rename(columns={0: \"error\"})\n\nerror_log\n\n\n\n\n\n\n\n\n\nerror\n\n\n\n\n0\n2023-09-27 14:02:11 - ERROR - No tables found ...\n\n\n1\n2023-09-27 14:02:17 - ERROR - No tables found ...\n\n\n2\n2023-09-27 14:02:22 - ERROR - No tables found ...\n\n\n3\n2023-09-27 14:02:25 - ERROR - No tables found ...\n\n\n4\n2023-09-27 14:02:34 - ERROR - No tables found ...\n\n\n...\n...\n\n\n3510\n2023-09-27 16:43:45 - ERROR - Duplicate labels...\n\n\n3511\n2023-09-27 16:43:57 - ERROR - No tables found ...\n\n\n3512\n2023-09-27 16:43:57 - ERROR - Duplicate labels...\n\n\n3513\n2023-09-27 16:43:57 - ERROR - Duplicate labels...\n\n\n3514\n2023-09-27 16:44:03 - ERROR - No tables found ...\n\n\n\n\n3515 rows × 1 columns\n\n\n\n\nMost common errors from log file\nIt’s clear that a significant majority of the errors, accounting for 1,848 cases, result from the absence of tables on the pages. These errors are primarily found on listing ads and index pages. To address this issue, we’ve introduced an if clause into our method extract_ads_from_given_page, which can be found in the make_dataset.py module. The clause, if \"immoweb.be\" in item and \"https://www.immoweb.be/en/search\" not in item, enables us to filter out undesired pages that don’t contain relevant table information for our ads. This not only helps mitigate errors but also speeds up the dataset collection process by reducing the number of pages we scrape.\nAnother category of errors, totaling 1,460 cases, is related to the presence of duplicate labels during processing. We may need to investigate this issue further at a later stage to ensure data quality and accuracy.\nA smaller proportion of errors is linked to the “Empty data” message, primarily related to ads. Finally, the remaining errors encompass errorrs related to data type conversion. We can consider either leaving these columns as is, since the error is not that frequent, or removing these features altogether.\n\n\nCode\n(\n    error_log.error.str.split(\"-\", expand=True)[4]\n    .str.rsplit(\" \", n=1, expand=True)[0]\n    .value_counts()\n)\n\n\n0\n No tables found while processing                                                                                                        1848\n Duplicate labels found while processing                                                                                                 1460\n Empty data while processing                                                                                                               18\n An error occurred on page 327: (\"Expected bytes, got a 'int' object\", 'Conversion failed for column Number of frontages with type          1\n An error occurred on page 255: (\"Expected bytes, got a 'int' object\", 'Conversion failed for column Number of frontages with type          1\n                                                                                                                                         ... \n An error occurred on page 173: (\"Expected bytes, got a 'int' object\", 'Conversion failed for column Number of frontages with type          1\n An error occurred on page 174: (\"Expected bytes, got a 'int' object\", 'Conversion failed for column Number of frontages with type          1\n An error occurred on page 176: (\"Expected bytes, got a 'int' object\", 'Conversion failed for column Number of frontages with type          1\n An error occurred on page 177: (\"Expected bytes, got a 'int' object\", 'Conversion failed for column Number of frontages with type          1\n An error occurred on page 331: (\"Expected bytes, got a 'int' object\", 'Conversion failed for column Outdoor parking spaces with type       1\nName: count, Length: 192, dtype: int64\n\n\nWe’ve identified five columns responsible for the last set of data type conversion problems (See below):\n\nConstruction year\nNumber of frontages\nOutdoor parking spaces\nCovered parking spaces\nBedrooms\n\n\n\nCode\n(\n    error_log.error.str.split(\"-\", expand=True)[4]\n    .str.rsplit(\" \", n=1, expand=True)[0]\n    .value_counts()[3:]\n    .to_frame()\n    .reset_index()\n    .rename(columns={0: \"error_type\"})\n    .assign(\n        error_type=lambda df: df.error_type.str.split(\",\", expand=True)[2].str.split(\n            \" \", n=5, expand=True\n        )[5]\n    )\n    .error_type.value_counts()\n)\n\n\nerror_type\nConstruction year with type         83\nNumber of frontages with type       65\nOutdoor parking spaces with type     7\nCovered parking spaces with type     7\nBedrooms with type                   1\nName: count, dtype: int64\n\n\n\n\nUnique URLs from the error logs\nUpon conducting a more comprehensive analysis of the URLs extracted from error messages, a noteworthy observation comes to light: we’ve encountered only 433 unique URLs. This suggests that the 3,515 errors are stemming from a relatively restricted set of web addresses.\nNow, here’s a question that arises: the website implies the presence of 10,000 ads on the page. However, given our successful extraction of only 3,906 ads, along with the 433 URLs associated with errors, there is a substantial disparity evident.\nIf you have any insights or hypotheses regarding this difference, I’d be eager to hear your thoughts and discuss potential reasons for this variation.\n\n\nCode\n(\n    error_log.error.str.split(\"-\", expand=True)[4]\n    .str.rsplit(\" \", n=1, expand=True)[1]\n    .unique()\n    .shape\n)\n\n\n(433,)"
  },
  {
    "objectID": "projects/Predicting Belgian Real Estate Prices_ Part 2_Basic_clean_up_after_scraping/NB_2_ACs_Basic_clean_up_after_scraping.html#inspecting-the-data-itself",
    "href": "projects/Predicting Belgian Real Estate Prices_ Part 2_Basic_clean_up_after_scraping/NB_2_ACs_Basic_clean_up_after_scraping.html#inspecting-the-data-itself",
    "title": "Predicting Belgian Real Estate Prices: Part 2: Data Cleanup After Web Scraping",
    "section": "Inspecting the data itself",
    "text": "Inspecting the data itself\nAfter eliminating rows where all values were missing and filtering for rows with non-missing prices, we’ve successfully refined our dataset to include 3,660 ads.\nIn our subsequent analysis, we focus on the features with the lowest percentage of missing da, just like we did in Part 1ta. Notably, “day of retrieval” and “price” are complete, with all values present. However, it’s important to recognize that roughly one-third of the data related to “dining_room” and “office” is missing, highlighting the need for improving data completeness in these specific attributet.s.\n\n\nCode\n(\n    finer_pre_cleaned.dropna(axis=0, how=\"all\")\n    .query(\"price.notna()\")\n    .notna()\n    .sum()\n    .sort_values()\n    .div(3660)\n    .mul(100)\n    .round(1)\n)\n\n\ndining_room                                        29.3\noffice                                             29.5\nplanning_permission_obtained                       32.1\ntv_cable                                           34.0\nproceedings_for_breach_of_planning_regulations     36.3\nsubdivision_permit                                 38.2\nyearly_theoretical_total_energy_consumption        39.5\nwidth_of_the_lot_on_the_street                     42.4\nco2_emission                                       43.0\nconnection_to_sewer_network                        44.7\npossible_priority_purchase_right                   45.5\nstreet_frontage_width                              46.5\nbasement                                           46.6\nas_built_plan                                      47.0\nlatest_land_use_designation                        47.1\ngarden_surface                                     47.4\noutdoor_parking_spaces                             48.0\nfurnished                                          48.4\nsurroundings_type                                  50.8\nflood_zone_type                                    54.4\nbedroom_3_surface                                  54.8\ncovered_parking_spaces                             54.9\nkitchen_surface                                    58.2\navailable_as_of                                    58.3\nliving_room_surface                                64.7\nbedroom_2_surface                                  66.6\ngas_water__electricity                             67.0\nbedroom_1_surface                                  68.2\nconstruction_year                                  71.0\ncadastral_income                                   78.3\nkitchen_type                                       83.4\ndouble_glazing                                     84.6\nwebsite                                            84.6\nheating_type                                       87.2\nexternal_reference                                 90.3\ntoilets                                            90.8\nnumber_of_frontages                                94.6\nbathrooms                                          94.9\nhouse_number                                       94.9\nprimary_energy_consumption                         96.0\nbuilding_condition                                 96.0\nsurface_of_the_plot                                96.6\nliving_area                                        98.2\ntenement_building                                  99.1\nzip                                                99.5\ncity                                               99.5\nbedrooms                                           99.5\nstreet                                             99.5\nreference_number_of_the_epc_report                 99.9\nenergy_class                                       99.9\nprice                                             100.0\nad_url                                            100.0\nday_of_retrieval                                  100.0\ndtype: float64\n\n\nOur filter_out_missing_indexes function proves to be quite valuable in the post-processing of our scraped data. This function is located at the end of our pre-process chain saving our data to the INTERIM_DATA_PATH folder after we’ve completed pre-processing and removed missing values.\n\n\nCode\ndef filter_out_missing_indexes(\n    df: pd.DataFrame,\n    filepath: Path = utils.Configuration.INTERIM_DATA_PATH.joinpath(\n        f\"{str(pd.Timestamp.now())[:10]}_Processed_dataset.parquet.gzip\"\n    ),\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Filter out rows with missing values in a DataFrame and save the processed dataset.\n\n    This function filters out rows with all missing values (NaN) and retains only rows\n    with non-missing values in the 'price' column. The resulting DataFrame is then saved\n    in Parquet format with gzip compression.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame.\n        filepath (Path, optional): The path to save the processed dataset in Parquet format.\n            Defaults to a timestamp-based filepath in the interim data directory.\n\n    Returns:\n        pd.DataFrame: The filtered DataFrame with missing rows removed.\n\n    Example:\n        To filter out missing rows and save the processed dataset:\n        &gt;&gt;&gt; data = pd.read_csv(\"raw_data.csv\")\n        &gt;&gt;&gt; filtered_data = filter_out_missing_indexes(data)\n        &gt;&gt;&gt; print(filtered_data.head())\n\n    Notes:\n        - Rows with missing values in any column other than 'price' are removed.\n        - The processed dataset is saved with gzip compression to conserve disk space.\n    \"\"\"\n    processed_df = df.dropna(axis=0, how=\"all\").query(\"price.notna()\")\n    processed_df.to_parquet(filepath, compression=\"gzip\", index=False)\n    return processed_df\n\n\nIt appears that we’ve successfully completed the data transformation phase of our scraped dataset. With the implementation of functions like filter_out_missing_indexes, alongside pre_process_dataframe and separate_address, we’ve assembled the essential tools required for preparing our dataset for the machine learning pipeline.\nIn Part 3, we’ll provide a fundamental overview and characterization of the cleaned scraped data. We’ll assess feature cardinality, examine distributions, and explore correlations among variables. I look forward to delving into these insights with you in the next installment. See you there!"
  },
  {
    "objectID": "projects/Predicting Belgian Real Estate Prices_ Part 4_Building_a_baseline_model/NB_4_ACs_Building_a_baseline_model.html",
    "href": "projects/Predicting Belgian Real Estate Prices_ Part 4_Building_a_baseline_model/NB_4_ACs_Building_a_baseline_model.html",
    "title": "Predicting Belgian Real Estate Prices: Part 4: Building a Baseline Model",
    "section": "",
    "text": "Photo by Stephen Phillips - Hostreviews.co.uk on UnSplash\nIn the preceding Part 3, our emphasis was on establishing a fundamental understanding of our data through characterizing the cleaned scraped dataset. We delved into feature cardinality, distributions, and potential correlations with our target variable—property price. Moving on to Part 4, our agenda includes examining essential sample pre-processing steps before modeling. We will craft the necessary pipeline, assess multiple algorithms, and ultimately select a suitable baseline model. Let’s get started!"
  },
  {
    "objectID": "projects/Predicting Belgian Real Estate Prices_ Part 4_Building_a_baseline_model/NB_4_ACs_Building_a_baseline_model.html#read-in-the-processed-file",
    "href": "projects/Predicting Belgian Real Estate Prices_ Part 4_Building_a_baseline_model/NB_4_ACs_Building_a_baseline_model.html#read-in-the-processed-file",
    "title": "Predicting Belgian Real Estate Prices: Part 4: Building a Baseline Model",
    "section": "Read in the processed file",
    "text": "Read in the processed file\nAfter importing our preprocessed dataframe, a crucial step in our data refinement process involves the culling of certain columns. Specifically, we intend to exclude columns with labels such as “external_reference,” “ad_url,” “day_of_retrieval,” “website,” “reference_number_of_the_epc_report,” and “housenumber.” Our rationale behind this action is to enhance the efficiency of our model by eliminating potentially non-contributory features.\n\n\nCode\nutils.seed_everything(utils.Configuration.seed)\n\ndf = (\n    pd.read_parquet(\n        utils.Configuration.INTERIM_DATA_PATH.joinpath(\n            \"2023-10-01_Processed_dataset_for_NB_use.parquet.gzip\"\n        )\n    )\n    .sample(frac=1, random_state=utils.Configuration.seed)\n    .reset_index(drop=True)\n    .assign(price=lambda df: np.log10(df.price))\n    .drop(\n        columns=[\n            \"external_reference\",\n            \"ad_url\",\n            \"day_of_retrieval\",\n            \"website\",\n            \"reference_number_of_the_epc_report\",\n            \"housenumber\",\n        ]\n    )\n)\n\nprint(f\"Shape of dataframe after read-in a pre-processing: {df.shape}\")\nX = df.drop(columns=utils.Configuration.target_col)\ny = df[utils.Configuration.target_col]\n\nprint(f\"Shape of X: {X.shape}\")\nprint(f\"Shape of y: {y.shape}\")\n\n\nShape of dataframe after read-in a pre-processing: (3660, 50)\nShape of X: (3660, 49)\nShape of y: (3660,)"
  },
  {
    "objectID": "projects/Predicting Belgian Real Estate Prices_ Part 4_Building_a_baseline_model/NB_4_ACs_Building_a_baseline_model.html#train-test-split",
    "href": "projects/Predicting Belgian Real Estate Prices_ Part 4_Building_a_baseline_model/NB_4_ACs_Building_a_baseline_model.html#train-test-split",
    "title": "Predicting Belgian Real Estate Prices: Part 4: Building a Baseline Model",
    "section": "Train-test split",
    "text": "Train-test split\nThe subsequent phase in our data preparation involves the partitioning of our dataset into training and testing subsets. To accomplish this, we’ll leverage the model_selection.train_test_split method. This step ensures that we have distinct sets for model training and evaluation, a fundamental practice in machine learning.\n\n\nCode\nX_train, X_test, y_train, y_test = model_selection.train_test_split(\n    X, y, test_size=0.2, random_state=utils.Configuration.seed\n)\n\nprint(f\"Shape of X-train: {X_train.shape}\")\nprint(f\"Shape of X-test: {X_test.shape}\")\n\n\nShape of X-train: (2928, 49)\nShape of X-test: (732, 49)"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Predicting Belgian Real Estate Prices: Part 5: Initial feature selection\n\n\n\n\n\n\n\nPredicting Belgian Real Estate Prices\n\n\n\n\n\n\n\n\n\n\n\nNov 7, 2023\n\n\nAdam Cseresznye\n\n\n\n\n\n\n  \n\n\n\n\nPredicting Belgian Real Estate Prices: Part 4: Building a Baseline Model\n\n\n\n\n\n\n\nPredicting Belgian Real Estate Prices\n\n\n\n\n\n\n\n\n\n\n\nNov 6, 2023\n\n\nAdam Cseresznye\n\n\n\n\n\n\n  \n\n\n\n\nPredicting Belgian Real Estate Prices: Part 3: Characterizing Belgian Real Estate Data Post-Scraping\n\n\n\n\n\n\n\nPredicting Belgian Real Estate Prices\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2023\n\n\nAdam Cseresznye\n\n\n\n\n\n\n  \n\n\n\n\nPredicting Belgian Real Estate Prices: Part 2: Data Cleanup After Web Scraping\n\n\n\n\n\n\n\nPredicting Belgian Real Estate Prices\n\n\n\n\n\n\n\n\n\n\n\nNov 4, 2023\n\n\nAdam Cseresznye\n\n\n\n\n\n\n  \n\n\n\n\nPredicting Belgian Real Estate Prices: Part 1: Feature Selection for Web Scraping\n\n\n\n\n\n\n\nPredicting Belgian Real Estate Prices\n\n\n\n\n\n\n\n\n\n\n\nNov 3, 2023\n\n\nAdam Cseresznye\n\n\n\n\n\n\nNo matching items"
  }
]