[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Coming soon!\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/polars speed/polars_speed.html",
    "href": "posts/polars speed/polars_speed.html",
    "title": "The Ultimate Speed Test: Pandas vs Polars",
    "section": "",
    "text": "After I shared my initial article about ‚ÄúGetting Started with Polars‚Äù, I couldn‚Äôt help but thinking about a potential speed comparison between Polars and Pandas given the excitement about the new release of Pandas 2.0, which promised to bring blazing-fast performance to the table. Pandas 2.0 was announced to come packed with cool features, including the addition of Apache Arrow (pyarrow) as its backing memory format. The big perk of Apache Arrow is that it makes operations speedier and more memory-friendly. Naturally, this got me wondering: how does Pandas 2.0 measure up against Polars? Let‚Äôs dive in and find out!"
  },
  {
    "objectID": "posts/polars speed/polars_speed.html#csv",
    "href": "posts/polars speed/polars_speed.html#csv",
    "title": "The Ultimate Speed Test: Pandas vs Polars",
    "section": "CSV",
    "text": "CSV\nReading CSV files from disk is a task that data scientists often find themselves doing. Now, let‚Äôs see how these two libraries compare for this particular job. To maximize the blazing-fast data handling capabilities of PyArrow, we‚Äôll equip Pandas with the engine=\"pyarrow\" and dtype_backend=\"pyarrow\" arguments. Let‚Äôs see how these choices shape the performance!\n\n\nCode\ntest_dict = {\n    \"Pandas\": lambda: pd.read_csv(\n        \"sample.csv\", engine=\"pyarrow\", dtype_backend=\"pyarrow\"\n    ),\n    \"Polars\": lambda: pl.read_csv(\"sample.csv\"),\n}\nread_csv = run_test(test_dict, \"Read csv\")\n\n\n\n\n                                                \nFigure¬†1: Reading in data From a CSV File\n\n\n\nFor the sake of comparison, we‚Äôll also demonstrate the timeit function invoked using Jupyter cell magic. You‚Äôll notice that the numbers generated this way are quite closely aligned with ours.\n\n\nCode\n%%timeit\npd.read_csv(\"sample.csv\", engine=\"pyarrow\", dtype_backend=\"pyarrow\")\n\n\n8.37 ms ¬± 207 ¬µs per loop (mean ¬± std. dev. of 7 runs, 100 loops each)\n\n\n\n\nCode\n%%timeit\npl.read_csv(\"sample.csv\")\n\n\n2.49 ms ¬± 251 ¬µs per loop (mean ¬± std. dev. of 7 runs, 100 loops each)"
  },
  {
    "objectID": "posts/polars speed/polars_speed.html#parquet",
    "href": "posts/polars speed/polars_speed.html#parquet",
    "title": "The Ultimate Speed Test: Pandas vs Polars",
    "section": "parquet",
    "text": "parquet\nNow, let‚Äôs read the data in Parquet format.\n\n\nCode\ntest_dict = {\n    \"Pandas\": lambda: pd.read_parquet(\n        \"sample.parquet\", engine=\"pyarrow\", dtype_backend=\"pyarrow\"\n    ),\n    \"Polars\": lambda: pl.read_parquet(\"sample.parquet\"),\n}\nread_parquet = run_test(test_dict, \"Read parquet\")\n\n\n\n\n                                                \nFigure¬†2: Reading in data From a parquet File\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nPolars unquestionably wins this round, boasting a speed advantage of 2 to 4 times over Pandas."
  },
  {
    "objectID": "posts/polars speed/polars_speed.html#files-awaiting-in-memory-reading",
    "href": "posts/polars speed/polars_speed.html#files-awaiting-in-memory-reading",
    "title": "The Ultimate Speed Test: Pandas vs Polars",
    "section": "Files awaiting in-memory reading",
    "text": "Files awaiting in-memory reading\nA clever approach to conserve memory and enhance speed involves reading only the columns essential for operations. Consider a scenario where we‚Äôre interested in displaying just the names from this dataset. The big question now: how do these libraries measure up in terms of speed? Let‚Äôs find out!\n\n\nCode\ntest_dict = {\n    \"Pandas\": lambda: pd.read_csv(\n        \"sample.csv\", engine=\"pyarrow\", dtype_backend=\"pyarrow\", usecols=[\"name\"]\n    ),\n    \"Polars\": lambda: pl.scan_csv(\"sample.csv\").select(pl.col(\"name\")).collect(),\n}\nselect_col_not_in_memory = run_test(test_dict, \"Select column (not in memory)\")\n\n\n\n\n                                                \nFigure¬†3: Selecting Columns from a File Not Yet in Memory"
  },
  {
    "objectID": "posts/polars speed/polars_speed.html#file-is-in-memory",
    "href": "posts/polars speed/polars_speed.html#file-is-in-memory",
    "title": "The Ultimate Speed Test: Pandas vs Polars",
    "section": "File is in memory",
    "text": "File is in memory\nAs anticipated, Polars continues to showcase its swiftness. It‚Äôs worth highlighting the usage of the lazy and collect methods in Polars. These nifty tools grant us access to the library‚Äôs clever query optimization techniques, which play a pivotal role in significantly enhancing performance. OK, one step further: suppose our files are already loaded into memory. Would there still be a distinction in performance under this circumstance?\n\n\nCode\ndf_pandas = pd.read_csv(\"sample.csv\", engine=\"pyarrow\", dtype_backend=\"pyarrow\")\ndf_polars = pl.read_csv(\"sample.csv\")\n\n\n\n\nCode\ntest_dict = {\n    \"Pandas\": lambda: df_pandas.loc[:, \"name\"],\n    \"Polars\": lambda: df_polars.lazy().select(pl.col(\"name\")).collect(),\n}\nselect_col_in_memory = run_test(test_dict, \"Select column\")\n\n\n\n\n                                                \nFigure¬†4: Selecting Columns from a File Already in Memory\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhile Polars showed a significant 3-4 times speed advantage for tasks involving pre-read files, both libraries perform similarly when the files are already in memory."
  },
  {
    "objectID": "posts/polars speed/polars_speed.html#based-on-one-condition",
    "href": "posts/polars speed/polars_speed.html#based-on-one-condition",
    "title": "The Ultimate Speed Test: Pandas vs Polars",
    "section": "Based on one condition",
    "text": "Based on one condition\nFor our simple scenario, we‚Äôll be narrowing down our focus to filter data based on individuals with the name ‚ÄúDavid‚Äù.\n\n\nCode\ntest_dict = {\n    \"Pandas\": lambda: (df_pandas.query(\"name=='David'\")),\n    \"Polars\": lambda: (df_polars.lazy().filter((pl.col(\"name\") == \"David\")).collect()),\n}\nfilter_row_one_condition = run_test(test_dict, \"Filter (simple)\")\n\n\n\n\n                                                \nFigure¬†5: Filtering Rows Based on One Condition"
  },
  {
    "objectID": "posts/polars speed/polars_speed.html#based-on-multiple-conditions",
    "href": "posts/polars speed/polars_speed.html#based-on-multiple-conditions",
    "title": "The Ultimate Speed Test: Pandas vs Polars",
    "section": "Based on multiple conditions",
    "text": "Based on multiple conditions\nNow, for a more intricate challenge, we‚Äôre going to dive into querying the data to extract individuals who meet specific criteria: those named David, born after 1980, residing in a city other than London, married, and with three children.\n\n\nCode\ntest_dict = {\n    \"Pandas\": lambda: (\n        df_pandas.query(\n            \"name=='David' and born&gt;1980 and city != 'London' or is_married == True and children &gt;= 3\"\n        )\n    ),\n    \"Polars\": lambda: (\n        df_polars.lazy()\n        .filter(\n            (pl.col(\"name\") == \"David\")\n            & (pl.col(\"born\") &gt; 1980)\n            & (pl.col(\"city\") != \"London\")\n            | (pl.col(\"is_married\") == True) & (pl.col(\"children\") &gt;= 3)\n        )\n        .collect()\n    ),\n}\nfilter_row_multiple_condition = run_test(test_dict, \"Filter (complex)\")\n\n\n\n\n                                                \nFigure¬†6: Filtering Rows Based on Multiple Condition\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nBoth libraries tackled this challenge quite well, yet Pandas struggled to keep pace with Polars. It‚Äôs intriguing to observe that while Pandas required nearly twice the time for the more intricate task, Polars managed to complete it in almost the same amount of time. Parallelization in action."
  },
  {
    "objectID": "posts/polars speed/polars_speed.html#single-operation",
    "href": "posts/polars speed/polars_speed.html#single-operation",
    "title": "The Ultimate Speed Test: Pandas vs Polars",
    "section": "Single operation",
    "text": "Single operation\nAs a single operation, we‚Äôll simply calculate the century in which these individuals were born.\n\n\nCode\ntest_dict = {\n    \"Pandas\": lambda: (df_pandas.assign(born=lambda df: df.born.div(100).round())),\n    \"Polars\": lambda: (\n        df_polars.lazy().with_columns((pl.col(\"born\") / 100).round()).collect()\n    ),\n}\noperate_one_column = run_test(test_dict, \"Operate (one column)\")\n\n\n\n\n                                                \nFigure¬†7: Performing a Singme Operation on a Column"
  },
  {
    "objectID": "posts/polars speed/polars_speed.html#multiple-operations",
    "href": "posts/polars speed/polars_speed.html#multiple-operations",
    "title": "The Ultimate Speed Test: Pandas vs Polars",
    "section": "Multiple operations",
    "text": "Multiple operations\nLet‚Äôs also explore what happens when performing multiple operations on the columns. We‚Äôll mix things up with some string operations, mapping, and math calculations to see how these libraries handle it!\n\n\nCode\ntest_dict = {\n    \"Pandas\": lambda: (\n        df_pandas.assign(\n            born=lambda df: df.born.div(100).round(),\n            name=lambda df: df.name.str.lower(),\n            city=lambda df: df.city.str.upper(),\n            zip_code=lambda df: df.zip_code.mul(2),\n            income=lambda df: df.income.div(10).astype(\"str\"),\n            is_married=lambda df: df.is_married.map({False: 0, True: 1}),\n            children=lambda df: df.children.astype(\"bool\"),\n            car=lambda df: df.car.str[0],\n        )\n    ),\n    \"Polars\": lambda: (\n        df_polars.lazy()\n        .with_columns(\n            [\n                (pl.col(\"born\") / 100).round(),\n                pl.col(\"name\").str.to_lowercase(),\n                pl.col(\"city\").str.to_uppercase(),\n                pl.col(\"zip_code\") * 2,\n                (pl.col(\"income\") / 10).cast(pl.Utf8),\n                pl.col(\"is_married\").map_dict({False: 0, True: 1}),\n                pl.col(\"children\").cast(pl.Boolean),\n                pl.col(\"car\").str.slice(0, length=1),\n            ]\n        )\n        .collect()\n    ),\n}\noperate_multiple_column = run_test(test_dict, \"Operate (more columns)\")\n\n\n\n\n                                                \nFigure¬†8: Perfrming a Multiple Operation on Columns\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nOnce again, Polars takes the lead. While both libraries required more time for the task involving multiple operations, Polars demonstrated superior scalability in this scenario."
  },
  {
    "objectID": "posts/polars speed/polars_speed.html#simple",
    "href": "posts/polars speed/polars_speed.html#simple",
    "title": "The Ultimate Speed Test: Pandas vs Polars",
    "section": "Simple",
    "text": "Simple\nTime to shift our attention to aggregation. First up, a simple task: let‚Äôs calculate the mean income based on names. Then, for a bit more complexity, we‚Äôll dive into computing statistics involving the income, children, and car columns. Things are about to get interesting!\n\n\nCode\ntest_dict = {\n    \"Pandas\": lambda: (df_pandas.groupby(\"name\").income.mean()),\n    \"Polars\": lambda: (df_polars.lazy().groupby(\"name\").mean().collect()),\n}\naggregate_simple = run_test(test_dict, \"Aggregate (simple)\")\n\n\n\n\n                                                \nFigure¬†10: Performing a simple aggregation"
  },
  {
    "objectID": "posts/polars speed/polars_speed.html#more-complex",
    "href": "posts/polars speed/polars_speed.html#more-complex",
    "title": "The Ultimate Speed Test: Pandas vs Polars",
    "section": "More complex",
    "text": "More complex\n\n\nCode\ntest_dict = {\n    \"Pandas\": lambda: (\n        df_pandas.groupby([\"name\", \"car\", \"is_married\"]).agg(\n            born_min=(\"born\", min),\n            born_max=(\"born\", max),\n            income_mean=(\"income\", np.mean),\n            income_median=(\"income\", np.median),\n            children_mean=(\"children\", np.mean),\n            car_count=(\"car\", \"count\"),\n        )\n    ),\n    \"Polars\": lambda: (\n        df_polars.lazy()\n        .groupby([\"name\", \"car\", \"is_married\"])\n        .agg(\n            [\n                pl.col(\"born\").min().alias(\"born_min\"),\n                pl.col(\"born\").max().alias(\"born_max\"),\n                pl.col(\"income\").mean().alias(\"income_mean\"),\n                pl.col(\"income\").median().alias(\"income_median\"),\n                pl.col(\"children\").mean().alias(\"children_mean\"),\n                pl.col(\"car\").count().alias(\"car_count\"),\n            ]\n        )\n        .collect()\n    ),\n}\naggregate_complex = run_test(test_dict, \"Aggregate (complex)\")\n\n\n\n\n                                                \nFigure¬†11: Performing a complex aggregation\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhile Polars demonstrated impressive speed for the straightforward aggregation, the more complex task revealed striking differences between the two libraries. In this case, Polars surged ahead, boasting a remarkable 80-100 times faster performance compared to Pandas."
  },
  {
    "objectID": "posts/polars speed/polars_speed.html#using-pl.read_csv",
    "href": "posts/polars speed/polars_speed.html#using-pl.read_csv",
    "title": "The Ultimate Speed Test: Pandas vs Polars",
    "section": "Using pl.read_csv",
    "text": "Using pl.read_csv\n\n\nCode\ntest_dict = {\n    \"Pandas\": lambda: (\n        df_pandas.loc[:, lambda df: ~df.columns.isin([\"is_married\"])]\n        .query(\"name=='David' and born&gt;1980 and city != 'London' and children &gt;= 3\")\n        .assign(\n            born=lambda df: df.born.div(100).round(),\n            name=lambda df: df.name.str.lower(),\n            city=lambda df: df.city.str.upper(),\n            zip_code=lambda df: df.zip_code.mul(2),\n            income=lambda df: df.income.div(10),\n            children=lambda df: df.children.astype(\"bool\"),\n            car=lambda df: df.car.str[0],\n        )\n        .groupby(\n            [\n                \"name\",\n                \"car\",\n            ]\n        )\n        .agg(\n            born_min=(\"born\", min),\n            born_max=(\"born\", max),\n            income_mean=(\"income\", np.mean),\n            income_median=(\"income\", np.median),\n            children_mean=(\"children\", np.mean),\n            car_count=(\"car\", \"count\"),\n        )\n    ),\n    \"Polars\": lambda: (\n        df_polars.lazy()\n        .select(cs.all() - cs.ends_with(\"married\"))\n        .filter(\n            (pl.col(\"name\") == \"David\")\n            & (pl.col(\"born\") &gt; 1980)\n            & (pl.col(\"city\") != \"London\")\n            & (pl.col(\"children\") &gt;= 3)\n        )\n        .with_columns(\n            [\n                (pl.col(\"born\") / 100).round(),\n                pl.col(\"name\").str.to_lowercase(),\n                pl.col(\"city\").str.to_uppercase(),\n                pl.col(\"zip_code\") * 2,\n                (pl.col(\"income\") / 10),\n                pl.col(\"children\").cast(pl.Boolean),\n                pl.col(\"car\").str.slice(0, length=1),\n            ]\n        )\n        .groupby(\n            [\n                \"name\",\n                \"car\",\n            ]\n        )\n        .agg(\n            [\n                pl.col(\"born\").min().alias(\"born_min\"),\n                pl.col(\"born\").max().alias(\"born_max\"),\n                pl.col(\"income\").mean().alias(\"income_mean\"),\n                pl.col(\"income\").median().alias(\"income_median\"),\n                pl.col(\"children\").mean().alias(\"children_mean\"),\n                pl.col(\"car\").count().alias(\"car_count\"),\n            ]\n        )\n        .collect()\n    ),\n}\nwhole_workflow_read_csv = run_test(test_dict, \"Whole workflow\")\n\n\n\n\n                                                \nFigure¬†12: Performing a representative workflow"
  },
  {
    "objectID": "posts/polars speed/polars_speed.html#using-pl.scan_csv",
    "href": "posts/polars speed/polars_speed.html#using-pl.scan_csv",
    "title": "The Ultimate Speed Test: Pandas vs Polars",
    "section": "Using pl.scan_csv",
    "text": "Using pl.scan_csv\n\n\nCode\ntest_dict = {\n    \"Pandas\": lambda: (\n        df_pandas.loc[:, lambda df: ~df.columns.isin([\"is_married\"])]\n        .query(\"name=='David' and born&gt;1980 and city != 'London' and children &gt;= 3\")\n        .assign(\n            born=lambda df: df.born.div(100).round(),\n            name=lambda df: df.name.str.lower(),\n            city=lambda df: df.city.str.upper(),\n            zip_code=lambda df: df.zip_code.mul(2),\n            income=lambda df: df.income.div(10),\n            children=lambda df: df.children.astype(\"bool\"),\n            car=lambda df: df.car.str[0],\n        )\n        .groupby(\n            [\n                \"name\",\n                \"car\",\n            ]\n        )\n        .agg(\n            born_min=(\"born\", min),\n            born_max=(\"born\", max),\n            income_mean=(\"income\", np.mean),\n            income_median=(\"income\", np.median),\n            children_mean=(\"children\", np.mean),\n            car_count=(\"car\", \"count\"),\n        )\n    ),\n    \"Polars\": lambda: (\n        pl.scan_csv(\"sample.csv\")\n        .select(cs.all() - cs.ends_with(\"married\"))\n        .filter(\n            (pl.col(\"name\") == \"David\")\n            & (pl.col(\"born\") &gt; 1980)\n            & (pl.col(\"city\") != \"London\")\n            & (pl.col(\"children\") &gt;= 3)\n        )\n        .with_columns(\n            [\n                (pl.col(\"born\") / 100).round(),\n                pl.col(\"name\").str.to_lowercase(),\n                pl.col(\"city\").str.to_uppercase(),\n                pl.col(\"zip_code\") * 2,\n                (pl.col(\"income\") / 10),\n                pl.col(\"children\").cast(pl.Boolean),\n                pl.col(\"car\").str.slice(0, length=1),\n            ]\n        )\n        .groupby(\n            [\n                \"name\",\n                \"car\",\n            ]\n        )\n        .agg(\n            [\n                pl.col(\"born\").min().alias(\"born_min\"),\n                pl.col(\"born\").max().alias(\"born_max\"),\n                pl.col(\"income\").mean().alias(\"income_mean\"),\n                pl.col(\"income\").median().alias(\"income_median\"),\n                pl.col(\"children\").mean().alias(\"children_mean\"),\n                pl.col(\"car\").count().alias(\"car_count\"),\n            ]\n        )\n        .collect()\n    ),\n}\nwhole_workflow_scan_csv = run_test(test_dict, \"Whole workflow (scan_csv)\")\n\n\n\n\n                                                \nFigure¬†13: Performing a representative workflow (using pl_scan_csv for Polars)\n\n\n\nAs evident, the utilization of scan_csv increased the required time by about 3-4 times. However, even with this increase, Polars still manages to maintain a substantial advantage of around 5 times faster than the entire workflow executed using Pandas.\n\n\n\n\n\n\nNote\n\n\n\nWhen we consider the entirety of the data processing pipeline, irrespective of the file reading approach, Polars emerges as the victor. It consistently exhibits a speed advantage of 5 to 20 times compared to Pandas."
  },
  {
    "objectID": "posts/Lets-Plot/2023_08_05_How_to_use_the_Lets_Plot_library_by_JetBrains.html",
    "href": "posts/Lets-Plot/2023_08_05_How_to_use_the_Lets_Plot_library_by_JetBrains.html",
    "title": "How to use the Lets-Plot library by JetBrains",
    "section": "",
    "text": "When I embarked on my data science journey, due to my academics background I quickly gravitated towards the R programming language. Like many R novices, I began with Hadley Wickham‚Äôs R for Data Science book which introduced me to the wonderful ggplot2 library. As my interest in machine learning grew, I made the switch to Python. Nowadays, for most of my data plotting needs, I rely mainly on matplotlib or seaborn. Though I love these libraries, their multiple ways of accomplishing the same tasks can be a bit cumbersome and challenging to learn at first.\nThat‚Äôs why in this article, I‚Äôm excited to introduce you to the Lets-Plot library by JetBrains. It is the closest you can get to ggplot‚Äôs syntax while using Python. While some traditional Python users might find the syntax a bit unfamiliar initially, I‚Äôm here to make a case for this fantastic library and hopefully inspire you to incorporate it into your day-to-day data science activities.\nTo showcase (some of) the key features of Lets-Plot, we will be utilizing the penguins dataset üêß from Github.\nWithout further ado, let‚Äôs dive right in and discover the power and beauty of Lets-Plot! üêçüìä\n\n\nCode\n# Installation\n# pip install lets-plot \n\n\n\n\nCode\n# Import libraries\nimport numpy as np\nimport pandas as pd\nfrom lets_plot import *\nfrom lets_plot.mapping import as_discrete\nLetsPlot.setup_html()\n\n\n\n            \n            \n            \n\n\n\n\nCode\naddress='https://gist.githubusercontent.com/slopp/ce3b90b9168f2f921784de84fa445651/raw/4ecf3041f0ed4913e7c230758733948bc561f434/penguins.csv'\ndf=pd.read_csv(address)\ndf.head()\n\n\n\n\n\n\n\n\n\nrowid\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\n1\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\n2\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\n3\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\n4\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\n5\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n\n\n\n\n\n\nSyntax Similarities: Lets-Plot and ggplot2\nFor our first exercise, I thought it would be beneficial to replicate a basic plot inspired by Hadley‚Äôs book. When comparing my code here with the one presented by him, you‚Äôll notice that there is very little difference between the two. The syntax is nearly identical, making it a smooth transition from ggplot to Lets-Plot.\nNow, let‚Äôs take a closer look at the code. In the ggplot function, we define the DataFrame we‚Äôll be working with, and the aesthetic mappings are set at the global level. We assign the values for the x and y axes, as well as the color argument, which groups the data based on the categorical variable representing the three different penguin species: Adelie, Gentoo, and Chinstrap. This color parameter is quite similar to seaborn‚Äôs hue, making it easy for those familiar with seaborn to adapt to Lets-Plot seamlessly.\nAfter the ggplot() function sets the global aesthetic mappings, the geom_point() function comes into play and draws the points defined by the x and y parameters, effectively creating a scatter plot. These points represent the data points from the penguins dataset, with x and y coordinates corresponding to the specified variables.\nAdditionally, we enhance the plot by using geom_smooth(method=‚Äòlm‚Äô), which adds a smoothed conditional mean. The lm method stands for ‚Äòlinear model,‚Äô indicating that the smoothing is based on a linear regression. This smoothed line helps reveal trends and patterns in the data, making it easier to observe any overall relationships between the variables.\nLet‚Äôs continue exploring more of what Lets-Plot has in store for us! üìäüêßüåà\n\n\nCode\n(ggplot(df,\n        aes(x='flipper_length_mm',\n            y = 'body_mass_g',\n            color='species'\n           )\n       )\n + geom_point() # Draw points defined by an x and y coordinate, as for a scatter plot.\n + geom_smooth(method='lm') # Add a smoothed conditional mean. ‚Äòlm‚Äô stands for 'linear model' as Smoothing method\n) \n\n\n   \n   \n\n\nIn the previous example, we highlighted the importance of placing the color parameter at the global level, which grouped the data by the three penguin species and showed separate regression lines for each group. However, if we prefer to depict the regression line for the entire dataset, regardless of the group association, we can do so just as easily. All we need to do is remove the color parameter from the aesthetics of the ggplot function and place it solely in the geom_point.\nAdditionally, to enhance the plot further, we can properly label the x and y axes, add a title and subtitle. With these simple adjustments, we can achieve the same output as Hadley‚Äôs original code, with little to no modification.\n\n\nCode\n(ggplot(df, \n        aes(x='flipper_length_mm',\n            y = 'body_mass_g',\n           )\n       )\n + geom_point(aes(color='species', shape='species'))\n + geom_smooth(method='lm')\n + labs(\n    title = \"Body mass and flipper length\",\n    subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n    x = \"Flipper length (mm)\", y = \"Body mass (g)\",\n  ) \n + scale_color_viridis() # lets-plots equivalent of the scale_color_colorblind()\n) \n\n\n   \n   \n\n\n\n\nVisualizing Data Based on Categorical Variables\nLets-Plot provides numerous options to showcase our data using categorical variables. From bar plots, box plots, and violin plots to pie charts, the possibilities are diverse. You can check out their API for reference. Let‚Äôs explore some examples to demonstrate the versatility of Lets-Plot in visualizing categorical data.\n\n\nCode\npenguin_bar = (ggplot(df,aes(x='species'))\n               + geom_bar()\n              )\n\npenguin_box = (ggplot(df,aes(x = 'species', y = 'body_mass_g'))\n               + geom_boxplot()\n              )\n\npenguin_density = (ggplot(df,aes('body_mass_g', color='species', fill='species'))\n                   + geom_density(alpha=0.5)\n                  )\n\npenguin_rel_frequency = (ggplot(df,aes(x = 'island', fill = 'species'))\n                         + geom_bar(position='fill')\n                        )\ngggrid([penguin_bar, penguin_box, penguin_density, penguin_rel_frequency], ncol=2)\n\n\n   \n   \n\n\n\n\nIncorporate Multiple Variables with facet_wrap\nSo far we‚Äôve discovered how easy it is to plot data based on a single categorical variable. However, what if we want to depict relationships involving two or more categorical variables? That‚Äôs where facet_wrap comes in handy. This versatile function bears resemblance to similar functions found in seaborn or ggplot2 libraries.\nTo unlock the potential of facet_wrap, we simply need to define aesthetics, which can either be global or local to the mapping function. Then, we can use facet_wrap with the relevant categorical variable we want to visualize. It‚Äôs as simple as that!\n\n\nCode\n(ggplot(df, aes(x = 'flipper_length_mm', y = 'body_mass_g'))  \n + geom_point(aes(color = 'species', shape = 'species'), size = 4) \n + facet_wrap('island', nrow=1)\n + labs(title = \"Body mass and flipper length based on island\",\n        subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n        x = \"Flipper length (mm)\", y = \"Body mass (g)\",\n       )\n + theme(plot_title=element_text(size=20,face='bold'))\n + ggsize(1000,500)\n)\n\n\n   \n   \n\n\n\n\n\n\nReordering Categorical Variables Based On Statistics\nWhen visualizing data, a task I frequently encounter is ordering categorical variables in either ascending or descending order, based on statistics like median or mean. In my previous point on ‚ÄúVisualizing Data Based on Categorical Variables,‚Äù you noticed that the boxplot displayed categories in an unordered manner. However, consider how we can present them in an ascending order, determined by the median. This not only enhances the aesthetics of the plot but also provides valuable insights into the relationships among the categories.\n\n\nCode\n(ggplot(df,aes(as_discrete('species', order=1, order_by='..middle..'), \n               y = 'body_mass_g'))\n + geom_boxplot()\n)\n\n\n   \n   \n\n\nBy incorporating the as_discrete function, specifying the column, the ordering direction (1 for ascending, -1 for descending), and setting the order_by variable to middle (representing the median), the plot has become significantly more informative. This simple addition has allowed us to organize the categorical variables in a meaningful manner, improving the visualization‚Äôs clarity and aiding in the interpretation of relationships among the categories.\n\n\nChaining Pandas Code with Lets Plot Visualization\nOne of the best features of the pandas library is its remarkable customizability. With the help of the pd.pipe function, we can seamlessly integrate any of our own functions into method chains, as long as they return a DataFrame or Series. This opens up exciting possibilities to fully incorporate Lets-Plot into our code, just like pandas‚Äô own built-in plotting functionality.\nWhile Lets-Plot may be slightly more verbose than pandas plotting, it offers significantly more flexibility and freedom for customization. Not to mention that some may consider it visually more appealing. With Lets-Plot integrated into our pandas code, we can effortlessly create stunning and informative plots, making data analysis an even more enjoyable experience.\n\n\nCode\n(df\n .groupby('species')\n [['body_mass_g', 'flipper_length_mm']]\n .mean()\n .reset_index()\n .pipe(lambda df: (ggplot(df)\n                   + geom_pie(aes(slice='body_mass_g', fill='species'), \n                              stat='identity',size=30, hole=0.2, stroke=1.0,\n                              labels=layer_labels().line('@body_mass_g').format('@body_mass_g', '{.0f}').size(20)\n                             )\n                   + labs(title = \"Body mass based on species\",\n                          subtitle = \"Representing how Lets-Plot can be used with pd. pipe\",\n                          x = \"\", y = \"\",\n                         )\n                   + theme(axis='blank',\n                          plot_title=element_text(size=20,face='bold'))\n                   + ggsize(500,400)\n                  )\n )\n)\n\n\n   \n   \n\n\nThat‚Äôs a wrap on the Lets-Plot library! There‚Äôs so much more to explore and learn about this powerful tool. I hope you found this introduction helpful and consider integrating Lets-Plot into your daily data analysis routine.\nHappy coding üêçüñ•Ô∏èüîçüöÄ"
  },
  {
    "objectID": "posts/folium/2023-07-09-Folium.html",
    "href": "posts/folium/2023-07-09-Folium.html",
    "title": "ü•≥üéâ Two IBM certificates and some geospatial data",
    "section": "",
    "text": "I‚Äôm happy to share that I‚Äôve recently completed both IBM‚Äôs Data Analyst and Data Science Professional Certificates within the past month. The course content was well-structured, and I learned a great deal from these programs. For instance, I‚Äôve always been interested in learning SQL, and this was the perfect chance to start exploring it.\nIf you‚Äôre curious about these certificates, you can find more information through the links provided below. But my learning journey doesn‚Äôt stop here‚ÄîI‚Äôm planning to tackle most of the courses listed in the Data Science learning path on Coursera, so there‚Äôs more to come.\nWhile I‚Äôm at it, I wanted to introduce you to a neat library called Folium, which is fantastic for working with geospatial data. I came across Folium during the capstone project of the Data Science Specialization, where we had a fun task of predicting and visualizing the success of SpaceX rocket launches.\nIn this post, I‚Äôll briefly share what I‚Äôve learned about this library. I hope you‚Äôll find it useful too. Let‚Äôs dive in!\n\n\nCode\nimport folium\nimport pandas as pd\nimport os\nfrom folium import plugins\n\n\nWe‚Äôll be utilizing the dataset made available by https://open.toronto.ca/. This dataset includes the locations of bicycles installed on sidewalks and boulevards across the City of Toronto, wherever there‚Äôs a requirement for public bicycle parking facilities. By the way, I discovered this dataset through the Awesome Public Datasets repository on GitHub. If you haven‚Äôt already, I recommend checking them out.\n\n\nCode\n# Let's read in the file\n\nfor file in os.listdir():\n    if file.endswith(\".csv\"):\n        toronto_df = pd.read_csv(file)\n\n        print(f\"{file} read in as pandas dataframe\")\n\n\nStreet bicycle parking data - 4326.csv read in as pandas dataframe\n\n\nConsidering the original dataset has over 17,300 entries, we‚Äôll keep things light by working with just 500 rows for now. It‚Äôs all for the sake of a demonstration, after all!\n\n\nCode\ntoronto_df = toronto_df.sample(n=500)\ntoronto_df.head()\n\n\n\n\n\n\n\n\n\n_id\nOBJECTID\nID\nADDRESSNUMBERTEXT\nADDRESSSTREET\nFRONTINGSTREET\nSIDE\nFROMSTREET\nDIRECTION\nSITEID\nWARD\nBIA\nASSETTYPE\nSTATUS\nSDE_STATE_ID\nX\nY\nLONGITUDE\nLATITUDE\ngeometry\n\n\n\n\n784\n4481427\n10424\nBP-05283\n15\nDundonald St\nNaN\nNaN\nDundonald St\nNaN\nNaN\n13.0\nNaN\nRing\nTemporarily Removed\n0\nNaN\nNaN\nNaN\nNaN\n{'type': 'MultiPoint', 'coordinates': [[-79.38...\n\n\n3297\n4483940\n15253\nBP-35603\n49\nHarbour Sq\nQueens Quay W\nSouth\nHarbour Sq\nWest\nNaN\n10.0\nThe Waterfront\nRing\nExisting\n0\nNaN\nNaN\nNaN\nNaN\n{'type': 'MultiPoint', 'coordinates': [[-79.37...\n\n\n13971\n4494614\n31121\nBP-22492\n200\nElizabeth St\nElizabeth St\nWest\nLa Plante Ave\nWest\nNaN\n11.0\nNaN\nRing\nExisting\n0\nNaN\nNaN\nNaN\nNaN\n{'type': 'MultiPoint', 'coordinates': [[-79.38...\n\n\n5139\n4485782\n17465\nBP-40070\n70\nPeter St\nKing St W\nNorth\nPeter St\nWest\nNaN\n10.0\nToronto Downtown West\nRing\nExisting\n0\nNaN\nNaN\nNaN\nNaN\n{'type': 'MultiPoint', 'coordinates': [[-79.39...\n\n\n7635\n4488278\n20375\nBP-27153\n39\nPrince Arthur Ave\nPrince Arthur Ave\nSouth\nBedford Rd\nEast\nNaN\n11.0\nNaN\nRing\nExisting\n0\nNaN\nNaN\nNaN\nNaN\n{'type': 'MultiPoint', 'coordinates': [[-79.39...\n\n\n\n\n\n\n\nThe geometry column holds the longitude and latitude information, but before we dive in, we need to extract the valuable details. No worries ‚Äì we‚Äôll make use of pandas‚Äô str.extract for this task.\n\n\nCode\npattern = r\"(-?\\d+\\.\\d+),\\s*(-?\\d+\\.\\d+)\"\n\ntoronto_df_processed = toronto_df.assign(\n    LONGITUDE=lambda df: df.geometry.str.extract(pattern)[0],\n    LATITUDE=lambda df: df.geometry.str.extract(pattern)[1],\n).loc[:, [\"ASSETTYPE\", \"STATUS\", \"LONGITUDE\", \"LATITUDE\"]]\ntoronto_df_processed.head()\n\n\n\n\n\n\n\n\n\nASSETTYPE\nSTATUS\nLONGITUDE\nLATITUDE\n\n\n\n\n784\nRing\nTemporarily Removed\n-79.38378423783222\n43.6660359833018\n\n\n3297\nRing\nExisting\n-79.3774934493851\n43.6407633657936\n\n\n13971\nRing\nExisting\n-79.386799735149\n43.6589303889453\n\n\n5139\nRing\nExisting\n-79.3926661761316\n43.6460273003346\n\n\n7635\nRing\nExisting\n-79.3973838724551\n43.6693038734947\n\n\n\n\n\n\n\n\nCreating the map and displaying it\nHere‚Äôs an example of how to create a map without any overlaid data points.\n\n\nCode\ntoronto_map = folium.Map(\n    location=[43.651070, -79.347015], zoom_start=11, tiles=\"OpenStreetMap\"\n)\ntoronto_map\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\nFigure¬†1: The City of Toronto\n\n\n\n\n\nSuperimposing bike locations on the map with FeatureGroup\nAfter instantiating FeatureGroup, we can easily add the bike locations using the add_child method. It is really easy!\n\n\nCode\n# let's start with a clean copy of the map of Toronto\ntoronto_map = folium.Map(\n    location=[43.651070, -79.347015], zoom_start=11, tiles=\"OpenStreetMap\"\n)\n\n# instantiate a feature group \nbike_stations = folium.map.FeatureGroup()\n\n# loop through the bike stations\nfor lat, long in zip(toronto_df_processed.LATITUDE, toronto_df_processed.LONGITUDE):\n    bike_stations.add_child(\n        folium.features.CircleMarker(\n            [lat, long],\n            radius=5,\n            color=\"red\",\n            fill=True,\n            fill_color=\"yellow\",\n            fill_opacity=1,\n        )\n    )\n# add bike stations to the map\ntoronto_map.add_child(bike_stations)\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\nFigure¬†2: The City of Toronto with available bike locations\n\n\n\n\n\nAdding pop-up text with relevant information\nWe can also enhance this by adding a pop-up box that displays custom text of our choice.\n\n\nCode\n# let's start with a clean copy of the map of Toronto\ntoronto_map = folium.Map(\n    location=[43.651070, -79.347015], zoom_start=11, tiles=\"OpenStreetMap\"\n)\n\n# instantiate a feature group \nbike_stations = folium.map.FeatureGroup()\n\n# loop through the bike stations\nfor lat, long in zip(toronto_df_processed.LATITUDE, toronto_df_processed.LONGITUDE):\n    bike_stations.add_child(\n        folium.features.CircleMarker(\n            [lat, long],\n            radius=5,\n            color=\"grey\",\n            fill=True,\n            fill_color=\"white\",\n            fill_opacity=1,\n        )\n    )\n\n# add pop-up text to each marker on the map\nlatitudes = list(toronto_df_processed.LATITUDE)\nlongitudes = list(toronto_df_processed.LONGITUDE)\nlabels = list(toronto_df_processed.STATUS)\n\nfor lat, lng, label in zip(latitudes, longitudes, labels):\n    folium.Marker([lat, lng], popup=label).add_to(toronto_map)\n\n# add bike stations to map\ntoronto_map.add_child(bike_stations)\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\nFigure¬†3: The City of Toronto with available bike locations\n\n\n\n\n\nClustering the rental locations with MarkerCluster\nAnd the best part, which happens to be my favorite, is that we can also integrate a MarkerCluster. This comes in handy when we‚Äôre dealing with numerous data points clustered closely together on the map. With a MarkerCluster, you get to see their combined values instead of each one individually. It‚Äôs a fantastic feature!\n\n\nCode\n# let's start with a clean copy of the map of Toronto\ntoronto_map = folium.Map(\n    location=[43.651070, -79.347015], zoom_start=11, tiles=\"OpenStreetMap\"\n)\n\n# instantiate a mark cluster object \nbike_stations_cluster = plugins.MarkerCluster().add_to(toronto_map)\n\n# loop through the dataframe and add each data point to the mark cluster\nfor lat, lng, label, in zip(\n    toronto_df_processed.LATITUDE,\n    toronto_df_processed.LONGITUDE,\n    toronto_df_processed.STATUS,\n):\n    folium.Marker(\n        location=[lat, lng],\n        icon=None,\n        popup=label,\n    ).add_to(bike_stations_cluster)\n\n# display map\ntoronto_map\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\nFigure¬†4: Aggregated Bike Locations in the City of Toronto\n\n\n\nThat‚Äôs a wrap! I hope these examples have been helpful. Feel free to use these techniques in your next data science or geospatial project. Until next time, happy exploring!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "‚ÄúConstant dripping wears away a stone‚Äù\n\n\nHi there! Welcome to my website! \n\nI am Adam, a PhD candidate at the University of Antwerp focusing on the field of environmental toxicology. If you‚Äôre curious about my academic journey and accomplishments, feel free to explore Pubmed for more information.\nThis platform is mainly dedicated to my pursuit of mastering data science, and I would invite you to join me. Here, you‚Äôll come across various projects, blog posts, and valuable tidbits that could inspire your own endeavors.\nLet‚Äôs embark on this data-driven journey together!"
  },
  {
    "objectID": "articles.html",
    "href": "articles.html",
    "title": "Articles",
    "section": "",
    "text": "The Ultimate Speed Test: Pandas vs Polars\n\n\n\n\n\n\n\nPolars\n\n\nPandas\n\n\n\n\n\n\n\n\n\n\n\nAug 13, 2023\n\n\nAdam Cseresznye\n\n\n\n\n\n\n  \n\n\n\n\nGetting Started with Polars\n\n\n\n\n\n\n\nPolars\n\n\n\n\n\n\n\n\n\n\n\nAug 11, 2023\n\n\nAdam Cseresznye\n\n\n\n\n\n\n  \n\n\n\n\nü•≥üéâ Two IBM certificates and some geospatial data\n\n\n\n\n\n\n\ncelebration\n\n\ngeospatial data\n\n\n\n\n\n\n\n\n\n\n\nAug 10, 2023\n\n\nAdam Cseresznye\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday: Hot Ones Episodes\n\n\n\n\n\n\n\nTidy Tuesday\n\n\n\n\n\n\n\n\n\n\n\nAug 9, 2023\n\n\nAdam Cseresznye\n\n\n\n\n\n\n  \n\n\n\n\nHow to use the Lets-Plot library by JetBrains\n\n\n\n\n\n\n\nLets-Plot\n\n\nggplot2\n\n\n\n\n\n\n\n\n\n\n\nAug 5, 2023\n\n\nAdam Cseresznye\n\n\n\n\n\n\n  \n\n\n\n\nTop 10 things I learned from the book Effective Pandas by Matt Harrison\n\n\n\n\n\n\n\nPandas\n\n\n\n\n\n\n\n\n\n\n\nAug 2, 2023\n\n\nAdam Cseresznye\n\n\n\n\n\n\n  \n\n\n\n\nüèÜü•â3rd place at Kaggle‚Äôs Classification with a Tabular Vector Borne Disease Dataset\n\n\n\n\n\n\n\ncelebration\n\n\nKaggle\n\n\n\n\n\n\n\n\n\n\n\nJun 4, 2023\n\n\nAdam Cseresznye\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-08-08-HotOnes/2023_08_08_HotOnes.html",
    "href": "posts/2023-08-08-HotOnes/2023_08_08_HotOnes.html",
    "title": "Tidy Tuesday: Hot Ones Episodes",
    "section": "",
    "text": "I‚Äôve decided to get more involved in the Tidy Tuesday movement. I think it‚Äôs a really enjoyable way to improve my skills in working with data, like analyzing, organizing, and visualizing it. The cool datasets they provide make it even more interesting. More information on Tidy Tuesday and their past datasets can be found here.\nThis week we have a dataset related to the show Hot Ones. Hot Ones is a unique web series that combines spicy food challenges with celebrity interviews. Hosted by Sean Evans, guests tackle increasingly hot chicken wings while answering questions, leading to candid and entertaining conversations. The show‚Äôs blend of heat and honesty has turned it into a global sensation, offering a fresh take on interviews and captivating audiences worldwide.\nLet‚Äôs see what we can learn from the data üî¨üïµÔ∏è‚Äç‚ôÇÔ∏è."
  },
  {
    "objectID": "posts/2023-08-08-HotOnes/2023_08_08_HotOnes.html#what-differences-can-be-observed-in-the-spiciness-of-sauces-as-we-look-across-the-various-seasons",
    "href": "posts/2023-08-08-HotOnes/2023_08_08_HotOnes.html#what-differences-can-be-observed-in-the-spiciness-of-sauces-as-we-look-across-the-various-seasons",
    "title": "Tidy Tuesday: Hot Ones Episodes",
    "section": "What differences can be observed in the spiciness of sauces as we look across the various seasons?",
    "text": "What differences can be observed in the spiciness of sauces as we look across the various seasons?\n\n\nCode\n(sauces\n .groupby('season')\n .agg(AVG_scoville=('scoville', 'mean'),\n      median_scoville=('scoville', 'median'),\n     )\n .reset_index()\n .melt(id_vars='season',\n       var_name='statistics',\n      )\n .pipe(lambda df: (ggplot(df, aes('season', 'value',fill='statistics'))\n                   + geom_bar(stat='identity', show_legend= False)\n                   + facet_wrap('statistics',nrow=1,scales='free_y')\n                   + labs(x='Seasons',\n                          y='Scoville Units'\n                         )\n                   + theme(plot_title=element_text(size=20,face='bold'))\n                  + ggsize(1000,600)\n                  )\n      )\n \n)\n\n\n\n   \n   \nFigure¬†1: Average and median Scoville Units across the Hot Ones‚Äô seasons\n\n\n\nThere seems to be a shift during the season 3-5 period as can be seen in Figure¬†1. Both indicators ‚Äì mean and median Scoville Units ‚Äì show a consistent upward trend over this time frame and later on they stabilize.\nWhat about the overall spread of the data? ü§î\n\n\nCode\n(sauces\n .loc[:, ['season', 'scoville']]\n .pipe(lambda df: (ggplot(df, aes('season', 'scoville'))\n                   + geom_boxplot()\n                   + scale_y_log10()\n                   + labs(x='Seasons',\n                          y='log(Scoville Units)'\n                         )\n                   + theme(plot_title=element_text(size=20,face='bold'))\n                   + ggsize(1000,600)\n                  )\n      )\n)\n\n\n\n   \n   \nFigure¬†2: Spread of Scoville Units across the Hot Ones‚Äô seasons\n\n\n\nHere are some observations: Season 5 exhibits the widest range, featuring sauces with Scoville Units spanning from 450 to 2,000,000. In addition, starting from season 6 onwards, the averages, medians, and ranges of Scoville Units appear to even out."
  },
  {
    "objectID": "posts/2023-08-08-HotOnes/2023_08_08_HotOnes.html#has-every-individual-successfully-completed-all-the-episodes",
    "href": "posts/2023-08-08-HotOnes/2023_08_08_HotOnes.html#has-every-individual-successfully-completed-all-the-episodes",
    "title": "Tidy Tuesday: Hot Ones Episodes",
    "section": "Has every individual successfully completed all the episodes?",
    "text": "Has every individual successfully completed all the episodes?\nTo answer this question we will use the episodes dataframe. Keep in mind there are 300 episodes in this dataframe. The finished column can be useful here. Just by looking for entries where finished == False we will have our answer.\n\n\nCode\n(episodes\n .query(\"finished==False\")\n [['season', 'guest', 'guest_appearance_number']]\n)\n\n\n\n\n\n\n\n\n\nseason\nguest\nguest_appearance_number\n\n\n\n\n0\n1\nTony Yayo\n1\n\n\n7\n1\nDJ Khaled\n1\n\n\n19\n2\nMike Epps\n1\n\n\n20\n2\nJim Gaffigan\n1\n\n\n24\n2\nRob Corddry\n1\n\n\n51\n3\nRicky Gervais\n1\n\n\n90\n4\nMario Batali\n1\n\n\n96\n5\nTaraji P. Henson\n1\n\n\n129\n7\nLil Yachty\n1\n\n\n130\n7\nE-40\n1\n\n\n144\n8\nShaq\n1\n\n\n171\n10\nChance the Rapper\n1\n\n\n185\n12\nEric Andr√©\n2\n\n\n218\n15\nQuavo\n1\n\n\n251\n17\nPusha T\n1\n\n\n\n\n\n\n\nTaking a closer look, it seems that around 15 participants didn‚Äôt make it through the entire Hot Ones interview challenge.Not bad out of 300 shows. And guess what? Eric Andr√© popped up on the show not just once, but twice! Now, the big question: did he conquer the hot seat in at least one of those interviews? Let‚Äôs plot the data to make it more visual‚Ä¶\n\n\nCode\n(episodes\n .query(\"finished==False\")\n .groupby('season')\n .finished\n .count()\n .to_frame()\n .reset_index()\n .pipe(lambda df: (ggplot(df, aes('season', 'finished'))\n                   + geom_bar(stat='identity')\n                   + labs(x='Seasons',\n                          y='Number of incomplete interviews'\n                         )\n                   + theme(plot_title=element_text(size=20,face='bold'))\n                   + ggsize(600,400)\n                  )\n      )\n\n)\n\n\n\n   \n   \nFigure¬†3: Number of incomplete interviews per season\n\n\n\nInterestingly, a majority of these incomplete interviews belong to season 2 (Figure¬†3). This fact is quite surprising, especially when you consider that the maximum Scoville value for that season was only 550,000 ‚Äì nearly a quarter of the following year‚Äôs value, where only one person faced difficulty finishing the challenge."
  },
  {
    "objectID": "posts/2023-08-08-HotOnes/2023_08_08_HotOnes.html#what-is-the-completion-rate-per-season",
    "href": "posts/2023-08-08-HotOnes/2023_08_08_HotOnes.html#what-is-the-completion-rate-per-season",
    "title": "Tidy Tuesday: Hot Ones Episodes",
    "section": "What is the completion rate per season?",
    "text": "What is the completion rate per season?\nTo get to the bottom of this question, let‚Äôs start by figuring out how many episodes were in each season. We can grab this info from the season dataset. Just a heads-up, in season 9 they seem to threw in an extra episode. So, keep this in mind! Otherwise, you might end up with percentages that go beyond 100%.\n\n\nCode\n# First we need to find out the total number of episodes per season\n\nepisodes_per_season = (season\n                       [['season', 'episodes', 'note']]\n                       .set_index('season')\n                       # we need to extract the one extra episode in season 9\n                       .assign(note=lambda df: df.note\n                               .str.extract(r'([0-9.]+)')\n                               .astype(float),\n                        # add the two column together\n                               episodes=lambda df: df.episodes\n                               .add(df.note, fill_value=0)\n                              )\n                       .drop(columns='note')\n                       .squeeze()\n                      )\n\n\n\n\nCode\ncompletion_rate = (episodes\n                   .query(\"finished==True\")\n                   .groupby('season')\n                   .finished\n                   .sum()\n                   .div(episodes_per_season)\n                   .mul(100)\n                   .to_frame().reset_index()\n                   .rename(columns={0:'completion_rate'})\n                  )\n                   \n(completion_rate                  \n .pipe(lambda df: (ggplot(df, aes('season', 'completion_rate'))\n                   + geom_line(stat='identity')\n                   + labs(x='Seasons',\n                          y='% successful participants'\n                         )\n                   + theme(plot_title=element_text(size=20,face='bold'))\n                   + ggsize(600,400)\n                  )\n      )\n)\n\n\n\n   \n   \nFigure¬†4: Completion rate per season\n\n\n\nTaking a peek at Figure¬†4, it seems like the normalized completion rate hits its lowest point in season 1, closely followed by season 7. However, even in those seasons, the rate remains surprisingly high."
  },
  {
    "objectID": "posts/2023-08-08-HotOnes/2023_08_08_HotOnes.html#is-there-a-correlation-between-scoville-units-and-completion-rate",
    "href": "posts/2023-08-08-HotOnes/2023_08_08_HotOnes.html#is-there-a-correlation-between-scoville-units-and-completion-rate",
    "title": "Tidy Tuesday: Hot Ones Episodes",
    "section": "Is there a correlation between Scoville Units and completion rate?",
    "text": "Is there a correlation between Scoville Units and completion rate?\nHere‚Äôs a curious thought: could there be a link between Scoville Units and the completion rate? I‚Äôm just wondering if the spiciness level affects how well participants handle the challenge. Exploring this connection might add a spicy twist to the Hot Ones experience ‚Äì let‚Äôs see where the data takes us!\n\n\nCode\n# AVG_scoville code comes from a code snippet \n# 'What differences can be observed in the spiciness \n# of sauces as we look across the various seasons?'\n\nAVG_scoville = (sauces\n                .groupby('season')\n                .agg(AVG_scoville=('scoville', 'mean'))\n                .squeeze()\n               )\n\n# Let's calculate the Pearson correlation coefficient. We have to discard the last value\n# as the completion rate is not defined for that\n\nstats.pearsonr(AVG_scoville.values[:-1],completion_rate.completion_rate.values[:-1])\n\n\nPearsonRResult(statistic=0.5039021286250108, pvalue=0.02349225734224539)\n\n\nThe Pearson correlation coefficient has its say: there‚Äôs actually a moderate positive correlation(0.5, p&lt;0.05) between Scoville Units and completion rate. Quite intriguing, isn‚Äôt it? Honestly, I was expecting the opposite outcome myself! It seems like the higher the average spiciness, the more determined the guests become. Take a look at Figure¬†5.\n\n\nCode\n(pd.concat([AVG_scoville,completion_rate],axis=1)\n .rename(columns={0:'success_rate'})\n .pipe(lambda df: (ggplot(df, aes('AVG_scoville', 'completion_rate'))\n                   + geom_point(size=5, alpha=0.5)\n                   + geom_smooth()\n                   + labs(x='Average Scoville units',\n                          y='% successful participants'\n                         )\n                   + theme(plot_title=element_text(size=20,face='bold'))\n                   + ggsize(600,400)\n                  )\n      )\n)\n\n\n\n   \n   \nFigure¬†5: Correlation between Average Scoville units and Completion rates"
  },
  {
    "objectID": "posts/2023-08-08-HotOnes/2023_08_08_HotOnes.html#are-there-any-returning-guests",
    "href": "posts/2023-08-08-HotOnes/2023_08_08_HotOnes.html#are-there-any-returning-guests",
    "title": "Tidy Tuesday: Hot Ones Episodes",
    "section": "Are there any returning guests?",
    "text": "Are there any returning guests?\nHas there been a brave soul who dared to make a return to the show for a second time? The column guest_appearance_number holds the answers you‚Äôre looking for.\n\n\nCode\n(episodes\n .query(\"guest_appearance_number &gt; 1\")\n [['guest','season', 'episode_season', 'finished']]\n)\n\n\n\n\n\n\n\n\n\nguest\nseason\nepisode_season\nfinished\n\n\n\n\n124\nEddie Huang\n6\n13\nTrue\n\n\n161\nJay Pharoah\n9\n999\nTrue\n\n\n183\nTom Segura\n12\n1\nTrue\n\n\n185\nEric Andr√©\n12\n3\nFalse\n\n\n188\nT-Pain\n12\n6\nTrue\n\n\n189\nAdam Richman\n12\n7\nTrue\n\n\n190\nAction Bronson\n12\n8\nTrue\n\n\n203\nNaN\n13\n11\nTrue\n\n\n214\nRussell Brand\n14\n11\nTrue\n\n\n215\nSteve-O\n14\n12\nTrue\n\n\n241\nGordon Ramsay\n16\n14\nTrue\n\n\n254\nPost Malone\n18\n1\nTrue\n\n\n\n\n\n\n\nIt looks like a total of 12 individuals have taken on the challenge not once, but twice. Hats off to their courage! üé©"
  },
  {
    "objectID": "posts/Kaggle/Kaggle.html",
    "href": "posts/Kaggle/Kaggle.html",
    "title": "üèÜü•â3rd place at Kaggle‚Äôs Classification with a Tabular Vector Borne Disease Dataset",
    "section": "",
    "text": "Greetings everyone! I am thrilled to announce that I have achieved a significant milestone in my data science journey. I am delighted to share that I secured the 3rd position in Kaggle‚Äôs ‚ÄúClassification with a Tabular Vector Borne Disease Dataset‚Äù competition, surpassing 931 talented contenders! If you‚Äôre interested in learning more about my approach to the competition, I invite you to check out my detailed solution on the Kaggle discussion page.\n\n\n\n\n\nIn this exciting competition, the aim was to predict the three most probable diseases out of a total of 11 vector-borne diseases, including well-known names such as Zika, Yellow Fever, Malaria, and more. Our task involved analyzing the provided symptoms and using the MPA@3 evaluation metric for accurate predictions.\nMy journey in the world of Kaggle has been a relatively short one, with this competition marking my third participation. To give you some context, let me share a bit about my previous placements:\n\nReaching this point has been a long and rewarding endeavor. Over the past two years, I have been studying this topic in my free time while also pursuing a full-time Ph.D.¬†research. It has been an exciting balancing act!\nParticipating in Kaggle competitions has been an invaluable learning experience for me. It has allowed me to apply my knowledge, explore new techniques, and connect with fellow data enthusiasts. I am deeply grateful to the organizers of Kaggle for providing a platform that fosters growth and healthy competition.\nWhile this achievement brings a sense of accomplishment, I am already diving into new competitions and expanding my skills. Currently, I am particularly interested in reproducible data and developing robust architectures for machine learning projects to enhance code reproducibility. In fact, I am considering writing a blog on this often overlooked topic.\nStay tuned for more :)\n#DataScience #KaggleCompetition #VectorBorneDiseases #MachineLearning #ReproducibleData"
  },
  {
    "objectID": "posts/polars/Polars_revised.html",
    "href": "posts/polars/Polars_revised.html",
    "title": "Getting Started with Polars",
    "section": "",
    "text": "In the ever-evolving field of data science, effective data manipulation tools are essential. Enter Polars, a Rust-based library garnering attention within the data community. Boasting impressive speed and versatile capabilities, Polars is redefining our data management practices. In this blog, we delve into Polars‚Äô core functions and practical applications, shedding light on how it empowers data professionals to efficiently tackle complex tasks.\nFor those well-versed in Pandas, Polars offers a blend of familiarity and innovation. Although this document is not designed to substitute the official documentation, it serves as a tool to provide you with insights into the capabilities that polars offers. Our goal is to ensure the continuous updating of this document.\nOur exploration of Polars is guided by insights from the Polars User Guide, Kevin Heavey‚Äôs perspectives in Modern Polars and Matt Harrison‚Äôs engaging tutorial on Polars at PyCon. We kickstart our exploration with Matt Harrison‚Äôs shared dataset, the US Department of Energy‚Äôs Automobile Fuel Economy data. Let‚Äôs begin this journey!"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#use-take-for-series-or-expressions",
    "href": "posts/polars/Polars_revised.html#use-take-for-series-or-expressions",
    "title": "Getting Started with Polars",
    "section": "Use take for Series or Expressions",
    "text": "Use take for Series or Expressions\nAn interesting approach is through Polars‚Äô take method. This method can be used with Expr, Series, or np.ndarray. However, it‚Äôs worth noting that if you intend to utilize it with DataFrames, a preliminary conversion to Series is necessary.\n\n\nCode\ndf.select(pl.col(\"make\")).to_series().take(list(range(0, 5)))\n\n\n\n\nshape: (5,)\n\n\n\nmake\n\n\ncat\n\n\n\n\n\"Alfa Romeo\"\n\n\n‚Ä¶\n\n\n\"Subaru\""
  },
  {
    "objectID": "posts/polars/Polars_revised.html#using-filter",
    "href": "posts/polars/Polars_revised.html#using-filter",
    "title": "Getting Started with Polars",
    "section": "Using filter",
    "text": "Using filter\nSimilar to Pandas, Polars features a filter method that assists in the process of selectively filtering rows based on one or multiple conditions.\n\nEvaluate a single condition\n\n\nCode\ndf.filter(pl.col(\"model\") == \"Testarossa\")\n\n\n\nshape: (14, 11)\n\n\n\nmake\nmodel\ndrive\ncylinders\ndispl\nfuelType\nmpgData\ncity08\nhighway08\ncomb08\nyear\n\n\ncat\ncat\ncat\ni8\nf32\ncat\nbool\ni8\ni8\ni8\ndate\n\n\n\n\n\"Ferrari\"\n\"Testarossa\"\n\"Rear-Wheel Dri‚Ä¶\n12\n4.9\n\"Regular\"\nfalse\n9\n14\n11\n1985-01-01\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\"Ferrari\"\n\"Testarossa\"\n\"Rear-Wheel Dri‚Ä¶\n12\n4.9\n\"Premium\"\nfalse\n10\n15\n11\n1992-01-01\n\n\n\n\n\n\n\n\nCombine multiple conditions\n\n\nCode\ndf.filter((pl.col(\"model\") == \"Testarossa\") & (pl.col(\"make\") == \"Ferrari\"))\n\n\n\nshape: (9, 11)\n\n\n\nmake\nmodel\ndrive\ncylinders\ndispl\nfuelType\nmpgData\ncity08\nhighway08\ncomb08\nyear\n\n\ncat\ncat\ncat\ni8\nf32\ncat\nbool\ni8\ni8\ni8\ndate\n\n\n\n\n\"Ferrari\"\n\"Testarossa\"\n\"Rear-Wheel Dri‚Ä¶\n12\n4.9\n\"Regular\"\nfalse\n9\n14\n11\n1985-01-01\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\"Ferrari\"\n\"Testarossa\"\n\"Rear-Wheel Dri‚Ä¶\n12\n4.9\n\"Premium\"\nfalse\n10\n15\n11\n1992-01-01"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#imitate-pandas-index-using-with_row_count",
    "href": "posts/polars/Polars_revised.html#imitate-pandas-index-using-with_row_count",
    "title": "Getting Started with Polars",
    "section": "Imitate pandas‚Äô index using with_row_count",
    "text": "Imitate pandas‚Äô index using with_row_count\nEven in the absence of a traditional index, you can make use of the with_row_count method in Polars. This clever method comes to the rescue for tasks like indexing and filtering, providing an alternative approach.\n\n\nCode\n(df.with_row_count().filter(pl.col(\"row_nr\") == 5))\n\n\n\nshape: (1, 12)\n\n\n\nrow_nr\nmake\nmodel\ndrive\ncylinders\ndispl\nfuelType\nmpgData\ncity08\nhighway08\ncomb08\nyear\n\n\nu32\ncat\ncat\ncat\ni8\nf32\ncat\nbool\ni8\ni8\ni8\ndate\n\n\n\n\n5\n\"Subaru\"\n\"Loyale\"\n\"Front-Wheel Dr‚Ä¶\n4\n1.8\n\"Regular\"\nfalse\n21\n24\n22\n1993-01-01\n\n\n\n\n\n\n\nUse is_in\nThe is_in function can be employed to verify whether elements of a given expression are present within another Series. We can use this to filter our rows.\n\n\nCode\ndf.filter(pl.col(\"cylinders\").is_in([i for i in range(6, 8)]))\n\n\n\nshape: (14_284, 11)\n\n\n\nmake\nmodel\ndrive\ncylinders\ndispl\nfuelType\nmpgData\ncity08\nhighway08\ncomb08\nyear\n\n\ncat\ncat\ncat\ni8\nf32\ncat\nbool\ni8\ni8\ni8\ndate\n\n\n\n\n\"Audi\"\n\"100\"\n\"Front-Wheel Dr‚Ä¶\n6\n2.8\n\"Premium\"\ntrue\n17\n22\n19\n1993-01-01\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\"Pontiac\"\n\"Grand Am\"\n\"Front-Wheel Dr‚Ä¶\n6\n3.3\n\"Regular\"\nfalse\n18\n26\n21\n1993-01-01"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#difference-between-select-and-with_columns",
    "href": "posts/polars/Polars_revised.html#difference-between-select-and-with_columns",
    "title": "Getting Started with Polars",
    "section": "Difference between select and with_columns`",
    "text": "Difference between select and with_columns`\nAs evident from the examples below, both select and with_column can be utilized for both column selection and column assignment. However, there‚Äôs a crucial distinction between the two. After performing column operations, the select method drops the unselected columns and retains only the columns that underwent operations. Conversely, the with_column method appends the new columns to the dataframe, preserving the original ones.\n\n\nCode\n# everything else is dropped\n\n(\n    df.select(\n        (pl.col(\"city08\") + 10).alias(\"added\"),\n        (pl.col(\"city08\") - 10).alias(\"substracted\"),\n    )\n)\n\n\n\n\nshape: (41_144, 2)\n\n\n\nadded\nsubstracted\n\n\ni8\ni8\n\n\n\n\n29\n9\n\n\n‚Ä¶\n‚Ä¶\n\n\n26\n6\n\n\n\n\n\n\n\nCode\n# columns are kept\n\n(\n    df.with_columns(\n        (pl.col(\"city08\") + 10).alias(\"added\"),\n        (pl.col(\"city08\") - 10).alias(\"substracted\"),\n    )\n)\n\n\n\nshape: (41_144, 13)\n\n\n\nmake\nmodel\ndrive\ncylinders\ndispl\nfuelType\nmpgData\ncity08\nhighway08\ncomb08\nyear\nadded\nsubstracted\n\n\ncat\ncat\ncat\ni8\nf32\ncat\nbool\ni8\ni8\ni8\ndate\ni8\ni8\n\n\n\n\n\"Alfa Romeo\"\n\"Spider Veloce ‚Ä¶\n\"Rear-Wheel Dri‚Ä¶\n4\n2.0\n\"Regular\"\ntrue\n19\n25\n21\n1985-01-01\n29\n9\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\"Subaru\"\n\"Legacy AWD Tur‚Ä¶\n\"4-Wheel or All‚Ä¶\n4\n2.2\n\"Premium\"\nfalse\n16\n21\n18\n1993-01-01\n26\n6\n\n\n\n\n\n\nNow that we‚Äôve clarified this point, let‚Äôs proceed to explore the fundamental methods for selecting columns."
  },
  {
    "objectID": "posts/polars/Polars_revised.html#select-all-columns",
    "href": "posts/polars/Polars_revised.html#select-all-columns",
    "title": "Getting Started with Polars",
    "section": "Select all columns",
    "text": "Select all columns\nA particularly handy tool is pl.all(), which provides the current state of the dataframe‚Äîsimilar to pd.assign(foo=lambda df) in Pandas. This can prove useful, particularly when dealing with operations like groupby and aggregation.\n\n\nCode\n# this is analogous to df.select(pl.col(\"*\")), where * represents the wildcard component\ndf.select(pl.all())\n\n\n\nshape: (41_144, 11)\n\n\n\nmake\nmodel\ndrive\ncylinders\ndispl\nfuelType\nmpgData\ncity08\nhighway08\ncomb08\nyear\n\n\ncat\ncat\ncat\ni8\nf32\ncat\nbool\ni8\ni8\ni8\ndate\n\n\n\n\n\"Alfa Romeo\"\n\"Spider Veloce ‚Ä¶\n\"Rear-Wheel Dri‚Ä¶\n4\n2.0\n\"Regular\"\ntrue\n19\n25\n21\n1985-01-01\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\"Subaru\"\n\"Legacy AWD Tur‚Ä¶\n\"4-Wheel or All‚Ä¶\n4\n2.2\n\"Premium\"\nfalse\n16\n21\n18\n1993-01-01"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#select-all-columns-except-for",
    "href": "posts/polars/Polars_revised.html#select-all-columns-except-for",
    "title": "Getting Started with Polars",
    "section": "Select all columns except for‚Ä¶",
    "text": "Select all columns except for‚Ä¶\n\n\nCode\ndf.select(pl.col(\"*\").exclude(\"year\", \"comb08\", \"highway08\"))\n\n\n\nshape: (41_144, 8)\n\n\n\nmake\nmodel\ndrive\ncylinders\ndispl\nfuelType\nmpgData\ncity08\n\n\ncat\ncat\ncat\ni8\nf32\ncat\nbool\ni8\n\n\n\n\n\"Alfa Romeo\"\n\"Spider Veloce ‚Ä¶\n\"Rear-Wheel Dri‚Ä¶\n4\n2.0\n\"Regular\"\ntrue\n19\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\"Subaru\"\n\"Legacy AWD Tur‚Ä¶\n\"4-Wheel or All‚Ä¶\n4\n2.2\n\"Premium\"\nfalse\n16"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#select-a-subset-of-columns",
    "href": "posts/polars/Polars_revised.html#select-a-subset-of-columns",
    "title": "Getting Started with Polars",
    "section": "Select a subset of columns",
    "text": "Select a subset of columns\n\n\nCode\ndf.select(pl.col(\"year\", \"comb08\", \"highway08\"))\n\n\n\n\nshape: (41_144, 3)\n\n\n\nyear\ncomb08\nhighway08\n\n\ndate\ni8\ni8\n\n\n\n\n1985-01-01\n21\n25\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n1993-01-01\n18\n21"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#select-columns-based-on-regular-expression",
    "href": "posts/polars/Polars_revised.html#select-columns-based-on-regular-expression",
    "title": "Getting Started with Polars",
    "section": "Select columns based on regular expression",
    "text": "Select columns based on regular expression\n\n\nCode\ndf.select(pl.col(\"^.*(mo|ma).*$\"))\n\n\n\n\nshape: (41_144, 2)\n\n\n\nmake\nmodel\n\n\ncat\ncat\n\n\n\n\n\"Alfa Romeo\"\n\"Spider Veloce ‚Ä¶\n\n\n‚Ä¶\n‚Ä¶\n\n\n\"Subaru\"\n\"Legacy AWD Tur‚Ä¶"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#select-columns-based-on-dtypes",
    "href": "posts/polars/Polars_revised.html#select-columns-based-on-dtypes",
    "title": "Getting Started with Polars",
    "section": "Select columns based on dtypes",
    "text": "Select columns based on dtypes\nThis may remind you of pandas‚Äô select_dtypes method.\n\n\nCode\ndf.select(pl.col(pl.Int8, pl.Boolean))\n\n\n\nshape: (41_144, 5)\n\n\n\ncylinders\nmpgData\ncity08\nhighway08\ncomb08\n\n\ni8\nbool\ni8\ni8\ni8\n\n\n\n\n4\ntrue\n19\n25\n21\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n4\nfalse\n16\n21\n18"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#using-selectors",
    "href": "posts/polars/Polars_revised.html#using-selectors",
    "title": "Getting Started with Polars",
    "section": "Using selectors",
    "text": "Using selectors\nSelectors offer a more intuitive approach to selecting columns from DataFrame or LazyFrame objects, taking into account factors like column names, data types, and other properties. They consolidate and enhance the functionality that‚Äôs accessible through the col() expression. More on them here.\n\nBy dtypes\n\n\nCode\ndf.select(cs.integer(), cs.float(), cs.temporal())\n\n\n\nshape: (41_144, 6)\n\n\n\ncylinders\ncity08\nhighway08\ncomb08\ndispl\nyear\n\n\ni8\ni8\ni8\ni8\nf32\ndate\n\n\n\n\n4\n19\n25\n21\n2.0\n1985-01-01\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n4\n16\n21\n18\n2.2\n1993-01-01\n\n\n\n\n\n\n\n\nCode\n# all columns except for the ones containing float\ndf.select(cs.all() - cs.numeric())\n\n\n\nshape: (41_144, 6)\n\n\n\nmake\nmodel\ndrive\nfuelType\nmpgData\nyear\n\n\ncat\ncat\ncat\ncat\nbool\ndate\n\n\n\n\n\"Alfa Romeo\"\n\"Spider Veloce ‚Ä¶\n\"Rear-Wheel Dri‚Ä¶\n\"Regular\"\ntrue\n1985-01-01\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\"Subaru\"\n\"Legacy AWD Tur‚Ä¶\n\"4-Wheel or All‚Ä¶\n\"Premium\"\nfalse\n1993-01-01\n\n\n\n\n\n\n\n\nCode\n# same as the one above but it uses the tilde\ndf.select(~cs.numeric())\n\n\n\nshape: (41_144, 6)\n\n\n\nmake\nmodel\ndrive\nfuelType\nmpgData\nyear\n\n\ncat\ncat\ncat\ncat\nbool\ndate\n\n\n\n\n\"Alfa Romeo\"\n\"Spider Veloce ‚Ä¶\n\"Rear-Wheel Dri‚Ä¶\n\"Regular\"\ntrue\n1985-01-01\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\"Subaru\"\n\"Legacy AWD Tur‚Ä¶\n\"4-Wheel or All‚Ä¶\n\"Premium\"\nfalse\n1993-01-01\n\n\n\n\n\n\n\n\nBy column names\n\n\nCode\ndf.select(cs.contains(\"08\"))\n\n\n\n\nshape: (41_144, 3)\n\n\n\ncity08\nhighway08\ncomb08\n\n\ni8\ni8\ni8\n\n\n\n\n19\n25\n21\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n16\n21\n18\n\n\n\n\n\n\n\nCode\ndf.select(cs.starts_with(\"d\"))\n\n\n\n\nshape: (41_144, 2)\n\n\n\ndrive\ndispl\n\n\ncat\nf32\n\n\n\n\n\"Rear-Wheel Dri‚Ä¶\n2.0\n\n\n‚Ä¶\n‚Ä¶\n\n\n\"4-Wheel or All‚Ä¶\n2.2\n\n\n\n\n\nThe possibilities here are incredibly vast! I‚Äôm pretty sure you‚Äôll find a function that suits your needs just right."
  },
  {
    "objectID": "posts/polars/Polars_revised.html#basic-operations",
    "href": "posts/polars/Polars_revised.html#basic-operations",
    "title": "Getting Started with Polars",
    "section": "Basic operations",
    "text": "Basic operations\n\n\nCode\n(\n    df.select(\n        pl.col(\"make\").cast(pl.Utf8).str.contains(\"Hyundai|Kia\").alias(\"contains\"),\n        pl.col(\"make\").cast(pl.Utf8).str.starts_with(\"Alfa\").alias(\"starts_with\"),\n        pl.col(\"make\").cast(pl.Utf8).str.ends_with(\"ari\").alias(\"ends_with\"),\n    )\n)\n\n\n\nshape: (41_144, 3)\n\n\n\ncontains\nstarts_with\nends_with\n\n\nbool\nbool\nbool\n\n\n\n\nfalse\ntrue\nfalse\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\nfalse\nfalse\nfalse\n\n\n\n\n\n\nWe can extract the numbers from the different car models:\n\n\nCode\n(\n    df.select(\n        pl.col(\"model\")\n        .cast(pl.Utf8)\n        .str.extract(r\"(\\d+)\", group_index=1)\n        .cast(pl.Int32)\n        .alias(\"extracted_number\"),\n    )\n)\n\n\n\n\nshape: (41_144, 1)\n\n\n\nextracted_number\n\n\ni32\n\n\n\n\n2000\n\n\n‚Ä¶\n\n\nnull\n\n\n\n\n\nAs per usual, we can replace values in a given column:\n\n\nCode\n(\n    df.select(\n        pl.col(\"make\").cast(pl.Utf8).str.replace(\"r\", 0).alias(\"replaced\"),\n    )\n)\n\n\n\n\nshape: (41_144, 1)\n\n\n\nreplaced\n\n\nstr\n\n\n\n\n\"Alfa Romeo\"\n\n\n‚Ä¶\n\n\n\"Suba0u\""
  },
  {
    "objectID": "posts/polars/Polars_revised.html#changing-column-names",
    "href": "posts/polars/Polars_revised.html#changing-column-names",
    "title": "Getting Started with Polars",
    "section": "Changing column names",
    "text": "Changing column names\nAltering column names is quite reminiscent of the process in Pandas:\n\n\nCode\ndf.rename({\"make\": \"car maker\", \"model\": \"car model\"})\n\n\n\nshape: (41_144, 11)\n\n\n\ncar maker\ncar model\ndrive\ncylinders\ndispl\nfuelType\nmpgData\ncity08\nhighway08\ncomb08\nyear\n\n\ncat\ncat\ncat\ni8\nf32\ncat\nbool\ni8\ni8\ni8\ndate\n\n\n\n\n\"Alfa Romeo\"\n\"Spider Veloce ‚Ä¶\n\"Rear-Wheel Dri‚Ä¶\n4\n2.0\n\"Regular\"\ntrue\n19\n25\n21\n1985-01-01\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\"Subaru\"\n\"Legacy AWD Tur‚Ä¶\n\"4-Wheel or All‚Ä¶\n4\n2.2\n\"Premium\"\nfalse\n16\n21\n18\n1993-01-01\n\n\n\n\n\n\nIn case you would like to alter multiple column names all at once:\n\n\nCode\ndf.select(pl.all().map_alias(lambda name: name.upper().replace(\"I\", \"0000\")))\n\n\n\nshape: (41_144, 11)\n\n\n\nMAKE\nMODEL\nDR0000VE\nCYL0000NDERS\nD0000SPL\nFUELTYPE\nMPGDATA\nC0000TY08\nH0000GHWAY08\nCOMB08\nYEAR\n\n\ncat\ncat\ncat\ni8\nf32\ncat\nbool\ni8\ni8\ni8\ndate\n\n\n\n\n\"Alfa Romeo\"\n\"Spider Veloce ‚Ä¶\n\"Rear-Wheel Dri‚Ä¶\n4\n2.0\n\"Regular\"\ntrue\n19\n25\n21\n1985-01-01\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\"Subaru\"\n\"Legacy AWD Tur‚Ä¶\n\"4-Wheel or All‚Ä¶\n4\n2.2\n\"Premium\"\nfalse\n16\n21\n18\n1993-01-01\n\n\n\n\n\n\n\nstr.split with expand='all'\nI often find myself using the str.split method, and it caught me off guard that there isn‚Äôt a direct equivalent in Polars. Fingers crossed, we might come across an implementation like expand=True from Pandas, which would be a real game-changer here too!\n\n\nCode\n(\n    df.select(\n        pl.col(\"drive\")\n        .cast(pl.Utf8)\n        .str.split_exact(\"-\", n=1)\n        .struct.rename_fields([\"first_part\", \"second_part\"])\n        .alias(\"fields\"),\n    ).unnest(\"fields\")\n)\n\n\n\n\nshape: (41_144, 2)\n\n\n\nfirst_part\nsecond_part\n\n\nstr\nstr\n\n\n\n\n\"Rear\"\n\"Wheel Drive\"\n\n\n‚Ä¶\n‚Ä¶\n\n\n\"4\"\n\"Wheel or All\""
  },
  {
    "objectID": "posts/polars/Polars_revised.html#a-simple-split-apply-combine-operation",
    "href": "posts/polars/Polars_revised.html#a-simple-split-apply-combine-operation",
    "title": "Getting Started with Polars",
    "section": "A simple split-apply-combine operation",
    "text": "A simple split-apply-combine operation\n\n\nCode\n(df.groupby(\"make\").count().sort(by=\"count\", descending=True).limit(5))\n\n\n\n\nshape: (5, 2)\n\n\n\nmake\ncount\n\n\ncat\nu32\n\n\n\n\n\"Chevrolet\"\n4003\n\n\n‚Ä¶\n‚Ä¶\n\n\n\"Toyota\"\n2071"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#using-agg",
    "href": "posts/polars/Polars_revised.html#using-agg",
    "title": "Getting Started with Polars",
    "section": "Using agg",
    "text": "Using agg\nYou can use agg to calculate statistics over either a single or multiple columns all at once.\n\n\nCode\n(df.groupby(\"make\").agg(pl.count(), pl.col(\"model\")))\n\n\n\nshape: (136, 3)\n\n\n\nmake\ncount\nmodel\n\n\ncat\nu32\nlist[cat]\n\n\n\n\n\"TVR Engineerin‚Ä¶\n4\n[\"TVR 280i/350i Convertible\", \"TVR 280i/350i Coupe\", ‚Ä¶ \"TVR 280i Coupe\"]\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\"Grumman Allied‚Ä¶\n1\n[\"LLV\"]\n\n\n\n\n\n\nHere‚Äôs an illustration of how to utilize the is_not_nan method. In Polars, unknown values are represented with floating-point precision, similar to how pandas handles them. It‚Äôs important not to mix this up with missing values, which are indicated by Null in Polars. Additionally, take note of the use of limit instead of head, a concept quite akin to SQL.\n\n\nCode\n(\n    df.groupby(\"make\")\n    .agg(\n        pl.col(\"city08\").mean().alias(\"mean_cyl\"),\n        pl.col(\"displ\").mean().alias(\"mean_disp\"),\n    )\n    .filter(pl.col(\"mean_cyl\").is_not_nan())\n    .sort(by=[\"mean_cyl\", \"mean_disp\"], descending=[True, True])\n    .limit(5)\n)\n\n\n\nshape: (5, 3)\n\n\n\nmake\nmean_cyl\nmean_disp\n\n\ncat\nf64\nf64\n\n\n\n\n\"Tesla\"\n94.925373\nNaN\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\"Azure Dynamics‚Ä¶\n62.0\nNaN\n\n\n\n\n\n\nAggregated columns can be renamed immediately with the alias method.\n\n\nCode\n(\n    df.groupby(\"make\", \"fuelType\")\n    .agg(pl.col(\"comb08\").mean().alias(\"filtered\"))\n    .filter((pl.col(\"fuelType\") == \"CNG\") | (pl.col(\"fuelType\") == \"Diesel\"))\n)\n\n\n\nshape: (38, 3)\n\n\n\nmake\nfuelType\nfiltered\n\n\ncat\ncat\nf64\n\n\n\n\n\"Mazda\"\n\"Diesel\"\n29.714286\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\"Ford\"\n\"Diesel\"\n29.542857\n\n\n\n\n\n\nHere‚Äôs a scenario for performing aggregation based on the year. In this case, the ‚Äúyear‚Äù column holds the year-related data, which can be extracted using the dt.year attribute. Keep in mind that Polars doesn‚Äôt have native plotting capabilities like pandas. To visualize the data, as a final step, you can convert the dataframe to pandas and make use of its .plot method.\n\n\nCode\n(\n    df.groupby(\n        [\n            pl.col(\"year\").dt.year().alias(\"year\"),\n        ]\n    )\n    .agg([pl.col(\"comb08\").mean(), pl.col(\"city08\").mean()])\n    .sort(\n        [\n            \"year\",\n        ]\n    )\n    .to_pandas()\n    .set_index(\"year\")\n    .plot()\n)\n\n\n&lt;Axes: xlabel='year'&gt;\n\n\n\n\n\nHere‚Äôs another fantastic option for you: the groupby_dynamic feature. It‚Äôs really impressive, by the way. You can employ this when grouping based on years or any other time-related information. Additionally, you can make use of the every parameter for resampling, which closely resembles what you can do with pandas.\n\n\nCode\n(\n    df.sort(by=\"year\")\n    .groupby_dynamic(index_column=\"year\", every=\"2y\")\n    .agg([pl.col(\"comb08\").mean(), pl.col(\"city08\").mean()])\n    .sort(\n        [\n            \"year\",\n        ]\n    )\n    .to_pandas()\n    .set_index(\"year\")\n    .plot()\n)\n\n\n&lt;Axes: xlabel='year'&gt;"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#pivot",
    "href": "posts/polars/Polars_revised.html#pivot",
    "title": "Getting Started with Polars",
    "section": "Pivot",
    "text": "Pivot\nYou can also accomplish split-apply-combine tasks seamlessly using the pivot function. It‚Äôs remarkably straightforward.\n\n\nCode\n(\n    df.with_columns(\n        pl.col(\"fuelType\").cast(pl.Utf8)\n    )  # conversion to str is needed for the next step\n    .filter(pl.col(\"fuelType\").is_in([\"Regular\", \"Premium\"]))\n    .pivot(values=\"city08\", index=\"make\", columns=\"fuelType\", aggregate_function=\"mean\")\n)\n\n\n\nshape: (126, 3)\n\n\n\nmake\nRegular\nPremium\n\n\ncat\nf64\nf64\n\n\n\n\n\"Alfa Romeo\"\n17.142857\n18.902439\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\"PAS Inc - GMC\"\nnull\n14.0"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#getting-missing-value-count-per-column",
    "href": "posts/polars/Polars_revised.html#getting-missing-value-count-per-column",
    "title": "Getting Started with Polars",
    "section": "Getting missing value count per column",
    "text": "Getting missing value count per column\nThe null_count function showcases the count of missing values in the dataframe. Remember, these are not the same as np.nans; it‚Äôs important to be aware of this distinction. This functionality is akin to the pandas df.isna().sum() operation if you‚Äôre familiar with pandas.\n\n\nCode\ndf.null_count()\n\n\n\nshape: (1, 11)\n\n\n\nmake\nmodel\ndrive\ncylinders\ndispl\nfuelType\nmpgData\ncity08\nhighway08\ncomb08\nyear\n\n\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\n\n\n\n\n0\n0\n1189\n206\n204\n0\n0\n27\n0\n7\n0"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#getting-boolean-array-of-missing-values",
    "href": "posts/polars/Polars_revised.html#getting-boolean-array-of-missing-values",
    "title": "Getting Started with Polars",
    "section": "Getting boolean array of missing values",
    "text": "Getting boolean array of missing values\nIf you ever need to manipulate a series based on its null values, you can also obtain a boolean mask using the is_null function. This can be really handy for targeted data manipulation.\n\n\nCode\ndf.select(pl.col(\"city08\").is_null())\n\n\n\n\nshape: (41_144, 1)\n\n\n\ncity08\n\n\nbool\n\n\n\n\nfalse\n\n\n‚Ä¶\n\n\nfalse"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#filling-missing-values",
    "href": "posts/polars/Polars_revised.html#filling-missing-values",
    "title": "Getting Started with Polars",
    "section": "Filling missing values",
    "text": "Filling missing values\nYou have a variety of options at your disposal for filling in missing values. Here‚Äôs a list of some of the most common ones, although it‚Äôs not an exhaustive rundown.\n\nWith literals\n\n\nCode\n(df.with_columns(pl.col(\"cylinders\").fill_null(pl.lit(2))))\n\n\n\nshape: (41_144, 11)\n\n\n\nmake\nmodel\ndrive\ncylinders\ndispl\nfuelType\nmpgData\ncity08\nhighway08\ncomb08\nyear\n\n\ncat\ncat\ncat\ni8\nf32\ncat\nbool\ni8\ni8\ni8\ndate\n\n\n\n\n\"Alfa Romeo\"\n\"Spider Veloce ‚Ä¶\n\"Rear-Wheel Dri‚Ä¶\n4\n2.0\n\"Regular\"\ntrue\n19\n25\n21\n1985-01-01\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\"Subaru\"\n\"Legacy AWD Tur‚Ä¶\n\"4-Wheel or All‚Ä¶\n4\n2.2\n\"Premium\"\nfalse\n16\n21\n18\n1993-01-01\n\n\n\n\n\n\n\n\nCode\n(df.with_columns(pl.col(\"cylinders\").fill_null(strategy=\"zero\")))\n\n\n\nshape: (41_144, 11)\n\n\n\nmake\nmodel\ndrive\ncylinders\ndispl\nfuelType\nmpgData\ncity08\nhighway08\ncomb08\nyear\n\n\ncat\ncat\ncat\ni8\nf32\ncat\nbool\ni8\ni8\ni8\ndate\n\n\n\n\n\"Alfa Romeo\"\n\"Spider Veloce ‚Ä¶\n\"Rear-Wheel Dri‚Ä¶\n4\n2.0\n\"Regular\"\ntrue\n19\n25\n21\n1985-01-01\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\"Subaru\"\n\"Legacy AWD Tur‚Ä¶\n\"4-Wheel or All‚Ä¶\n4\n2.2\n\"Premium\"\nfalse\n16\n21\n18\n1993-01-01\n\n\n\n\n\n\n\n\nCode\n(df.with_columns(pl.col(\"cylinders\").fill_null(strategy=\"forward\")))\n\n\n\nshape: (41_144, 11)\n\n\n\nmake\nmodel\ndrive\ncylinders\ndispl\nfuelType\nmpgData\ncity08\nhighway08\ncomb08\nyear\n\n\ncat\ncat\ncat\ni8\nf32\ncat\nbool\ni8\ni8\ni8\ndate\n\n\n\n\n\"Alfa Romeo\"\n\"Spider Veloce ‚Ä¶\n\"Rear-Wheel Dri‚Ä¶\n4\n2.0\n\"Regular\"\ntrue\n19\n25\n21\n1985-01-01\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\"Subaru\"\n\"Legacy AWD Tur‚Ä¶\n\"4-Wheel or All‚Ä¶\n4\n2.2\n\"Premium\"\nfalse\n16\n21\n18\n1993-01-01\n\n\n\n\n\n\n\n\nWith an expression\nThis can be very useful for machine learning pipelines. When addressing missing values, you can set them to the mode, median, mean, or any other value that suits your needs.\n\n\nCode\n(df.with_columns(pl.col(\"cylinders\").fill_null(pl.col(\"cylinders\").mode())))\n\n\n\nshape: (41_144, 11)\n\n\n\nmake\nmodel\ndrive\ncylinders\ndispl\nfuelType\nmpgData\ncity08\nhighway08\ncomb08\nyear\n\n\ncat\ncat\ncat\ni8\nf32\ncat\nbool\ni8\ni8\ni8\ndate\n\n\n\n\n\"Alfa Romeo\"\n\"Spider Veloce ‚Ä¶\n\"Rear-Wheel Dri‚Ä¶\n4\n2.0\n\"Regular\"\ntrue\n19\n25\n21\n1985-01-01\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\"Subaru\"\n\"Legacy AWD Tur‚Ä¶\n\"4-Wheel or All‚Ä¶\n4\n2.2\n\"Premium\"\nfalse\n16\n21\n18\n1993-01-01\n\n\n\n\n\n\n\n\nUsing the over function\nThe over function in Polars is used to perform window operations, akin to what you‚Äôd achieve with window functions in SQL or the transform function in pandas. This function enables you to compute aggregations over a specified window or range of rows defined by a window specification. It‚Äôs perfect for tasks like calculating rolling averages, cumulative sums, and other operations that involve working with a specific subset of rows within the dataset.\nLet‚Äôs begin by filtering the car makes to encompass only the top 5 car brands. Following that, we can utilize the over function to derive several statistics.\n\n\nCode\ntop5 = (\n    df.select(pl.col(\"make\"))\n    .to_series()\n    .value_counts()\n    .sort(by=\"counts\", descending=True)\n    .limit(3)\n    .select(pl.col(\"make\"))\n    .to_series()\n)\n\n(\n    df.filter(pl.col(\"make\").is_in(top5)).select(\n        \"make\",\n        \"model\",\n        pl.col(\"city08\").mean().over(\"make\").alias(\"avg_city_mpg_by_make\"),\n        pl.col(\"city08\").mean().over(\"model\").alias(\"avg_city_mpg_by_model\"),\n        pl.col(\"comb08\").mean().over([\"make\", \"model\"]).alias(\"avg_comb_mpg_by_model\"),\n    )\n)\n\n\n\nshape: (9_957, 5)\n\n\n\nmake\nmodel\navg_city_mpg_by_make\navg_city_mpg_by_model\navg_comb_mpg_by_model\n\n\ncat\ncat\nf64\nf64\nf64\n\n\n\n\n\"Dodge\"\n\"Charger\"\n15.462253\n18.197531\n21.320988\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\"Dodge\"\n\"B150/B250 Wago‚Ä¶\n15.462253\n11.654321\n12.925926"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#operating-on-multiple-columns-and-renaming-them",
    "href": "posts/polars/Polars_revised.html#operating-on-multiple-columns-and-renaming-them",
    "title": "Getting Started with Polars",
    "section": "Operating on multiple columns and renaming them",
    "text": "Operating on multiple columns and renaming them\nYou have the flexibility to conduct column operations and then easily rename them. Additionally, you can make use of the prefix and suffix functions to streamline your workflow.\n\n\nCode\n(df.with_columns((cs.numeric() * 2).prefix(\"changed_\")))\n\n\n\nshape: (41_144, 16)\n\n\n\nmake\nmodel\ndrive\ncylinders\ndispl\nfuelType\nmpgData\ncity08\nhighway08\ncomb08\nyear\nchanged_cylinders\nchanged_displ\nchanged_city08\nchanged_highway08\nchanged_comb08\n\n\ncat\ncat\ncat\ni8\nf32\ncat\nbool\ni8\ni8\ni8\ndate\ni8\nf32\ni8\ni8\ni8\n\n\n\n\n\"Alfa Romeo\"\n\"Spider Veloce ‚Ä¶\n\"Rear-Wheel Dri‚Ä¶\n4\n2.0\n\"Regular\"\ntrue\n19\n25\n21\n1985-01-01\n8\n4.0\n38\n50\n42\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\"Subaru\"\n\"Legacy AWD Tur‚Ä¶\n\"4-Wheel or All‚Ä¶\n4\n2.2\n\"Premium\"\nfalse\n16\n21\n18\n1993-01-01\n8\n4.4\n32\n42\n36"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#count-unique-values",
    "href": "posts/polars/Polars_revised.html#count-unique-values",
    "title": "Getting Started with Polars",
    "section": "Count unique values",
    "text": "Count unique values\nAbsolutely, you can also count the unique values within a series in Polars, much like you would with the unique function in pandas. This can be really useful for understanding the distribution and diversity of data within a specific column.\n\n\nCode\ndf.select(pl.col(\"make\")).n_unique()\n\n\n136"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#value_counts-to-get-number-of-occurrences-per-item",
    "href": "posts/polars/Polars_revised.html#value_counts-to-get-number-of-occurrences-per-item",
    "title": "Getting Started with Polars",
    "section": "Value_counts to get number of occurrences per item",
    "text": "Value_counts to get number of occurrences per item\n\nOn a single column\n\n\nCode\n(df.select(pl.col(\"make\")).to_series().value_counts(sort=True))\n\n\n\n\nshape: (136, 2)\n\n\n\nmake\ncounts\n\n\ncat\nu32\n\n\n\n\n\"Chevrolet\"\n4003\n\n\n‚Ä¶\n‚Ä¶\n\n\n\"Isis Imports L‚Ä¶\n1\n\n\n\n\n\n\n\nCode\n(df.select(pl.col(\"make\")).groupby(\"make\").count().sort(by=\"count\", descending=True))\n\n\n\n\nshape: (136, 2)\n\n\n\nmake\ncount\n\n\ncat\nu32\n\n\n\n\n\"Chevrolet\"\n4003\n\n\n‚Ä¶\n‚Ä¶\n\n\n\"Panoz Auto-Dev‚Ä¶\n1\n\n\n\n\n\n\n\nOn multiple columns\nI noticed that Polars lacks the capability to perform value counts on multiple columns, unlike pandas‚Äô value_counts function which operates only on series. However, I‚Äôve discovered that combining a groupby operation with a count function can effectively achieve the same outcome for multiple columns. It‚Äôs all about finding alternative approaches that get the job done!\n\n\nCode\n(\n    df.select(pl.col(\"make\", \"model\"))\n    .groupby([\"make\", \"model\"])\n    .count()\n    .sort(by=\"count\", descending=True)\n)\n\n\n\n\nshape: (4_127, 3)\n\n\n\nmake\nmodel\ncount\n\n\ncat\ncat\nu32\n\n\n\n\n\"Ford\"\n\"F150 Pickup 2W‚Ä¶\n214\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\"Mercedes-Benz\"\n\"400SE\"\n1"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#conditionals-if-else-statements",
    "href": "posts/polars/Polars_revised.html#conditionals-if-else-statements",
    "title": "Getting Started with Polars",
    "section": "Conditionals/ if-else statements",
    "text": "Conditionals/ if-else statements\nIf you‚Äôre looking to integrate conditional if-else statements into your Polars chain, you can make use of the when, then, and otherwise functions. However, keep in mind that a series of chained when, then statements should be interpreted as if, elif, ‚Ä¶ elif, rather than if, if, ‚Ä¶ if. In other words, the first condition that evaluates to True will be selected. It‚Äôs crucial to understand this distinction when crafting your conditions.\n\n\nCode\n(\n    df.select(\n        pl.col(\"make\"),\n        pl.when(pl.col(\"comb08\") &lt; 15)\n        .then(\"bad\")\n        .when(pl.col(\"comb08\") &lt; 30)\n        .then(\"good\")\n        .otherwise(\"very good\")\n        .alias(\"fuel_economy\"),\n    )\n)\n\n\n\n\nshape: (41_144, 2)\n\n\n\nmake\nfuel_economy\n\n\ncat\nstr\n\n\n\n\n\"Alfa Romeo\"\n\"good\"\n\n\n‚Ä¶\n‚Ä¶\n\n\n\"Subaru\"\n\"good\"\n\n\n\n\n\nAlright, folks, that‚Äôs it for this introduction to Polars. I know it wasn‚Äôt exactly short, but I hope it was informative! I‚Äôll be adding more content to this article in the future, so stay tuned. Until then, happy coding üòéüíªüîé"
  },
  {
    "objectID": "posts/Top 10 things I learned from Effective Pandas/2023_08_02-Top_10_things_I_learned_from_Effective_Pandas.html",
    "href": "posts/Top 10 things I learned from Effective Pandas/2023_08_02-Top_10_things_I_learned_from_Effective_Pandas.html",
    "title": "Top 10 things I learned from the book Effective Pandas by Matt Harrison",
    "section": "",
    "text": "Effective Pandas by Matt Harrison is a guide to the Pandas library, a powerful Python tool for data manipulation and analysis. The book covers a wide range of topics, from basic data structures to advanced techniques for data cleaning, transformation, and visualization.\nI have found Effective Pandas to be a captivating and thought-provoking read. The book offers a genuinely unique take on data wrangling, putting a great emphasis on the utility of chaining methods and utilizing the lambda function. I have found these ideas to be so useful and practical that I have revisited the book multiple times just to make sure I keep them fresh in my memory. I must have read the book from back to back at least 3-4 times.\nIn this article, I will share the top 10 (+1) things I learned from Effective Pandas. These are the concepts and techniques that I found to be the most useful and practical.\nWe will use the Real World Smartphone‚Äôs Dataset by Abhijit Dahatonde from Kaggle. Let‚Äôs get to it.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\npd.set_option('display.max_columns', None)\n\n\n\nLoad the dataset\n\n\nCode\ndf=pd.read_csv('smartphones.csv')\n# some info about the dataframe, such as dimensions and dtypes of columns\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 980 entries, 0 to 979\nData columns (total 22 columns):\n #   Column                     Non-Null Count  Dtype  \n---  ------                     --------------  -----  \n 0   brand_name                 980 non-null    object \n 1   model                      980 non-null    object \n 2   price                      980 non-null    int64  \n 3   avg_rating                 879 non-null    float64\n 4   5G_or_not                  980 non-null    int64  \n 5   processor_brand            960 non-null    object \n 6   num_cores                  974 non-null    float64\n 7   processor_speed            938 non-null    float64\n 8   battery_capacity           969 non-null    float64\n 9   fast_charging_available    980 non-null    int64  \n 10  fast_charging              769 non-null    float64\n 11  ram_capacity               980 non-null    int64  \n 12  internal_memory            980 non-null    int64  \n 13  screen_size                980 non-null    float64\n 14  refresh_rate               980 non-null    int64  \n 15  num_rear_cameras           980 non-null    int64  \n 16  os                         966 non-null    object \n 17  primary_camera_rear        980 non-null    float64\n 18  primary_camera_front       975 non-null    float64\n 19  extended_memory_available  980 non-null    int64  \n 20  resolution_height          980 non-null    int64  \n 21  resolution_width           980 non-null    int64  \ndtypes: float64(8), int64(10), object(4)\nmemory usage: 168.6+ KB\n\n\n\n\nTip #1: Use pd.assign more extensively\nThe pd.assign method in Pandas is a very powerful tool that can be used to create new columns, modify existing columns, or both. It is a very versatile method that can be used in a variety of ways.\nOne of the most important benefits of using the assign method is that it can be incorporated into method chaining. This means that you can chain multiple assign methods together to create a more concise and readable code. Another benefit of using the assign method is that it completely sidesteps the infamous SettingWithCopyWarning. This warning is often triggered when you try to modify an existing column in a DataFrame. However, the assign method creates a new DataFrame, so there is no need to worry about this warning.\nProblem statement: Let‚Äôs say we would like to capitalize the brand names located in the brand_name column as well as calculate the Pixels Per Inch (PPI). PPI can be calculated following the equation described by the Pixel density page on Wikipedia.\n\n\nCode\n(df\n .assign(brand_name=lambda df: df.brand_name.str.capitalize(), # capitalizes the brand names \n         PPI=lambda df: (np.sqrt(np.square(df.resolution_height) + np.square(df.resolution_width))\n                         .div(df.screen_size)\n                         .round(1)\n                        )\n        )\n .loc[:, ['brand_name','model','PPI']]\n .sort_values(by='PPI',ascending=False)\n .head(5)\n)\n\n\n\n\n\n\n\n\n\nbrand_name\nmodel\nPPI\n\n\n\n\n689\nSony\nSony Xperia 1 IV (12GB RAM + 512GB)\n642.6\n\n\n696\nSony\nSony Xperia Pro-I\n642.6\n\n\n688\nSony\nSony Xperia 1 II\n642.6\n\n\n655\nSamsung\nSamsung Galaxy S20\n566.0\n\n\n656\nSamsung\nSamsung Galaxy S20 5G\n566.0\n\n\n\n\n\n\n\n\n\nTip #2: Simplify the management of multiple if-else conditions using np.select\nIf our goal is to incorporate if-else logic seamlessly into our code, we can effortlessly achieve this using either pd.mask or pd.where. Yet, what approach should we adopt when we need to evaluate multiple conditions instead of just two? In such situations, we have two options: we can either employ successive pd.mask or pd.where calls, or we can take advantage of the np.select function as an alternative solution.\nProblem statement: We want to identify the top 3 and top 5 most popular processor brands in smartphones. To do this, we will first create two lists, one for the top 3 brands and one for the top 5 brands. Any processor brand that is not in either of these lists will be categorized as ‚ÄúOther‚Äù.\n\n\nCode\n# Let's create the two lists that contain the top3 and top5 brand names\ntop3=df.processor_brand.value_counts().head(3).index\ntop5=df.processor_brand.value_counts().head(5).index\nprint(f'Top 3 most popular processors: {top3.tolist()}')\nprint(f'Top 5 most popular processors: {top5.tolist()}')\n\n\nTop 3 most popular processors: ['snapdragon', 'helio', 'dimensity']\nTop 5 most popular processors: ['snapdragon', 'helio', 'dimensity', 'exynos', 'bionic']\n\n\n\n\nCode\n'''\nHere's an example that employs two successive pd.where calls:\nIn the first pd.where call, it checks whether the brand is in the top 3; if not, it assigns the label \"Top5\" to it.\nThen, in the second call, it checks if the value is in the top 5; if not, it appends the category \"Other\".\nAs you can see, the logic can become intricate and difficult to grasp, especially when dealing with numerous conditions, \nmaking the code cumbersome and hard to manage.\n'''\n(df\n .assign(frequency=lambda df: df.processor_brand\n         .where(df.processor_brand.isin(top3), other = 'Top5')\n         .where(df.processor_brand.isin(top5), other = 'Other')\n        )\n .loc[:, 'frequency']\n .value_counts()\n)\n\n\nsnapdragon    413\nhelio         201\ndimensity     177\nTop5           95\nOther          94\nName: frequency, dtype: int64\n\n\n\n\nCode\n'''\n Now Let's see np.select!\n It simplifies the process significantly. By providing a list of conditions we want to evaluate and their \n corresponding values if the condition evaluates to True, we can handle multiple conditions effortlessly. \n Additionally, we can specify a default value if none of the conditions evaluates to True, making the code \n much more straightforward and easier to manage. \n'''\n(df\n .assign(frequency=lambda df: np.select(condlist=[df.processor_brand.isin(top3), df.processor_brand.isin(top5)],\n                                        choicelist=[df.processor_brand,'Top5'],\n                                        default='Other'\n                                       )\n        )\n .loc[:, 'frequency']\n .value_counts()\n)\n\n\nsnapdragon    413\nhelio         201\ndimensity     177\nTop5           95\nOther          94\nName: frequency, dtype: int64\n\n\n\n\nTip #3 Filter rows or columns with the combination of pd.loc and lambda\nSome experienced Pandas users might consider the following concept trivial, but it was an eye-opener for me after reading the book. It turns out that combining pd.loc and lambda (or any custom functions) allows us to filter both rows and columns, depending on our specific needs.\nProblem statement: We are interested in identifying phones with a battery capacity greater than 5000mAh.\n\n\nCode\n(df\n .loc[lambda df: df.battery_capacity.gt(5000),['model', 'battery_capacity']] # here we use pd.gt() to select values greater than 5000\n .sort_values(by='battery_capacity')\n)\n\n\n\n\n\n\n\n\n\nmodel\nbattery_capacity\n\n\n\n\n70\nGoogle Pixel 6 Pro (12GB RAM + 256GB)\n5003.0\n\n\n69\nGoogle Pixel 6 Pro\n5003.0\n\n\n977\nXiaomi Redmi Note 9 Pro Max\n5020.0\n\n\n922\nXiaomi Redmi Note 10 Lite\n5020.0\n\n\n923\nXiaomi Redmi Note 10 Lite (4GB RAM + 128GB)\n5020.0\n\n\n...\n...\n...\n\n\n624\nSamsung Galaxy F63\n7000.0\n\n\n411\nOukitel WP9\n8000.0\n\n\n410\nOukitel WP21\n9800.0\n\n\n409\nOukitel WP19\n21000.0\n\n\n58\nDoogee V Max\n22000.0\n\n\n\n\n113 rows √ó 2 columns\n\n\n\n\n\nTip #4 Rename multiple columns effortlessly with rename and replace\nOK. This is a big one for me. I used this one multiple times already. The title pretty much says it all. Let‚Äôs say our column names contain spaces, which makes column selection by attribute access pretty much impossible. Now, what do we do? Well‚Ä¶Let‚Äôs see.\nProblem statement: We would like to remove all the underscores from our column names.\n\n\nCode\ndf.columns # original column names for reference\n\n\nIndex(['brand_name', 'model', 'price', 'avg_rating', '5G_or_not',\n       'processor_brand', 'num_cores', 'processor_speed', 'battery_capacity',\n       'fast_charging_available', 'fast_charging', 'ram_capacity',\n       'internal_memory', 'screen_size', 'refresh_rate', 'num_rear_cameras',\n       'os', 'primary_camera_rear', 'primary_camera_front',\n       'extended_memory_available', 'resolution_height', 'resolution_width'],\n      dtype='object')\n\n\n\n\nCode\n# column names after replacing underscores\n(df\n .rename(columns = lambda x: x.replace('_', ''))\n .columns\n)\n\n\nIndex(['brandname', 'model', 'price', 'avgrating', '5Gornot', 'processorbrand',\n       'numcores', 'processorspeed', 'batterycapacity',\n       'fastchargingavailable', 'fastcharging', 'ramcapacity',\n       'internalmemory', 'screensize', 'refreshrate', 'numrearcameras', 'os',\n       'primarycamerarear', 'primarycamerafront', 'extendedmemoryavailable',\n       'resolutionheight', 'resolutionwidth'],\n      dtype='object')\n\n\n\n\nTip #5: Use pd.clip to easily remove outliers\n\n\nCode\n# First, we'll identify the phone brands with the most number of handsets present in our dataset.\"\ntop10_brand_names = (df\n                     .brand_name\n                     .value_counts()\n                     .head(10)\n                     .index\n                     .tolist()\n                    )\nprint(top10_brand_names)\n\n# Then we will sort them based on median price\ntop10_brand_names_ordered = (df\n                             .loc[lambda x: x.brand_name.isin(top10_brand_names),['brand_name', 'price']]\n                             .groupby('brand_name')\n                             .median()\n                             .sort_values(by='price')\n                             .index\n                             .to_list()\n                            )\nprint(top10_brand_names_ordered)\n\n\n['xiaomi', 'samsung', 'vivo', 'realme', 'oppo', 'motorola', 'apple', 'oneplus', 'poco', 'tecno']\n['tecno', 'poco', 'realme', 'xiaomi', 'motorola', 'vivo', 'oppo', 'samsung', 'oneplus', 'apple']\n\n\n\n\nCode\nfig, axs = plt.subplots(1,2, figsize=(10,5))\n\n# For reference, this is what our box plot looks if we leave in the outlier values\n(df\n .loc[lambda x: x.brand_name.isin(top10_brand_names),['brand_name', 'price']] # filter rows based on top10_brand_names and select columns\n .pivot(columns='brand_name',values='price') # pivot to get the brand names on the x axis later on\n .loc[:, top10_brand_names_ordered] # order the columns based on median price\n .plot\n .box(title='Top 10 most popular smartphone brands \\n -outlier values included-', \n      rot=90,\n      ax=axs[0]\n     )\n)\n\n(df\n .loc[lambda x: x['brand_name'].isin(top10_brand_names), ['brand_name', 'price']]\n .pivot(columns='brand_name', values='price')\n .loc[:, top10_brand_names_ordered]\n .pipe(lambda df: df.assign(**{col : df[col].clip(lower=df[col].quantile(0.05), # this is called dictionary unpacking\n                                                  upper=df[col].quantile(0.95))\n                              for col in df.columns}))\n .plot\n .box(title='Top 10 most popular smartphone brands \\n -only values between 5th and 95th percentiles included-',\n      rot=90,\n      ax=axs[1]\n     )\n)\naxs[0].set(ylabel='Price in local currency')\nplt.tight_layout()\n\n\n\n\n\n\n\nTip #6: Find corrupted entries with str.extract\nHow often have you encountered the situation where, for some reason, a column that is expected to only contain numerical values displays object as its dtype? This often indicates the presence of some string values mixed within the column. It would be beneficial to promptly identify all the erroneous values, correct them, and proceed with our analysis smoothly. Let‚Äôs explore what we can do in such scenarios.\nProblem statement: We would like to identify any cells in a specific column that contain non-numerical values.\n\n\nCode\ndf_bad_values = df.copy(deep=True) # let's prepare a copy of the original dataframe \n\n# let's modify some of the values in the price column randomly:\ndf_bad_values.loc[np.random.randint(0, high=df_bad_values.shape[0], size=10), 'price'] = '.'\ndf_bad_values.loc[np.random.randint(0, high=df_bad_values.shape[0], size=10), 'price'] = '-'\ndf_bad_values.loc[np.random.randint(0, high=df_bad_values.shape[0], size=10), 'price'] = '*'\ndf_bad_values.info() # the modified dataframe's price column now returns object as dtype\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 980 entries, 0 to 979\nData columns (total 22 columns):\n #   Column                     Non-Null Count  Dtype  \n---  ------                     --------------  -----  \n 0   brand_name                 980 non-null    object \n 1   model                      980 non-null    object \n 2   price                      980 non-null    object \n 3   avg_rating                 879 non-null    float64\n 4   5G_or_not                  980 non-null    int64  \n 5   processor_brand            960 non-null    object \n 6   num_cores                  974 non-null    float64\n 7   processor_speed            938 non-null    float64\n 8   battery_capacity           969 non-null    float64\n 9   fast_charging_available    980 non-null    int64  \n 10  fast_charging              769 non-null    float64\n 11  ram_capacity               980 non-null    int64  \n 12  internal_memory            980 non-null    int64  \n 13  screen_size                980 non-null    float64\n 14  refresh_rate               980 non-null    int64  \n 15  num_rear_cameras           980 non-null    int64  \n 16  os                         966 non-null    object \n 17  primary_camera_rear        980 non-null    float64\n 18  primary_camera_front       975 non-null    float64\n 19  extended_memory_available  980 non-null    int64  \n 20  resolution_height          980 non-null    int64  \n 21  resolution_width           980 non-null    int64  \ndtypes: float64(8), int64(9), object(5)\nmemory usage: 168.6+ KB\n\n\n\n\nCode\n# let's find the corrupted values easily:\n(df_bad_values\n .price\n .str.extract(r'([^a-zA-Z])') # returns NON-matching alphabetical characters\n .value_counts()\n)\n\n\n*    10\n-    10\n.    10\ndtype: int64\n\n\n\n\nTip #7: Sort values based on the key parameter\nThe pd.sort_values function surprised me with its versatility. Previously, I had only used its by, axis, and ascending parameters for sorting. However, Matt‚Äôs book introduced me to its key parameter, which allows us to apply any function to sort the values. The only constraint is that the function must return a Series.\nProblem statement: For whatever reason, we would like to sort the phone model names in ascending order based on their second letter.\n\n\nCode\n(df\n .iloc[:, 1:3]\n .sort_values(by='model',\n              key = lambda x: x.str[1],\n              ascending = True\n             )\n .head(10)\n)\n\n\n\n\n\n\n\n\n\nmodel\nprice\n\n\n\n\n55\nCAT S22 Flip\n14999\n\n\n697\nTCL Ion X\n8990\n\n\n195\nLG V60 ThinQ\n79990\n\n\n196\nLG Velvet 5G\n54999\n\n\n197\nLG Wing 5G\n54999\n\n\n108\niKall Z19 Pro\n8099\n\n\n107\niKall Z19\n7999\n\n\n106\niKall Z18\n6799\n\n\n54\nBLU F91 5G\n14990\n\n\n413\nPOCO C31 (4GB RAM + 64GB)\n7499\n\n\n\n\n\n\n\n\n\nTip #8: Reference an existing variable inside pd.query with @\nThis resembles Tip #4, as it‚Äôs a technique I frequently use. Since reading Matt‚Äôs book, I have started using pd.query extensively to filter rows based on values, instead of relying on .loc or .iloc. In case you choose to adopt pd.query as well, it‚Äôs essential to be aware of its capability to use ‚Äú@‚Äù to reference variables in the environment. This feature enhances its flexibility and makes it even more convenient to apply in various data filtering scenarios.\nProblem statement: Our objective is to identify phones that meet three specific criteria: being priced below the average market price, having more processor cores than the average, and possessing a battery capacity greater than the average.\n\n\nCode\naverage_price=df.price.mean()\naverage_cores=df.num_cores.mean()\naverage_battery=df.battery_capacity.mean()\n\n(df\n .query(\"(price &lt;= @average_price) and (num_cores &gt;= @average_cores) and (battery_capacity &gt;= @average_battery)\")\n .iloc[:,:3]\n .sort_values(by='price')\n .head()\n)\n\n\n\n\n\n\n\n\n\nbrand_name\nmodel\nprice\n\n\n\n\n498\nrealme\nRealme C30\n5299\n\n\n179\nitel\nitel Vision 3 (2GB RAM + 32GB)\n5785\n\n\n202\nmicromax\nMicromax IN 2C\n5999\n\n\n729\ntecno\nTecno Spark Go 2022\n6249\n\n\n499\nrealme\nRealme C30 (3GB RAM + 32GB)\n6299\n\n\n\n\n\n\n\n\n\nTip #9: Gain more insights by using style.background_gradient\nBeing a visual creature, I often struggle to comprehend data quickly just by examining the raw table alone. Fortunately, with the help of style.background_gradient, similar to the heatmap function in the seaborn library, we can represent cells, in terms of color gradient, based on their values. This enables us to identify trends and patterns in our data swiftly, making data analysis more intuitive and insightful.\nProblem statement: Our goal is to identify the overall trends related to key descriptors found among the Top 10 smartphone brands, aiming to determine which brand offers the most value for our money.\n\n\nCode\n(df\n .query(\"brand_name.isin(@top10_brand_names_ordered)\") # filter rows based on top10 brands\n .groupby(['brand_name',])\n [['avg_rating', 'processor_speed', 'ram_capacity', \n   'screen_size', 'battery_capacity', 'price']]\n .mean()\n .sort_values(by='avg_rating')\n .transpose()\n .rename(columns=str.capitalize) # capitalize brand names\n .style\n .set_caption(\"Key descriptors of the Top 10 smartphone brands\")\n .format(precision=1)\n .background_gradient(cmap = 'vlag', axis = 1)\n .set_table_styles([\n    {'selector': 'td', 'props': 'text-align: center;'},\n     {'selector': 'caption','props': 'font-size:1.5em; font-weight:bold;'}\n     ,]\n )\n)\n\n\n\n\n\nKey descriptors of the Top 10 smartphone brands\n\n\nbrand_name\nTecno\nRealme\nApple\nVivo\nPoco\nSamsung\nOppo\nXiaomi\nMotorola\nOneplus\n\n\n\n\navg_rating\n7.4\n7.6\n7.7\n7.7\n7.9\n7.9\n7.9\n7.9\n8.0\n8.2\n\n\nprocessor_speed\n2.1\n2.3\n3.1\n2.4\n2.5\n2.4\n2.5\n2.4\n2.5\n2.7\n\n\nram_capacity\n5.4\n5.7\n5.3\n6.7\n6.1\n6.5\n7.5\n6.4\n6.1\n8.2\n\n\nscreen_size\n6.7\n6.5\n6.1\n6.5\n6.6\n6.6\n6.6\n6.6\n6.6\n6.6\n\n\nbattery_capacity\n5333.9\n4903.1\n3527.2\n4703.7\n5009.4\n4917.4\n4667.2\n4957.6\n4863.1\n4759.5\n\n\nprice\n14545.4\n17461.4\n95966.5\n26782.4\n18479.2\n36843.0\n29650.0\n27961.1\n24099.9\n35858.6\n\n\n\n\n\n\n\nTip #10: Use pd.pipe to include any functions in our chain\nOne of the most valuable lessons I learned from Effective Pandas is the importance of arranging my code in a chain. Although it may feel somewhat restrictive at first, once you overcome the initial hurdles, you‚Äôll realize that your code becomes more readable and easier to understand. The need to invent unique names for temporary variables is completely eliminated, making coding a much happier experience.\nIn the chaining world, you often find yourself wanting to use various functions that are not explicitly designed for chaining. However, there‚Äôs good news! You can still achieve this. The pd.pipe function comes to the rescue, allowing you to use any function as long as it returns a Series or DataFrame. It‚Äôs a flexible solution that empowers you to seamlessly integrate different functions into your chaining workflow, making your data manipulation more efficient and enjoyable.\nProblem statement: We aim to visualize the impact of RAM capacity on user satisfaction. To achieve this, we will utilize the sns.lmplot function, which plots the data and corresponding regression models for the Top 5 phone brands.\n\n\nCode\ntop5_brand_names_ordered = df.brand_name.value_counts().head().index\n\nwith sns.axes_style(\"darkgrid\"):\n    g = (df\n         .query(\"brand_name.isin(@top5_brand_names_ordered)\") # filter rows based on top10 brands\n         [['brand_name', 'avg_rating', 'ram_capacity']]\n         .assign(brand_name=lambda df: df.brand_name.str.capitalize())\n         .rename(columns={'brand_name':'Brand name'})\n         .pipe(lambda df: sns.lmplot(data=df,\n                                     x='ram_capacity',\n                                     y='avg_rating',\n                                     hue='Brand name',\n                                     # height=4,\n                                     # aspect=1.2\n                                    )\n              )\n        )\n\n    g.set(title='Customer satisfaction correlates with RAM capacity', \n          xlabel='RAM capacity',\n          ylabel='User rating'\n         )\nplt.tight_layout()\n\n\n\n\n\n\n\nTip #10 + 1: Use the ‚Äúmargin‚Äù parameter of pd.crosstab to easily calculate row/column subtotals\nDespite primarily using the pandas groupby function for data aggregation, the pd.crosstab function has an enticing feature: the margin parameter. This option enables us to effortlessly calculate subtotals across rows and columns. Moreover, by normalizing our data, we can gain even more intuition about the questions we want to answer.\nProblem statement: Our objective is to evaluate how RAM capacity impacts user satisfaction across the Top 5 brands. Additionally, we will normalize our data to compare values comprehensively across the entire dataset.\n\n\nCode\n(df\n .query(\"brand_name.isin(@top5_brand_names_ordered)\") # filter rows based on top10 brands\n .assign(brand_name=lambda df: df.brand_name.str.capitalize())\n .pipe(lambda df: pd.crosstab(index=df['ram_capacity'],\n                              columns=df['brand_name'],\n                              values=df['avg_rating'],\n                              aggfunc='mean',\n                              margins=True,\n                              normalize='all'\n                             )\n      )\n .mul(100)\n .round(1)\n)\n\n\n\n\n\n\n\n\nbrand_name\nOppo\nRealme\nSamsung\nVivo\nXiaomi\nAll\n\n\nram_capacity\n\n\n\n\n\n\n\n\n\n\n2\n0.0\n2.7\n2.8\n2.7\n2.7\n11.5\n\n\n3\n2.9\n2.8\n2.9\n2.9\n2.8\n12.1\n\n\n4\n3.1\n3.2\n3.2\n3.2\n3.2\n13.5\n\n\n6\n3.3\n3.5\n3.5\n3.4\n3.5\n14.8\n\n\n8\n3.7\n3.7\n3.8\n3.7\n3.8\n15.7\n\n\n12\n3.8\n3.9\n3.9\n3.8\n3.9\n16.3\n\n\n16\n3.8\n0.0\n0.0\n0.0\n0.0\n16.2\n\n\nAll\n20.2\n19.6\n20.2\n19.8\n20.2\n100.0\n\n\n\n\n\n\n\nI hope this article has convinced you to pick up Matt Harrison‚Äôs Effective Pandas! There are plenty more exciting ideas in the book beyond the Top 10 I‚Äôve shared here (I didn‚Äôt even get into the fascinating time series part!). I hope you found these insights helpful and inspiring.\nHappy coding üêºüíªüöÄ"
  }
]