[
  {
    "objectID": "scientific_output.html",
    "href": "scientific_output.html",
    "title": "Scientific Output",
    "section": "",
    "text": "Publications\n\nCseresznye, A., Hardy, E., Bamai, Y. A., Cleys, P., Poma, G., Malarvannan, G., Scheepers, P., Viegas, S., Martins, C., Porras, S. P., Santonen, T., Godderis, L., Verdonck, J., Poels, K., Silva, M. J., Louro, H., Mārtiņsone, I., Akūlova, L., Van Dael, M., . . . Covaci, A. (2024). HBM4EU E-waste study: Assessing persistent organic pollutants in blood, silicone wristbands, and settled dust among E-waste recycling workers in Europe. Environmental Research, 118537. https://doi.org/10.1016/j.envres.2024.118537\nCleys, P., Hardy, E., Bamai, Y. A., Poma, G., Cseresznye, A., Malarvannan, G., Scheepers, P., Viegas, S., Porras, S. P., Santonen, T., Godderis, L., Verdonck, J., Poels, K., Martins, C., Silva, M. J., Louro, H., Mārtiņsone, I., Akūlova, L., Van Nieuwenhuyse, A., . . . Covaci, A. (2024). HBM4EU e-waste study: Occupational exposure of electronic waste workers to phthalates and DINCH in Europe. International Journal of Hygiene and Environmental Health, 255, 114286. https://doi.org/10.1016/j.ijheh.2023.114286\nYin, S., McGrath, T. J., Cseresznye, A., Bombeke, J., Poma, G., & Covaci, A. (2023). Assessment of silicone wristbands for monitoring personal exposure to chlorinated paraffins (C8-36): A pilot study. Environmental Research, 224, 115526. https://doi.org/10.1016/j.envres.2023.115526\nHuguenard, C., Cseresznye, A., Darcey, T., Nkiliza, A., Evans, J. E., Hazen, S. L., Mullan, M., Crawford, F., & Abdullah, L. (2023). Age and APOE affect L-carnitine system metabolites in the brain in the APOE-TR model. Frontiers in Aging Neuroscience, 14. https://doi.org/10.3389/fnagi.2022.1059017\nHuguenard, C., Cseresznye, A., Evans, J. E., Darcey, T., Nkiliza, A., Keegan, A. P., Luis, C. A., Bennett, D. A., Arvanitakis, Z., Yassine, H. N., Mullan, M., Crawford, F., & Abdullah, L. (2023). APOE ε4 and Alzheimer’s disease diagnosis associated differences in L-carnitine, GBB, TMAO, and acylcarnitines in blood and brain. Current Research in Translational Medicine, 71(1), 103362. https://doi.org/10.1016/j.retram.2022.103362\nNkiliza, A., Parks, M., Cseresznye, A., Oberlin, S., Evans, J. E., Darcey, T., Aenlle, K., Niedospial, D., Mullan, M., Crawford, F., Klimas, N. G., & Abdullah, L. (2021). Sex-specific plasma lipid profiles of ME/CFS patients and their association with pain, fatigue, and cognitive symptoms. Journal of Translational Medicine, 19(1). https://doi.org/10.1186/s12967-021-03035-6\nJoshi, U., Evans, J. E., Pearson, A., Saltiel, N., Cseresznye, A., Darcey, T., Ojo, J., Keegan, A. P., Oberlin, S., Mouzon, B., Paris, D., Klimas, N. G., Sullivan, K., Mullan, M., Crawford, F., & Abdullah, L. (2020). Targeting sirtuin activity with nicotinamide riboside reduces neuroinflammation in a GWI mouse model. Neurotoxicology, 79, 84–94. https://doi.org/10.1016/j.neuro.2020.04.006\nHuguenard, C., Cseresznye, A., Evans, J. E., Oberlin, S., Langlois, H., Ferguson, S., Darcey, T., Nkiliza, A., Dretsch, M. N., Mullan, M., Crawford, F., & Abdullah, L. (2020). Plasma Lipidomic Analyses in Cohorts With mTBI and/or PTSD Reveal Lipids Differentially Associated With Diagnosis and APOE ε4 Carrier Status. Frontiers in Physiology, 11. https://doi.org/10.3389/fphys.2020.00012\nJoshi, U., Pearson, A., Evans, J. E., Langlois, H., Saltiel, N., Ojo, J., Klimas, N. G., Sullivan, K., Keegan, A. P., Oberlin, S., Darcey, T., Cseresznye, A., Raya, B., Paris, D., Hammock, B. D., Vasylieva, N., Hongsibsong, S., Stern, L. J., Crawford, F., . . . Abdullah, L. (2019). A permethrin metabolite is associated with adaptive immune responses in Gulf War Illness. Brain Behavior and Immunity, 81, 545–559. https://doi.org/10.1016/j.bbi.2019.07.015\nWang, M., Palavicini, J. P., Cseresznye, A., & Han, X. (2017). Strategy for Quantitative Analysis of Isomeric Bis(monoacylglycero)phosphate and Phosphatidylglycerol Species by Shotgun Lipidomics after One-Step Methylation. Analytical Chemistry, 89(16), 8490–8495. https://doi.org/10.1021/acs.analchem.7b02058\n\n\n\nConference talks\n\nAdam Cseresznye, Fatima den Ouden, Liesa Engelen, Jasper Bombeke, Giulia Poma, Thomas J McGrath, Roger Peró Gascon, Gwen Falony, Ellen De Paepe, Lieselot Y Hemeryck, Tim S Nawrot, Jeroen Raes, Lynn Vanhaecke, Sarah De Saeger, Marthe De Boevre, Adrian Covaci. Serum Levels of Persistent Organic Pollutants in the Flemish Gut Flora Project Cohort. Dioxin, Maastricht, the Netherlands, 2023.\n\nEmilie M. Hardy, Adam Cseresznye , Paul T.J. Scheepers, Maria Torres Toda, Yu Ait Bamai, Simo Porras , Susana Viegas, Tiina Santonen, Adrian Covaci, An van Nieuwenhuyse, Radu C. Duca and HBM4EU e-waste study team. Assessment of e-waste occupational exposure to PCBs and PBDEs using settled dust and silicone wristbands analysis. Dioxin, Maastricht, the Netherlands, 2023.\n\nS. Yin, T J. McGrath, A. Cseresznye, J. Bombeke, G. Poma, A. Covaci. Assessment of Silicone Wristbands for Monitoring Personal Exposure to Chlorinated Paraffins (C8-36): a Pilot Study. Dioxin, Maastricht, the Netherlands, 2023.\n\nPaul T.J. Scheepers, Susana Viegas, Radu-Corneliu Duca, Jelle Verdonck, Emilie Hardy, Adrian Covaci, Paulien Cleys, Adam Cseresznye, Thomas Goën, Karen S. Galea, Jelle Verdonck, Katrien Poels, Lode Godderis, Elizabeth Leese, Henriqueta Louro, Maria Silva, Sophie Ndaw, Simo Porras, Selma Mahiout, Tiina Santonen. The HBM4EU e-waste study: exploratory survey of worker’s exposure to toxic contaminants. ISES Annual Meeting, Chicago, IL, USA, 2023.\n\nPaulien Cleys, Emilie Hardy, Yu Ait Bamai, Giulia Poma, Adam Cseresznye, Govindan Malarvannan, Paul T.J. Scheepers, Susana Viegas, Simo P. Porras, Tiina Santonen, Lode Godderis, Jelle Verdonck, Katrien Poels, Carla Martins, Maria João Silva, Henriqueta Louro, Inese Martinsone, Lāsma Akūlova, An van Nieuwenhuyse, Martien Graumans, Selma Mahiout, Radu Corneliu Duca, Adrian Covaci, HBM4EU e-waste study: Occupational Exposure of Electronic Waste Workers to Phthalates and Alternative Plasticizers in Europe. ISES Annual Meeting, Chicago, IL, USA, 2023.\n\nAdam Cseresznye, P. Cleys, G. Poma, Y.A. Bamai, E. Hardy, P. Scheepers, S. Viegas, S. Porras, T. Santonen, R.C. Duca, A. Covaci, and HBM4EU E-Waste study group. HBM4EU E-waste study: persistent organic pollutants in e-waste recycling workers in the European Union. ISBM-12, Porto, Portugal, 2023.\n\n\n\nPoster presentations\n\nR. Pero-Gascon, E. Maris, A. Cseresznye, F. den Ouden, L. Engelen, L.Y. Hemeryck, E. De Paepe, A. Vich Vila, S. Moossavi, M. Derrien, G. Poma, M. De Boevre, T.S. Nawrot, J. Raes, A. Covaci, L. Vanhaecke & S. De Saeger. FLEXiGUT: investigating the importance of the exposome in the development and progression of chronic low-grade gut inflammation. Benelux Metabolomics Days, Utrecht, the Netherlands, 2023\nAlicia Macan Schönleben , Thomas McGrath, Shanshan Yin, Adam Cseresznye, Alexander van Nuijs, Giulia Poma, Adrian Covaci. Chlorinated paraffins in seaweed chips – First insights using High-Resolution Mass Spectrometry. Dioxin, Maastricht, the Netherlands, 2023\n\nAurore Nkiliza, Claire Huguenard, Adam Cseresznye, Teresa Darcey, Corbin Bachmeier, James E. Evans, Andrew P. Keegan, Michael Mullan, Fiona Crawford and Laila Abdullah. The APOE e4 allele differentially influences plasma bioactive lipids in soldiers with mild TBI and PTSD, AAIC, 2020.\nHuguenard C, Cseresznye A, Darcey T, Bachmeier C, Evans J E, Cheryl L, Keegan A. P, Mullan M,Crawford F, Abdullah L. APOE4-dependent and independent changes in acylcarnitines indicate lipid metabolic changes early on in Alzheimer’s disease. AAIC, 2020.\nCseresznye A, Huguenard C, Evans JE, Crawford F, Mullan M, Abdullah L. LC/MS/HRMS analysis of short to very long chain free fatty acids following charge reversal chemical derivatization [MP 364]. ASMS conference, 2020.\nRoderick G. Davies, Sarah Diane Oberlin, Adam Cseresznye, Laila Abdullah, Michael Mullan, Fiona Crawford, Megan C Schwarz, Marion Sourisseau, James E. Evans, Matthew J. Evans. Probing the Mechanism of Zika Infection/Replication Using Lipidomics And Proteomics Analyses [MP 364]. ASMS conference, 2020.\nCseresznye A, Huguenard C, Nkiliza A, Pearson A, Evans J, Crawford F, Mullan M, Abdullah L. Targeted analysis of microglia eicosanoid species by LC/MS/HRMS using high-resolution parallel- reaction monitoring. Bioactive Lipid Conference, 2019.\nNkiliza A, Huguenard C, Cseresznye A, Evans J, Raya B, Joshi U, Darcey T, Oberlin S, Crawford F, Mullan M, Abdullah L. Pharmacokinetic study of OEA for the treatment of Gulf war illness. Bioactive Lipid Conference, 2019.\nEvans J, Joshi U, Oberlin S, Cseresznye A, Oberlin S, Pearson A, Langlois H, Darcey T, Huguenard C, Davis R, Keegan A, Crawford F, Mullan M, Abdullah L. Nicotinamide riboside improves brain bioenergetics and reduces brain neuroinflammation in mouse model of Gulf war illness. Bioactive Lipid Conference, 2019.\nHuguenard C, Cseresznye A, Oberlin S, Langlois H, Darcey T, Dretsch M, Evans J, Crawford F, Mullan M, Abdullah L. Plasma lipidomic analysis in mild TBI and PTSD identify changes in ethanolamides, acylcarnitines and other circulating lipids. Bioactive Lipid Conference, 2019.\nHuguenard C, Cseresznye A, Oberlin S, Langlois H, Darcey T, Dretsch M, Evans J, Crawford F, Mullan M, Abdullah L. Plasma lipidomics analyses for diagnostic classification of mild TBI and PTSD. Keystone Conference, 2018.\nAbdullah L, Huguenard C, Cseresznye A, Evans J, Darcey T, Bachmeier C, Luis C, Crawford F, Mullan M. APOE4 and altered acylcarnitine profiles implicate mitochondria specific lipid metabolism dysfunction in preclinical Alzhemier’s disease. Keystone Conference, 2018.\nAbdullah L, Huguenard C, Cseresznye A, Evans J, Darcey T, Bachmeier C, Luis C, Keegan A, Crawford F, Mullan M. APOE4 and altered carnitine, TMAO acylcarnitines implicate mitochondrial dysfunction in preclinical Alzhemier’s disease. Keystone Conference, 2018.\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/Quantifyer/quantifyer.html",
    "href": "projects/Quantifyer/quantifyer.html",
    "title": "Quantifyer: Simplifying Data Analysis for Targeted LC/GC-MS",
    "section": "",
    "text": "Photo generated by Dall·E 3\n\n\nQuantifyer is a software application designed to simplify and streamline data analysis for targeted mass-based LC/GC-MS applications. It enables users to quickly and accurately calculate key metrics, such as recovery, correction factor, and concentration values. While originally designed for environmental sciences, Quantifyer’s versatility extends to other fields that utilize mass-based quantitations. The application also generates informative plots that support informed decision-making and expedite turnaround times.\n\n\n\n\n\n\nNote\n\n\n\nYou can access the project’s app through its Streamlit website.\n\n\n\nIntroduction\nQuantifyer is a software application that simplifies and streamlines data analysis for targeted mass-based LC/GC-MS applications. With its user-friendly interface and automated calculations, Quantifyer eliminates the tedious and error-prone process of performing manual calculations in Excel. This feature not only saves time and effort but also ensures greater precision and reproducibility of data analysis.\nIn addition to its accuracy and efficiency, Quantifyer offers a comprehensive overview of the entire sample preparation pipeline, providing insights from extraction efficiency and correction factor calculations to final calculated concentrations. This holistic approach enables users to identify potential issues at any stage of the process and quickly pinpoint the source of any discrepancies.\nFor those who rely on third-party proprietary software for peak integration, Quantifyer serves as a valuable quality control measure. By allowing users to quickly verify their peak integration work, Quantifyer helps detect and correct errors early in the data analysis process, thereby enhancing the quality of their data.\nQuantifyer’s extensibility is built upon the strategy pattern using the open–closed principle, enabling the future integration of other quantitation techniques beyond mass-based quantitation, such as concentration-based quantitation. This modular design ensures that the software can adapt to evolving scientific methodologies and meet the needs of users as their research progresses.\nIn summary, Quantifyer is a valuable tool for researchers who utilize targeted LC/GC-MS data for their work. It simplifies data analysis, improves precision, promotes reproducibility, and provides a comprehensive overview of the sample preparation pipeline. By automating calculations, verifying peak integration, and offering extensibility, Quantifyer empowers scientists to make data-driven decisions with confidence.\n\n\nHow to use the App\n\nUpload your files here.\nSelect the metrics to calculate.\n\n\n\nGet in touch\nDid the app help with your research? Any ideas for making it better? Get in touch! I would love to hear from you."
  },
  {
    "objectID": "projects/Predicting Belgian Real Estate Prices_ Part 6_Feature_engineering/NB_6_ACs_Feature_engineering.html",
    "href": "projects/Predicting Belgian Real Estate Prices_ Part 6_Feature_engineering/NB_6_ACs_Feature_engineering.html",
    "title": "Predicting Belgian Real Estate Prices: Part 6: Feature Engineering",
    "section": "",
    "text": "Photo by Stephen Phillips - Hostreviews.co.uk on UnSplash\nIn Part 5, we looked at the significance of features in the initial scraped dataset using both the feature_importances_ method of CatBoostRegressor and SHAP values. We conducted feature elimination based on their importance and predictive capability.\nIn this upcoming phase, we’ll implement a robust cross-validation strategy to accurately and consistently evaluate our model’s performance across multiple folds of the dataWe will also i Identing and addreng potential outliers within our datas, whichet is crucial to prevent their undue influence on the model’s predictions.\nAdditionally, we’ll further refine and expand our feature engineering efforts by exploring new methodologies to create informative features that bolster our model’s predictive capabilities. Looking forward to these pivotal steps!"
  },
  {
    "objectID": "projects/Predicting Belgian Real Estate Prices_ Part 6_Feature_engineering/NB_6_ACs_Feature_engineering.html#read-in-dataframe",
    "href": "projects/Predicting Belgian Real Estate Prices_ Part 6_Feature_engineering/NB_6_ACs_Feature_engineering.html#read-in-dataframe",
    "title": "Predicting Belgian Real Estate Prices: Part 6: Feature Engineering",
    "section": "Read in dataframe",
    "text": "Read in dataframe\nDrawing from our findings in notebook 4, particularly with regards to our initial feature reduction efforts, we’ve developed a function named “prepare_data_for_modelling.” This function resides in the pre_process.py file, ensuring its reusability. The function performs essential data preprocessing steps, which include:\n\nRandomly shuffling the rows in the DataFrame.\nTransforming the ‘price’ column by taking the base 10 logarithm.\nHandling missing values in categorical variables by replacing them with ‘missing value.’\nSeparating the dataset into features (X) and the target variable (y).\n\nLet’s dive into the details of this function and prepare our X and y for the subsequent processing pipeline.\n\n\nCode\ndf = pd.read_parquet(\n    utils.Configuration.INTERIM_DATA_PATH.joinpath(\n        \"2023-10-01_Processed_dataset_for_NB_use.parquet.gzip\"\n    )\n)\n\n\n\n\nCode\ndef prepare_data(df: pd.DataFrame) -&gt; Tuple[pd.DataFrame, pd.Series]:\n    \"\"\"\n    Prepare data for machine learning modeling.\n\n    This function takes a DataFrame and prepares it for machine learning by performing the following steps:\n    1. Randomly shuffles the rows of the DataFrame.\n    2. Converts the 'price' column to the base 10 logarithm.\n    3. Fills missing values in categorical variables with 'missing value'.\n    4. Separates the features (X) and the target (y).\n\n    Parameters:\n    - df (pd.DataFrame): The input DataFrame containing the dataset.\n\n    Returns:\n    - Tuple[pd.DataFrame, pd.Series]: A tuple containing the prepared features (X) and the target (y).\n\n    Example use case:\n    # Load your dataset into a DataFrame (e.g., df)\n    df = load_data()\n\n    # Prepare the data for modeling\n    X, y = prepare_data_for_modelling(df)\n\n    # Now you can use X and y for machine learning tasks.\n    \"\"\"\n\n    processed_df = (\n        df.sample(frac=1, random_state=utils.Configuration.seed)\n        .reset_index(drop=True)\n        .assign(price=lambda df: np.log10(df.price))\n    )\n\n    # Fill missing categorical variables with \"missing value\"\n    for col in processed_df.columns:\n        if processed_df[col].dtype.name in (\"bool\", \"object\", \"category\"):\n            processed_df[col] = processed_df[col].fillna(\"missing value\")\n\n    # Separate features (X) and target (y)\n    X = processed_df.loc[:, utils.Configuration.features_to_keep_v1]\n    y = processed_df[utils.Configuration.target_col]\n\n    print(f\"Shape of X and y: {X.shape}, {y.shape}\")\n\n    return X, y\n\n\n\n\nCode\nX, y = prepare_data(df)\n\n\nShape of X and y: (3660, 16), (3660,)"
  },
  {
    "objectID": "projects/Predicting Belgian Real Estate Prices_ Part 6_Feature_engineering/NB_6_ACs_Feature_engineering.html#utilize-categorical-columns-for-grouping-and-transform-each-numerical-variable-based-on-the-median",
    "href": "projects/Predicting Belgian Real Estate Prices_ Part 6_Feature_engineering/NB_6_ACs_Feature_engineering.html#utilize-categorical-columns-for-grouping-and-transform-each-numerical-variable-based-on-the-median",
    "title": "Predicting Belgian Real Estate Prices: Part 6: Feature Engineering",
    "section": "Utilize categorical columns for grouping and transform each numerical variable based on the median",
    "text": "Utilize categorical columns for grouping and transform each numerical variable based on the median\nThe idea behind this feature engineering step is to leverage categorical columns as grouping criteria and then calculate the median value for each numerical variable within each group. By doing so, it aims to create new features that capture the central tendency of the numerical data for different categories, allowing the model to better understand and utilize the inherent patterns and variations within the data.\n\n\nCode\n# Number of unique categories per categorical variables:\n\nX_wo_outliers.select_dtypes(\"object\").nunique()\n\n\nstate                   9\nkitchen_type            9\nstreet                456\nbuilding_condition      7\ncity                  230\ndtype: int64\n\n\n\n\nCode\ndef FE_categorical_transform(\n    X: pd.DataFrame, y: pd.Series, transform_type: str = \"mean\"\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Feature Engineering: Transform categorical features using CatBoost Cross-Validation.\n\n    This function performs feature engineering by transforming categorical features using CatBoost\n    Cross-Validation. It calculates the mean and standard deviation of Out-Of-Fold (OOF) Root Mean\n    Squared Error (RMSE) scores for various combinations of categorical and numerical features.\n\n    Parameters:\n    - X (pd.DataFrame): The input DataFrame containing both categorical and numerical features.\n    - transform_type (str, optional): The transformation type, such as \"mean\" or other valid\n      CatBoost transformations. Defaults to \"mean\".\n\n    Returns:\n    - pd.DataFrame: A DataFrame with columns \"mean_OOFs,\" \"std_OOFs,\" \"categorical,\" and \"numerical,\"\n      sorted by \"mean_OOFs\" in ascending order.\n\n    Example:\n    # Load your DataFrame with features (X)\n    X = load_data()\n\n    # Perform feature engineering\n    result_df = FE_categorical_transform(X, transform_type=\"mean\")\n\n    # View the DataFrame with sorted results\n    print(result_df.head())\n\n    Notes:\n    - This function uses CatBoost Cross-Validation to assess the quality of transformations for\n      various combinations of categorical and numerical features.\n    - The resulting DataFrame provides insights into the effectiveness of different transformations.\n    - Feature engineering can help improve the performance of machine learning models.\n    \"\"\"\n    # Initialize a list to store results\n    results = []\n\n    # Get a list of categorical and numerical columns\n    categorical_columns = X.select_dtypes(\"object\").columns\n    numerical_columns = X.select_dtypes(\"number\").columns\n\n    # Combine the loops to have a single progress bar\n    for categorical in tqdm(categorical_columns, desc=\"Progress\"):\n        for numerical in tqdm(numerical_columns):\n            # Create a deep copy of the input data\n            temp = X.copy(deep=True)\n\n            # Calculate the transformation for each group within the categorical column\n            temp[\"new_column\"] = temp.groupby(categorical)[numerical].transform(\n                transform_type\n            )\n\n            # Run CatBoost Cross-Validation with the transformed data\n            mean_OOF, std_OOF = train_model.run_catboost_CV(temp, y)\n\n            # Store the results as a tuple\n            result = (mean_OOF, std_OOF, categorical, numerical)\n            results.append(result)\n\n            del temp, mean_OOF, std_OOF\n\n    # Create a DataFrame from the results and sort it by mean OOF scores\n    result_df = pd.DataFrame(\n        results, columns=[\"mean_OOFs\", \"std_OOFs\", \"categorical\", \"numerical\"]\n    )\n    result_df = result_df.sort_values(by=\"mean_OOFs\")\n    return result_df\n\n\n\n\n\n\n\n\nNote\n\n\n\nPlease, bear in mind that these feature engineering steps were precomputed due to the considerable computational time required. The outcomes were saved rather than executed during the notebook rendering to save time. However, it’s important to note that the results should remain unchanged.\n\n\n\n\nCode\n%%script echo skipping\n\nFE_categorical_transform_mean = feature_engineering.FE_categorical_transform(\n    X_wo_outliers, y_wo_outliers\n)\n\n\nCouldn't find program: 'echo'\n\n\n\n\nCode\n%%script echo skipping\n\nFE_categorical_transform_mean.to_parquet(\n    f'{utils.Configuration.INTERIM_DATA_PATH.joinpath(\"FE_categorical_transform_mean\")}.parquet.gzip',\n    compression=\"gzip\",\n)\n\nFE_categorical_transform_mean.head(15)\n\n\nCouldn't find program: 'echo'\n\n\nAs evident, the best result was obtained by treating the city feature as a categorical variable and calculating the median of cadastral_income based on this categorization. This result aligns logically with the feature importances seen in Part 5.\n\n\nCode\npd.read_parquet(\n    utils.Configuration.INTERIM_DATA_PATH.joinpath(\n        \"FE_categorical_transform_mean.parquet.gzip\"\n    )\n).head()\n\n\n\n\n\n\n\n\n\nmean_OOFs\nstd_OOFs\ncategorical\nnumerical\n\n\n\n\n53\n0.108973\n0.006262\ncity\ncadastral_income\n\n\n39\n0.108985\n0.004980\nbuilding_condition\nyearly_theoretical_total_energy_consumption\n\n\n33\n0.109381\n0.005434\nbuilding_condition\nbedrooms\n\n\n31\n0.109478\n0.004887\nstreet\ncadastral_income\n\n\n43\n0.109540\n0.004944\nbuilding_condition\nliving_area"
  },
  {
    "objectID": "projects/Predicting Belgian Real Estate Prices_ Part 6_Feature_engineering/NB_6_ACs_Feature_engineering.html#generate-bins-from-the-continuous-variables",
    "href": "projects/Predicting Belgian Real Estate Prices_ Part 6_Feature_engineering/NB_6_ACs_Feature_engineering.html#generate-bins-from-the-continuous-variables",
    "title": "Predicting Belgian Real Estate Prices: Part 6: Feature Engineering",
    "section": "Generate bins from the continuous variables",
    "text": "Generate bins from the continuous variables\nThe idea behind this feature engineering step is to discretize continuous variables by creating bins or categories from their values. These bins then serve as categorical columns. By using these new categorical columns for grouping, we can transform each numerical variable by replacing its values with the median of the respective category it belongs to, just like the feature engineering method we demonstrated above.\n\n\nCode\ndef FE_continuous_transform(\n    X: pd.DataFrame, y: pd.Series, transform_type: str = \"mean\"\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Feature Engineering: Transform continuous features using CatBoost Cross-Validation.\n\n    This function performs feature engineering by transforming continuous features using CatBoost\n    Cross-Validation. It calculates the mean and standard deviation of Out-Of-Fold (OOF) Root Mean\n    Squared Error (RMSE) scores for various combinations of discretized and transformed continuous\n    features.\n\n    Parameters:\n    - X (pd.DataFrame): The input DataFrame containing both continuous and categorical features.\n    - y (pd.Series): The target variable for prediction.\n    - transform_type (str, optional): The transformation type, such as \"mean\" or other valid\n      CatBoost transformations. Defaults to \"mean\".\n\n    Returns:\n    - pd.DataFrame: A DataFrame with columns \"mean_OOFs,\" \"std_OOFs,\" \"discretized_continuous,\"\n      and \"transformed_continuous,\" sorted by \"mean_OOFs\" in ascending order.\n\n    Example:\n    # Load your DataFrame with features (X) and target variable (y)\n    X, y = load_data()\n\n    # Perform feature engineering\n    result_df = FE_continuous_transform(X, y, transform_type=\"mean\")\n\n    # View the DataFrame with sorted results\n    print(result_df.head())\n\n    Notes:\n    - This function uses CatBoost Cross-Validation to assess the quality of transformations for\n      various combinations of discretized and transformed continuous features.\n    - The number of bins for discretization is determined using Sturges' rule.\n    - The resulting DataFrame provides insights into the effectiveness of different transformations.\n    - Feature engineering can help improve the performance of machine learning models.\n    \"\"\"\n    # Initialize a list to store results\n    results = []\n\n    # Get a list of continuous and numerical columns\n    continuous_columns = X.select_dtypes(\"number\").columns\n    optimal_bins = int(np.floor(np.log2(X.shape[0])) + 1)\n\n    # Combine the loops to have a single progress bar\n    for discretized_continuous in tqdm(continuous_columns, desc=\"Progress:\"):\n        for transformed_continuous in tqdm(continuous_columns):\n            if discretized_continuous != transformed_continuous:\n                # Create a deep copy of the input data\n                temp = X.copy(deep=True)\n\n                discretizer = pipeline.Pipeline(\n                    steps=[\n                        (\"imputer\", impute.SimpleImputer(strategy=\"median\")),\n                        (\n                            \"add_bins\",\n                            preprocessing.KBinsDiscretizer(\n                                encode=\"ordinal\", n_bins=optimal_bins\n                            ),\n                        ),\n                    ]\n                )\n\n                temp[discretized_continuous] = discretizer.fit_transform(\n                    X[[discretized_continuous]]\n                )\n\n                # Calculate the transformation for each group within the categorical column\n                temp[\"new_column\"] = temp.groupby(discretized_continuous)[\n                    transformed_continuous\n                ].transform(transform_type)\n\n                # Run CatBoost Cross-Validation with the transformed data\n                mean_OOF, std_OOF = train_model.run_catboost_CV(temp, y)\n\n                # Store the results as a tuple\n                result = (\n                    mean_OOF,\n                    std_OOF,\n                    discretized_continuous,\n                    transformed_continuous,\n                )\n                results.append(result)\n\n                del temp, mean_OOF, std_OOF\n\n    # Create a DataFrame from the results and sort it by mean OOF scores\n    result_df = pd.DataFrame(\n        results,\n        columns=[\n            \"mean_OOFs\",\n            \"std_OOFs\",\n            \"discretized_continuous\",\n            \"transformed_continuous\",\n        ],\n    )\n    result_df = result_df.sort_values(by=\"mean_OOFs\")\n    return result_df\n\n\n\n\nCode\n%%script echo skipping\n\nFE_continuous_transform_mean = feature_engineering.FE_continuous_transform(\n    X_wo_outliers, y_wo_outliers\n)\n\nFE_continuous_transform_mean.to_parquet(\n    f'{utils.Configuration.INTERIM_DATA_PATH.joinpath(\"FE_continuous_transform_mean\")}.parquet.gzip',\n    compression=\"gzip\",\n)\n\nFE_categorical_transform_mean.head(15)\n\n\nCouldn't find program: 'echo'\n\n\nThis approach was not as effective as our prior method. However, combining bathrooms with yearly_theoretical_total_energy_consumption yielded the best outcome.\n\n\nCode\npd.read_parquet(\n    utils.Configuration.INTERIM_DATA_PATH.joinpath(\n        \"FE_continuous_transform_mean.parquet.gzip\"\n    )\n).head(10)\n\n\n\n\n\n\n\n\n\nmean_OOFs\nstd_OOFs\ndiscretized_continuous\ntransformed_continuous\n\n\n\n\n55\n0.109328\n0.005094\nbathrooms\nyearly_theoretical_total_energy_consumption\n\n\n59\n0.109328\n0.005094\nbathrooms\nliving_area\n\n\n58\n0.109328\n0.005094\nbathrooms\ncadastral_income\n\n\n57\n0.109328\n0.005094\nbathrooms\nlat\n\n\n56\n0.109328\n0.005094\nbathrooms\nsurface_of_the_plot\n\n\n50\n0.109328\n0.005094\nbathrooms\nbedrooms\n\n\n52\n0.109328\n0.005094\nbathrooms\ntoilets\n\n\n39\n0.109417\n0.004831\nlng\nliving_area\n\n\n1\n0.109426\n0.004587\nbedrooms\ntoilets\n\n\n4\n0.109426\n0.004587\nbedrooms\nbathrooms"
  },
  {
    "objectID": "projects/Predicting Belgian Real Estate Prices_ Part 6_Feature_engineering/NB_6_ACs_Feature_engineering.html#introduce-polynomial-features",
    "href": "projects/Predicting Belgian Real Estate Prices_ Part 6_Feature_engineering/NB_6_ACs_Feature_engineering.html#introduce-polynomial-features",
    "title": "Predicting Belgian Real Estate Prices: Part 6: Feature Engineering",
    "section": "Introduce polynomial features",
    "text": "Introduce polynomial features\nThe idea behind introducing polynomial features is to capture non-linear relationships within the data. By raising individual features to higher powers or considering interactions between pairs of features, this step allows the model to better represent complex patterns that cannot be adequately expressed with linear relationships alone. It enhances the model’s ability to learn and predict outcomes that exhibit curvilinear or interactive behavior.\n\n\nCode\ndef FE_polynomial_features(\n    X: pd.DataFrame, y: pd.Series, combinations: int = 1\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate polynomial features for combinations of numerical columns and train a CatBoost model.\n\n    Parameters:\n        X (pd.DataFrame): The input DataFrame with features.\n        y (pd.Series): The target variable.\n        combinations (int, optional): The number of combinations of numerical columns. Default is 1.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing results sorted by mean OOF scores.\n\n    Example:\n        X_wo_outliers = pd.DataFrame(...)  # Your input data\n        y_wo_outliers = pd Series(...)  # Your target variable\n        result = FE_polynomial_features(X_wo_outliers, y_wo_outliers)\n\n    Transformations:\n        - Imputes missing values in numerical columns using the median.\n        - Generates polynomial features, including interaction terms, for selected numerical columns.\n        - Trains a CatBoost model and calculates mean and standard deviation of out-of-fold (OOF) scores.\n    \"\"\"\n\n    # Initialize a list to store results\n    results = []\n\n    # Get a list of continuous and numerical columns\n    numerical_columns = X.select_dtypes(\"number\").columns\n\n    # Combine the loops to have a single progress bar\n    for numerical_col in tqdm(\n        list(itertools.combinations(numerical_columns, r=combinations))\n    ):\n        polyfeatures = compose.make_column_transformer(\n            (\n                pipeline.make_pipeline(\n                    impute.SimpleImputer(strategy=\"median\"),\n                    preprocessing.PolynomialFeatures(interaction_only=False),\n                ),\n                list(numerical_col),\n            ),\n            remainder=\"passthrough\",\n        ).set_output(transform=\"pandas\")\n\n        temp = polyfeatures.fit_transform(X)\n        mean_OOF, std_OOF = train_model.run_catboost_CV(X=temp, y=y)\n\n        # Store the results as a tuple\n        result = (\n            mean_OOF,\n            std_OOF,\n            numerical_col,\n        )\n        results.append(result)\n\n        del temp, mean_OOF, std_OOF\n\n    # Create a DataFrame from the results and sort it by mean OOF scores\n    result_df = pd.DataFrame(\n        results,\n        columns=[\n            \"mean_OOFs\",\n            \"std_OOFs\",\n            \"numerical_col\",\n        ],\n    )\n    result_df = result_df.sort_values(by=\"mean_OOFs\")\n    return result_df\n\n\n\nn=1\nLet’s see the impact of applying polynomial feature engineering to a single feature.\n\n\nCode\n%%script echo skipping\n\nFE_polynomial_features_combinations_1 = FE_polynomial_features(\n    X_wo_outliers, y_wo_outliers\n)\n\nFE_polynomial_features_combinations_1.to_parquet(\n    f'{utils.Configuration.INTERIM_DATA_PATH.joinpath(\"FE_polynomial_features_combinations_1\")}.parquet.gzip',\n    compression=\"gzip\",\n)\n\nFE_polynomial_features_combinations_1.head(15)\n\n\nCouldn't find program: 'echo'\n\n\n\n\nCode\npd.read_parquet(\n    utils.Configuration.INTERIM_DATA_PATH.joinpath(\n        \"FE_polynomial_features_combinations_1.parquet.gzip\"\n    )\n).head(10)\n\n\n\n\n\n\n\n\n\nmean_OOFs\nstd_OOFs\nnumerical_col\n\n\n\n\n10\n0.109938\n0.005047\n[living_area]\n\n\n9\n0.110339\n0.004012\n[cadastral_income]\n\n\n3\n0.110628\n0.004018\n[lng]\n\n\n0\n0.111066\n0.004765\n[bedrooms]\n\n\n8\n0.111099\n0.005039\n[lat]\n\n\n4\n0.111166\n0.004879\n[primary_energy_consumption]\n\n\n6\n0.111271\n0.004908\n[yearly_theoretical_total_energy_consumption]\n\n\n1\n0.111276\n0.005359\n[number_of_frontages]\n\n\n7\n0.111332\n0.004815\n[surface_of_the_plot]\n\n\n2\n0.111782\n0.004741\n[toilets]\n\n\n\n\n\n\n\n\n\nn=2\nHow about two features combined…\n\n\nCode\n%%script echo skipping\n\nFE_polynomial_features_combinations_2 = FE_polynomial_features(\n    X_wo_outliers, y_wo_outliers, combinations=2\n)\n\nFE_polynomial_features_combinations_2.to_parquet(\n    f'{utils.Configuration.INTERIM_DATA_PATH.joinpath(\"FE_polynomial_features_combinations_2\")}.parquet.gzip',\n    compression=\"gzip\",\n)\n\nFE_polynomial_features_combinations_2.head(15)\n\n\nCouldn't find program: 'echo'\n\n\n\n\nCode\npd.read_parquet(\n    utils.Configuration.INTERIM_DATA_PATH.joinpath(\n        \"FE_polynomial_features_combinations_2.parquet.gzip\"\n    )\n).head(10)\n\n\n\n\n\n\n\n\n\nmean_OOFs\nstd_OOFs\nnumerical_col\n\n\n\n\n31\n0.109413\n0.004690\n[lng, lat]\n\n\n9\n0.109625\n0.005558\n[bedrooms, living_area]\n\n\n53\n0.109809\n0.005321\n[lat, living_area]\n\n\n52\n0.109814\n0.003485\n[lat, cadastral_income]\n\n\n7\n0.109847\n0.005399\n[bedrooms, lat]\n\n\n19\n0.109962\n0.004999\n[toilets, lng]\n\n\n17\n0.110057\n0.004554\n[number_of_frontages, cadastral_income]\n\n\n42\n0.110124\n0.004511\n[bathrooms, lat]\n\n\n46\n0.110128\n0.004944\n[yearly_theoretical_total_energy_consumption, ...\n\n\n28\n0.110154\n0.004644\n[lng, bathrooms]"
  },
  {
    "objectID": "projects/Predicting Belgian Real Estate Prices_ Part 6_Feature_engineering/NB_6_ACs_Feature_engineering.html#form-clusters-of-instances-using-k-means-clustering",
    "href": "projects/Predicting Belgian Real Estate Prices_ Part 6_Feature_engineering/NB_6_ACs_Feature_engineering.html#form-clusters-of-instances-using-k-means-clustering",
    "title": "Predicting Belgian Real Estate Prices: Part 6: Feature Engineering",
    "section": "Form clusters of instances using k-means clustering",
    "text": "Form clusters of instances using k-means clustering\nThe idea behind using k-means clustering in feature engineering is to group data points into clusters based on their similarity. By doing so, we create a new set of features that represent these clusters, which can capture patterns or relationships within the data that might be less apparent in the original features. These cluster features can be valuable for machine learning models, as they provide a more compact and informative representation of the data, potentially improving predictive performance.\n\n\nCode\nclass FeatureSelector(BaseEstimator, TransformerMixin):\n    \"\"\"\n    A transformer for selecting specific columns from a DataFrame.\n\n    This class inherits from the BaseEstimator and TransformerMixin classes from sklearn.base.\n    It overrides the fit and transform methods from the parent classes.\n\n    Attributes:\n        feature_names_in_ (list): The names of the features to select.\n        n_features_in_ (int): The number of features to select.\n\n    Methods:\n        fit(X, y=None): Fit the transformer. Returns self.\n        transform(X, y=None): Apply the transformation. Returns a DataFrame with selected features.\n    \"\"\"\n\n    def __init__(self, feature_names_in_):\n        \"\"\"\n        Constructs all the necessary attributes for the FeatureSelector object.\n\n        Args:\n            feature_names_in_ (list): The names of the features to select.\n        \"\"\"\n        self.feature_names_in_ = feature_names_in_\n        self.n_features_in_ = len(feature_names_in_)\n\n    def fit(self, X, y=None):\n        \"\"\"\n        Fit the transformer. This method doesn't do anything as no fitting is necessary.\n\n        Args:\n            X (DataFrame): The input data.\n            y (array-like, optional): The target variable. Defaults to None.\n\n        Returns:\n            self: The instance itself.\n        \"\"\"\n        return self\n\n    def transform(self, X, y=None):\n        \"\"\"\n        Apply the transformation. Selects the features from the input data.\n\n        Args:\n            X (DataFrame): The input data.\n            y (array-like, optional): The target variable. Defaults to None.\n\n        Returns:\n            DataFrame: A DataFrame with only the selected features.\n        \"\"\"\n        return X.loc[:, self.feature_names_in_].copy(deep=True)\n\n\n\n\nCode\ndef FE_KMeans(\n    X: pd.DataFrame,\n    y: pd.Series,\n    n_clusters_min: int = 1,\n    n_clusters_max: int = 8,\n) -&gt; pd.DataFrame:\n    \"\"\"Performs K-Means clustering-based feature engineering followed by model training.\n\n    Args:\n        X (pd.DataFrame): The input feature matrix.\n        y (pd.Series): The target variable.\n        n_clusters_min (int, optional): The minimum number of clusters to consider. Defaults to 1.\n        n_clusters_max (int, optional): The maximum number of clusters to consider. Defaults to 8.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the results of feature engineering with K-Means clustering.\n\n    Example:\n        &gt;&gt;&gt; results_df = FE_KNN(X_wo_outliers, y_wo_outliers)\n    \"\"\"\n    # Initialize a list to store results\n    results = []\n\n    # Get a list of continuous and numerical columns\n    numerical_columns = X.head().select_dtypes(\"number\").columns.to_list()\n    categorical_columns = X.head().select_dtypes(\"object\").columns.to_list()\n\n    for n_cluster in tqdm(range(n_clusters_min, n_clusters_max)):\n        # Prepare pipelines for corresponding columns:\n        numerical_pipeline = pipeline.Pipeline(\n            steps=[\n                (\"num_selector\", FeatureSelector(numerical_columns)),\n                (\"imputer\", impute.SimpleImputer(strategy=\"median\")),\n            ]\n        )\n\n        categorical_pipeline = pipeline.Pipeline(\n            steps=[\n                (\"cat_selector\", FeatureSelector(categorical_columns)),\n                (\"imputer\", impute.SimpleImputer(strategy=\"most_frequent\")),\n                (\n                    \"onehot\",\n                    preprocessing.OneHotEncoder(\n                        handle_unknown=\"ignore\", sparse_output=False\n                    ),\n                ),\n            ]\n        )\n\n        # Put all the pipelines inside a FeatureUnion:\n        data_preprocessing_pipeline = pipeline.FeatureUnion(\n            n_jobs=-1,\n            transformer_list=[\n                (\"numerical_pipeline\", numerical_pipeline),\n                (\"categorical_pipeline\", categorical_pipeline),\n            ],\n        )\n\n        temp = pd.DataFrame(data_preprocessing_pipeline.fit_transform(X))\n\n        KMeans = cluster.KMeans(n_init=10, n_clusters=n_cluster)\n        KMeans.fit_transform(temp)\n\n        groups = pd.Series(KMeans.labels_, name=\"groups\")\n\n        concatanated_df = pd.concat([temp, groups], axis=\"columns\")\n\n        mean_OOF, std_OOF = train_model.run_catboost_CV(X=concatanated_df, y=y)\n\n        # Store the results as a tuple\n        result = (\n            mean_OOF,\n            std_OOF,\n            n_cluster,\n        )\n        results.append(result)\n\n        del temp, mean_OOF, std_OOF, KMeans, groups, concatanated_df, result\n\n    # Create a DataFrame from the results and sort it by mean OOF scores\n    result_df = pd.DataFrame(\n        results,\n        columns=[\n            \"mean_OOFs\",\n            \"std_OOFs\",\n            \"n_cluster\",\n        ],\n    )\n    result_df = result_df.sort_values(by=\"mean_OOFs\")\n    return result_df\n\n\n\n\nCode\n%%script echo skipping\n\nFE_KMeans_df = FE_KMeans(\n    X_wo_outliers,\n    y_wo_outliers,\n    n_clusters_min=1,\n    n_clusters_max=101,\n)\n\nFE_KMeans_df.to_parquet(\n    f'{utils.Configuration.INTERIM_DATA_PATH.joinpath(\"FE_KNN_df\")}.parquet.gzip',\n    compression=\"gzip\",\n)\n\nFE_KNN_df.head(15)\n\n\nCouldn't find program: 'echo'\n\n\nAs k-means clustering is an unsupervised algorithm, determining the appropriate k-values requires testing various values to assess their impact on our validation scores. As observed, this approach didn’t yield significant results in our case, as the best validation score was obtained when n=1.\n\n\nCode\npd.read_parquet(\n    utils.Configuration.INTERIM_DATA_PATH.joinpath(\"FE_KNN_df.parquet.gzip\")\n).head(10)\n\n\n\n\n\n\n\n\n\nmean_OOFs\nstd_OOFs\nn_cluster\n\n\n\n\n0\n0.111884\n0.005143\n1\n\n\n1\n0.112153\n0.006174\n2\n\n\n5\n0.112264\n0.005858\n6\n\n\n24\n0.112313\n0.005602\n25\n\n\n7\n0.112326\n0.005124\n8\n\n\n43\n0.112333\n0.005819\n44\n\n\n31\n0.112352\n0.005588\n32\n\n\n11\n0.112366\n0.005206\n12\n\n\n95\n0.112454\n0.006621\n96\n\n\n26\n0.112472\n0.006096\n27"
  },
  {
    "objectID": "projects/Predicting Belgian Real Estate Prices_ Part 6_Feature_engineering/NB_6_ACs_Feature_engineering.html#implement-other-ideas-derived-from-empirical-observations-or-assumptions",
    "href": "projects/Predicting Belgian Real Estate Prices_ Part 6_Feature_engineering/NB_6_ACs_Feature_engineering.html#implement-other-ideas-derived-from-empirical-observations-or-assumptions",
    "title": "Predicting Belgian Real Estate Prices: Part 6: Feature Engineering",
    "section": "Implement other ideas derived from empirical observations or assumptions",
    "text": "Implement other ideas derived from empirical observations or assumptions\nThough new features can be generated through systematic methods, domain knowledge can also inspire their creation. The idea behind this is to allow for the incorporation of unconventional or domain-specific insights that may not fit standard feature engineering techniques. It encourages the exploration of novel features or transformations based on practical experiences or theoretical assumptions to potentially uncover hidden patterns or relationships within the data. This open-ended approach can lead to creative and tailored feature engineering solutions.\nHere are some ideas to consider:\n\nGeospatial Features:\n\nCreate clusters or neighborhoods based on features to capture similarities.\n\nArea-related Features:\n\nCalculate the ratio of “living_area” to “surface_of_the_plot” to get an idea of the density or spaciousness of the property.\n\nEnergy Efficiency Features:\n\nCompute the energy efficiency ratio by dividing “yearly_theoretical_total_energy_consumption” by “primary_energy_consumption.”\nCompute energy efficiency by dividing primary_energy_consumption with living_area\n\nToilet and Bathroom Features:\n\nCombine “toilets” and “bathrooms” into a single “total_bathrooms” feature to simplify the model.\nCalculate total number of rooms by adding up bedrooms + toilets + bathrooms\n\nTaxation Features:\n\nIncorporate “cadastral_income” as a measure of property value for taxation. You can create bins or categories for this variable.\n\nValue for Money:\n\nDivide cadastral_income by bedrooms to see if the property is a good bargain\nsimilarly, Divide cadastral_income by living_area\n\n\n\n\nCode\ndef FE_ideas(X):\n    \"\"\"Performs additional feature engineering on the input DataFrame.\n\n    Args:\n        X (pd.DataFrame): The input DataFrame containing the original features.\n\n    Returns:\n        pd.DataFrame: A DataFrame with additional engineered features.\n\n    Example:\n        &gt;&gt;&gt; engineered_data = FE_ideas(original_data)\n    \"\"\"\n    temp = X.assign(\n        energy_efficiency_1=lambda df: df.yearly_theoretical_total_energy_consumption\n        / df.primary_energy_consumption,\n        energy_efficiency_2=lambda df: df.primary_energy_consumption / df.living_area,\n        total_bathrooms=lambda df: df.toilets + df.bathrooms,\n        total_number_rooms=lambda df: df.toilets + df.bathrooms + df.bedrooms,\n        spaciousness_1=lambda df: df.living_area / df.surface_of_the_plot,\n        spaciousness_2=lambda df: df.living_area / df.total_number_rooms,\n        bargain_1=lambda df: df.cadastral_income / df.bedrooms,\n        bargain_2=lambda df: df.cadastral_income / df.living_area,\n    )\n    return temp.loc[:, \"energy_efficiency_1\":]\n\n\n\n\nCode\ndef FE_try_ideas(\n    X: pd.DataFrame,\n    y: pd.Series,\n) -&gt; pd.DataFrame:\n    \"\"\"Performs feature engineering experiments by adding new features and evaluating their impact on model performance.\n\n    Args:\n        X (pd.DataFrame): The input feature matrix.\n        y (pd.Series): The target variable.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the results of feature engineering experiments.\n\n    Example:\n        &gt;&gt;&gt; results_df = FE_try_ideas(X, y)\n    \"\"\"\n    # Initialize a list to store results\n    results = []\n\n    # Get a list of continuous and numerical columns\n    numerical_columns = X.select_dtypes(\"number\").columns\n\n    # Apply additional feature engineering ideas\n    feature_df = FE_ideas(X)\n\n    for feature in tqdm(feature_df.columns):\n        # Concatenate the original features with the newly engineered feature\n        temp = pd.concat([X, feature_df[feature]], axis=\"columns\")\n\n        # Train the model with the augmented features and get the mean and standard deviation of OOF scores\n        mean_OOF, std_OOF = train_model.run_catboost_CV(X=temp, y=y)\n\n        # Store the results as a tuple\n        result = (\n            mean_OOF,\n            std_OOF,\n            feature,\n        )\n        results.append(result)\n\n        del temp, mean_OOF, std_OOF\n\n    # Create a DataFrame from the results and sort it by mean OOF scores\n    result_df = pd.DataFrame(\n        results,\n        columns=[\n            \"mean_OOFs\",\n            \"std_OOFs\",\n            \"feature\",\n        ],\n    )\n    result_df = result_df.sort_values(by=\"mean_OOFs\")\n    return result_df\n\n\n\n\nCode\n%%script echo skipping\n\nFE_try_ideas = FE_try_ideas(X_wo_outliers, y_wo_outliers)\n\nFE_try_ideas.to_parquet(\n    f'{utils.Configuration.INTERIM_DATA_PATH.joinpath(\"FE_try_ideas\")}.parquet.gzip',\n    compression=\"gzip\",\n)\n\nFE_try_ideas\n\n\nCouldn't find program: 'echo'\n\n\nAs can be seen below, the best feature this time was spaciousness_1, representing df.living_area divided by df.surface_of_the_plot.\n\n\nCode\npd.read_parquet(\n    utils.Configuration.INTERIM_DATA_PATH.joinpath(\"FE_try_ideas.parquet.gzip\")\n)\n\n\n\n\n\n\n\n\n\nmean_OOFs\nstd_OOFs\nfeature\n\n\n\n\n4\n0.109305\n0.004652\nspaciousness_1\n\n\n7\n0.109558\n0.004372\nbargain_2\n\n\n3\n0.109969\n0.005117\ntotal_number_rooms\n\n\n6\n0.109976\n0.004303\nbargain_1\n\n\n2\n0.110545\n0.004884\ntotal_bathrooms\n\n\n5\n0.110603\n0.004715\nspaciousness_2\n\n\n1\n0.110666\n0.005749\nenergy_efficiency_2\n\n\n0\n0.110722\n0.005120\nenergy_efficiency_1"
  },
  {
    "objectID": "projects/Predicting Belgian Real Estate Prices_ Part 4_Building_a_baseline_model/NB_4_ACs_Building_a_baseline_model.html",
    "href": "projects/Predicting Belgian Real Estate Prices_ Part 4_Building_a_baseline_model/NB_4_ACs_Building_a_baseline_model.html",
    "title": "Predicting Belgian Real Estate Prices: Part 4: Building a Baseline Model",
    "section": "",
    "text": "Photo by Stephen Phillips - Hostreviews.co.uk on UnSplash\nIn the preceding Part 3, our emphasis was on establishing a fundamental understanding of our data through characterizing the cleaned scraped dataset. We delved into feature cardinality, distributions, and potential correlations with our target variable—property price. Moving on to Part 4, our agenda includes examining essential sample pre-processing steps before modeling. We will craft the necessary pipeline, assess multiple algorithms, and ultimately select a suitable baseline model. Let’s get started!"
  },
  {
    "objectID": "projects/Predicting Belgian Real Estate Prices_ Part 4_Building_a_baseline_model/NB_4_ACs_Building_a_baseline_model.html#read-in-the-processed-file",
    "href": "projects/Predicting Belgian Real Estate Prices_ Part 4_Building_a_baseline_model/NB_4_ACs_Building_a_baseline_model.html#read-in-the-processed-file",
    "title": "Predicting Belgian Real Estate Prices: Part 4: Building a Baseline Model",
    "section": "Read in the processed file",
    "text": "Read in the processed file\nAfter importing our preprocessed dataframe, a crucial step in our data refinement process involves the culling of certain columns. Specifically, we intend to exclude columns with labels such as “external_reference,” “ad_url,” “day_of_retrieval,” “website,” “reference_number_of_the_epc_report,” and “housenumber.” Our rationale behind this action is to enhance the efficiency of our model by eliminating potentially non-contributory features.\n\n\nCode\nutils.seed_everything(utils.Configuration.seed)\n\ndf = (\n    pd.read_parquet(\n        utils.Configuration.INTERIM_DATA_PATH.joinpath(\n            \"2023-10-01_Processed_dataset_for_NB_use.parquet.gzip\"\n        )\n    )\n    .sample(frac=1, random_state=utils.Configuration.seed)\n    .reset_index(drop=True)\n    .assign(price=lambda df: np.log10(df.price))\n    .drop(\n        columns=[\n            \"external_reference\",\n            \"ad_url\",\n            \"day_of_retrieval\",\n            \"website\",\n            \"reference_number_of_the_epc_report\",\n            \"housenumber\",\n        ]\n    )\n)\n\nprint(f\"Shape of dataframe after read-in a pre-processing: {df.shape}\")\nX = df.drop(columns=utils.Configuration.target_col)\ny = df[utils.Configuration.target_col]\n\nprint(f\"Shape of X: {X.shape}\")\nprint(f\"Shape of y: {y.shape}\")\n\n\nShape of dataframe after read-in a pre-processing: (3660, 50)\nShape of X: (3660, 49)\nShape of y: (3660,)"
  },
  {
    "objectID": "projects/Predicting Belgian Real Estate Prices_ Part 4_Building_a_baseline_model/NB_4_ACs_Building_a_baseline_model.html#train-test-split",
    "href": "projects/Predicting Belgian Real Estate Prices_ Part 4_Building_a_baseline_model/NB_4_ACs_Building_a_baseline_model.html#train-test-split",
    "title": "Predicting Belgian Real Estate Prices: Part 4: Building a Baseline Model",
    "section": "Train-test split",
    "text": "Train-test split\nThe subsequent phase in our data preparation involves the partitioning of our dataset into training and testing subsets. To accomplish this, we’ll leverage the model_selection.train_test_split method. This step ensures that we have distinct sets for model training and evaluation, a fundamental practice in machine learning.\n\n\nCode\nX_train, X_test, y_train, y_test = model_selection.train_test_split(\n    X, y, test_size=0.2, random_state=utils.Configuration.seed\n)\n\nprint(f\"Shape of X-train: {X_train.shape}\")\nprint(f\"Shape of X-test: {X_test.shape}\")\n\n\nShape of X-train: (2928, 49)\nShape of X-test: (732, 49)"
  },
  {
    "objectID": "projects/Predicting Belgian Real Estate Prices_ Part 2_Basic_clean_up_after_scraping/NB_2_ACs_Basic_clean_up_after_scraping.html",
    "href": "projects/Predicting Belgian Real Estate Prices_ Part 2_Basic_clean_up_after_scraping/NB_2_ACs_Basic_clean_up_after_scraping.html",
    "title": "Predicting Belgian Real Estate Prices: Part 2: Data Cleanup After Web Scraping",
    "section": "",
    "text": "Photo by Stephen Phillips - Hostreviews.co.uk on UnSplash\nIn part 1, we provided a brief introduction to the project’s purpose. Now, in part 2, we will dive deeper into the data processing steps required after scraping. We will discuss the handling of numerical data, categorical variables, and boolean values. Additionally, we’ll assess the data quality by examining the error log generated by the Immowebscraper class. Let’s get to it!"
  },
  {
    "objectID": "projects/Predicting Belgian Real Estate Prices_ Part 2_Basic_clean_up_after_scraping/NB_2_ACs_Basic_clean_up_after_scraping.html#reading-in-and-inspecting-the-log-file",
    "href": "projects/Predicting Belgian Real Estate Prices_ Part 2_Basic_clean_up_after_scraping/NB_2_ACs_Basic_clean_up_after_scraping.html#reading-in-and-inspecting-the-log-file",
    "title": "Predicting Belgian Real Estate Prices: Part 2: Data Cleanup After Web Scraping",
    "section": "Reading in and inspecting the log file",
    "text": "Reading in and inspecting the log file\nAnalyzing the error log file, we’ve identified a total of 3,515 errors encountered during the web scraping process on the Immoweb website. Let’s delve into these errors to pinpoint the most common issues and address them accordingly.\n\n\nCode\nerror_log = pd.read_table(\n    utils.Configuration.RAW_DATA_PATH.joinpath(\"make_dataset_error_for_NB2.log\"),\n    header=None,\n).rename(columns={0: \"error\"})\n\nerror_log\n\n\n\n\n\n\n\n\n\nerror\n\n\n\n\n0\n2023-09-27 14:02:11 - ERROR - No tables found ...\n\n\n1\n2023-09-27 14:02:17 - ERROR - No tables found ...\n\n\n2\n2023-09-27 14:02:22 - ERROR - No tables found ...\n\n\n3\n2023-09-27 14:02:25 - ERROR - No tables found ...\n\n\n4\n2023-09-27 14:02:34 - ERROR - No tables found ...\n\n\n...\n...\n\n\n3510\n2023-09-27 16:43:45 - ERROR - Duplicate labels...\n\n\n3511\n2023-09-27 16:43:57 - ERROR - No tables found ...\n\n\n3512\n2023-09-27 16:43:57 - ERROR - Duplicate labels...\n\n\n3513\n2023-09-27 16:43:57 - ERROR - Duplicate labels...\n\n\n3514\n2023-09-27 16:44:03 - ERROR - No tables found ...\n\n\n\n\n3515 rows × 1 columns\n\n\n\n\nMost common errors from log file\nIt’s clear that a significant majority of the errors, accounting for 1,848 cases, result from the absence of tables on the pages. These errors are primarily found on listing ads and index pages. To address this issue, we’ve introduced an if clause into our method extract_ads_from_given_page, which can be found in the make_dataset.py module. The clause, if \"immoweb.be\" in item and \"https://www.immoweb.be/en/search\" not in item, enables us to filter out undesired pages that don’t contain relevant table information for our ads. This not only helps mitigate errors but also speeds up the dataset collection process by reducing the number of pages we scrape.\nAnother category of errors, totaling 1,460 cases, is related to the presence of duplicate labels during processing. We may need to investigate this issue further at a later stage to ensure data quality and accuracy.\nA smaller proportion of errors is linked to the “Empty data” message, primarily related to ads. Finally, the remaining errors encompass errorrs related to data type conversion. We can consider either leaving these columns as is, since the error is not that frequent, or removing these features altogether.\n\n\nCode\n(\n    error_log.error.str.split(\"-\", expand=True)[4]\n    .str.rsplit(\" \", n=1, expand=True)[0]\n    .value_counts()\n)\n\n\n0\n No tables found while processing                                                                                                        1848\n Duplicate labels found while processing                                                                                                 1460\n Empty data while processing                                                                                                               18\n An error occurred on page 327: (\"Expected bytes, got a 'int' object\", 'Conversion failed for column Number of frontages with type          1\n An error occurred on page 255: (\"Expected bytes, got a 'int' object\", 'Conversion failed for column Number of frontages with type          1\n                                                                                                                                         ... \n An error occurred on page 173: (\"Expected bytes, got a 'int' object\", 'Conversion failed for column Number of frontages with type          1\n An error occurred on page 174: (\"Expected bytes, got a 'int' object\", 'Conversion failed for column Number of frontages with type          1\n An error occurred on page 176: (\"Expected bytes, got a 'int' object\", 'Conversion failed for column Number of frontages with type          1\n An error occurred on page 177: (\"Expected bytes, got a 'int' object\", 'Conversion failed for column Number of frontages with type          1\n An error occurred on page 331: (\"Expected bytes, got a 'int' object\", 'Conversion failed for column Outdoor parking spaces with type       1\nName: count, Length: 192, dtype: int64\n\n\nWe’ve identified five columns responsible for the last set of data type conversion problems (See below):\n\nConstruction year\nNumber of frontages\nOutdoor parking spaces\nCovered parking spaces\nBedrooms\n\n\n\nCode\n(\n    error_log.error.str.split(\"-\", expand=True)[4]\n    .str.rsplit(\" \", n=1, expand=True)[0]\n    .value_counts()[3:]\n    .to_frame()\n    .reset_index()\n    .rename(columns={0: \"error_type\"})\n    .assign(\n        error_type=lambda df: df.error_type.str.split(\",\", expand=True)[2].str.split(\n            \" \", n=5, expand=True\n        )[5]\n    )\n    .error_type.value_counts()\n)\n\n\nerror_type\nConstruction year with type         83\nNumber of frontages with type       65\nOutdoor parking spaces with type     7\nCovered parking spaces with type     7\nBedrooms with type                   1\nName: count, dtype: int64\n\n\n\n\nUnique URLs from the error logs\nUpon conducting a more comprehensive analysis of the URLs extracted from error messages, a noteworthy observation comes to light: we’ve encountered only 433 unique URLs. This suggests that the 3,515 errors are stemming from a relatively restricted set of web addresses.\nNow, here’s a question that arises: the website implies the presence of 10,000 ads on the page. However, given our successful extraction of only 3,906 ads, along with the 433 URLs associated with errors, there is a substantial disparity evident.\nIf you have any insights or hypotheses regarding this difference, I’d be eager to hear your thoughts and discuss potential reasons for this variation.\n\n\nCode\n(\n    error_log.error.str.split(\"-\", expand=True)[4]\n    .str.rsplit(\" \", n=1, expand=True)[1]\n    .unique()\n    .shape\n)\n\n\n(433,)"
  },
  {
    "objectID": "projects/Predicting Belgian Real Estate Prices_ Part 2_Basic_clean_up_after_scraping/NB_2_ACs_Basic_clean_up_after_scraping.html#inspecting-the-data-itself",
    "href": "projects/Predicting Belgian Real Estate Prices_ Part 2_Basic_clean_up_after_scraping/NB_2_ACs_Basic_clean_up_after_scraping.html#inspecting-the-data-itself",
    "title": "Predicting Belgian Real Estate Prices: Part 2: Data Cleanup After Web Scraping",
    "section": "Inspecting the data itself",
    "text": "Inspecting the data itself\nAfter eliminating rows where all values were missing and filtering for rows with non-missing prices, we’ve successfully refined our dataset to include 3,660 ads.\nIn our subsequent analysis, we focus on the features with the lowest percentage of missing da, just like we did in Part 1ta. Notably, “day of retrieval” and “price” are complete, with all values present. However, it’s important to recognize that roughly one-third of the data related to “dining_room” and “office” is missing, highlighting the need for improving data completeness in these specific attributet.s.\n\n\nCode\n(\n    finer_pre_cleaned.dropna(axis=0, how=\"all\")\n    .query(\"price.notna()\")\n    .notna()\n    .sum()\n    .sort_values()\n    .div(3660)\n    .mul(100)\n    .round(1)\n)\n\n\ndining_room                                        29.3\noffice                                             29.5\nplanning_permission_obtained                       32.1\ntv_cable                                           34.0\nproceedings_for_breach_of_planning_regulations     36.3\nsubdivision_permit                                 38.2\nyearly_theoretical_total_energy_consumption        39.5\nwidth_of_the_lot_on_the_street                     42.4\nco2_emission                                       43.0\nconnection_to_sewer_network                        44.7\npossible_priority_purchase_right                   45.5\nstreet_frontage_width                              46.5\nbasement                                           46.6\nas_built_plan                                      47.0\nlatest_land_use_designation                        47.1\ngarden_surface                                     47.4\noutdoor_parking_spaces                             48.0\nfurnished                                          48.4\nsurroundings_type                                  50.8\nflood_zone_type                                    54.4\nbedroom_3_surface                                  54.8\ncovered_parking_spaces                             54.9\nkitchen_surface                                    58.2\navailable_as_of                                    58.3\nliving_room_surface                                64.7\nbedroom_2_surface                                  66.6\ngas_water__electricity                             67.0\nbedroom_1_surface                                  68.2\nconstruction_year                                  71.0\ncadastral_income                                   78.3\nkitchen_type                                       83.4\ndouble_glazing                                     84.6\nwebsite                                            84.6\nheating_type                                       87.2\nexternal_reference                                 90.3\ntoilets                                            90.8\nnumber_of_frontages                                94.6\nbathrooms                                          94.9\nhouse_number                                       94.9\nprimary_energy_consumption                         96.0\nbuilding_condition                                 96.0\nsurface_of_the_plot                                96.6\nliving_area                                        98.2\ntenement_building                                  99.1\nzip                                                99.5\ncity                                               99.5\nbedrooms                                           99.5\nstreet                                             99.5\nreference_number_of_the_epc_report                 99.9\nenergy_class                                       99.9\nprice                                             100.0\nad_url                                            100.0\nday_of_retrieval                                  100.0\ndtype: float64\n\n\nOur filter_out_missing_indexes function proves to be quite valuable in the post-processing of our scraped data. This function is located at the end of our pre-process chain saving our data to the INTERIM_DATA_PATH folder after we’ve completed pre-processing and removed missing values.\n\n\nCode\ndef filter_out_missing_indexes(\n    df: pd.DataFrame,\n    filepath: Path = utils.Configuration.INTERIM_DATA_PATH.joinpath(\n        f\"{str(pd.Timestamp.now())[:10]}_Processed_dataset.parquet.gzip\"\n    ),\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Filter out rows with missing values in a DataFrame and save the processed dataset.\n\n    This function filters out rows with all missing values (NaN) and retains only rows\n    with non-missing values in the 'price' column. The resulting DataFrame is then saved\n    in Parquet format with gzip compression.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame.\n        filepath (Path, optional): The path to save the processed dataset in Parquet format.\n            Defaults to a timestamp-based filepath in the interim data directory.\n\n    Returns:\n        pd.DataFrame: The filtered DataFrame with missing rows removed.\n\n    Example:\n        To filter out missing rows and save the processed dataset:\n        &gt;&gt;&gt; data = pd.read_csv(\"raw_data.csv\")\n        &gt;&gt;&gt; filtered_data = filter_out_missing_indexes(data)\n        &gt;&gt;&gt; print(filtered_data.head())\n\n    Notes:\n        - Rows with missing values in any column other than 'price' are removed.\n        - The processed dataset is saved with gzip compression to conserve disk space.\n    \"\"\"\n    processed_df = df.dropna(axis=0, how=\"all\").query(\"price.notna()\")\n    processed_df.to_parquet(filepath, compression=\"gzip\", index=False)\n    return processed_df\n\n\nIt appears that we’ve successfully completed the data transformation phase of our scraped dataset. With the implementation of functions like filter_out_missing_indexes, alongside pre_process_dataframe and separate_address, we’ve assembled the essential tools required for preparing our dataset for the machine learning pipeline.\nIn Part 3, we’ll provide a fundamental overview and characterization of the cleaned scraped data. We’ll assess feature cardinality, examine distributions, and explore correlations among variables. I look forward to delving into these insights with you in the next installment. See you there!"
  },
  {
    "objectID": "projects/DeepLCMS/DeepLCMS.html",
    "href": "projects/DeepLCMS/DeepLCMS.html",
    "title": "DeepLCMS: A framework that leverages transfer learning for the classification of pseudo images in mass spectrometry-based analysis",
    "section": "",
    "text": "Representative examples of DeepLCMS predictions accompanied by their corresponding probability estimates.\nWelcome to DeepLCMS, a project that combines mass spectrometry analysis with the power of deep learning models!\nUnlike conventional methods, DeepLCMS streamlines the data processing pipeline, bypassing the need for time-consuming and expertise-dependent steps such as peak alignment, data annotation and quantitation. Instead, it relies on the power of deep learning by recognizing important features to directly classify mass spectrometry-based pseudo-images with high accuracy. To demonstrate the capabilities of pre-trained neural networks for high-resolution LC/MS data, we successfully apply our convolutional neural network (CNN) to categorize substance abuse cases. We utilize the openly available Golestan Cohort Study’s metabolomics LC/MS data to train and evaluate our CNN (Pourshams et al. 2010; Ghanbari et al. 2021; Li et al. 2020). We also go beyond classification by providing insights into the network’s decision-making process using the TorchCam library. TorchCam allows us to analyze the regions of interests that influence the network’s classifications, helping us identify key compound classes that play a crucial role in differentiating between them. This approach empowers us to unlock novel insights into the underlying molecular classes and potential pathways that drive phenotypic variations much faster compared to traditional methods.\nWe believe that applying cutting-edge CNN-based technologies, such as DeepLCMS, for non-discriminatory mass spectrometry analysis has the potential to significantly enhance the field of mass spectrometry analysis, enabling faster, more streamlined, and more informative data interpretation, particularly in domains that demand high-throughput and rapid decision-making processes. Its capacity to directly classify pseudo-images without extensive preprocessing could unlock a wealth of opportunities for researchers and clinicians alike."
  },
  {
    "objectID": "projects/DeepLCMS/DeepLCMS.html#application-of-pretrained-neural-networks-for-mass-spectrometry-data",
    "href": "projects/DeepLCMS/DeepLCMS.html#application-of-pretrained-neural-networks-for-mass-spectrometry-data",
    "title": "DeepLCMS: A framework that leverages transfer learning for the classification of pseudo images in mass spectrometry-based analysis",
    "section": "Application of Pretrained Neural Networks for Mass Spectrometry Data",
    "text": "Application of Pretrained Neural Networks for Mass Spectrometry Data\nThe use of pre-trained neural networks for mass spectrometry data analysis is relatively new, with only a handful of publications available to date. These studies have demonstrated the potential of deep learning models to extract meaningful information from raw mass spectrometry data and perform predictive tasks without the need for extensive data preprocessing as required by the traditional workflows."
  },
  {
    "objectID": "projects/DeepLCMS/DeepLCMS.html#previous-research",
    "href": "projects/DeepLCMS/DeepLCMS.html#previous-research",
    "title": "DeepLCMS: A framework that leverages transfer learning for the classification of pseudo images in mass spectrometry-based analysis",
    "section": "Previous Research",
    "text": "Previous Research\n\nIn 2018, Behrmann et al. (2018) used deep learning techniques for tumor classification in Imaging Mass Spectrometry (IMS) data.\nIn 2020, Seddiki et al. (2020) utilized MALDI-TOF images of rat brain samples to assess the ability of three different CNN architectures – LeNet, Lecun, and VGG9 – to differentiate between different types of cancers based on their molecular profiles.\nIn 2021, Cadow et al. (2021) explored the use of pre-trained networks for the classification of tumors from normal prostate biopsies derived from SWATH-MS data. They delved into the potential of deep learning models for analyzing raw mass spectrometry data and performing predictive tasks without the need for protein quantification. To process raw MS images, the authors employed pre-trained neural network models to convert them into numerical vectors, enabling further processing. They then compared several classifiers, including logistic regression, support vector machines, and random forests, to accurately predict the phenotype.\nIn 2022, Shen et al. (2022) released deepPseudoMSI, a deep learning-based pseudo-mass spectrometry imaging platform, designed to predict the gestational age in pregnant women based on LC-MS-based metabolomics data. This application consists of two components: Pseudo-MS Image Converter: for converting LC-MS data into pseudo-images and the deep learning model itself.\n\n\n\n\n\n\n\nProject Structure\n\n\n\nTo accommodate the high computational demands of neural network training, the DeepLCMS project is divided into two main parts. The first part focuses on data preprocessing, specifically converting LC/MS data into pseudo-images using the PyOpenMS library, which is written in C++ and optimized for efficiency. This task can be handled on a CPU, and the corresponding source code is found in the src/cpu_modules directory.\nTo effectively train the neural networks, the project utilizes the PyTorch Lightning framework, which facilitates the development of a well-structured, modular codebase. Training experiments are conducted within Jupyter Notebooks running on Google Colab, a cloud platform offering free access to GPUs. The training code and associated modules reside within the src/gpu_modules directory, seamlessly integrating with Google Colab for effortless module imports."
  },
  {
    "objectID": "projects/DeepLCMS/DeepLCMS.html#dataset",
    "href": "projects/DeepLCMS/DeepLCMS.html#dataset",
    "title": "DeepLCMS: A framework that leverages transfer learning for the classification of pseudo images in mass spectrometry-based analysis",
    "section": "Dataset",
    "text": "Dataset\nTo ensure the feasibility of our proof-of-concept demonstration, we selected a suitable dataset from the Metabolomics Workbench. We prioritized studies with distinct groups and a minimum sample size of 200. Additionally, we selected a dataset with a disk requirement of less than 50 GB to minimize computational resource demands. Based on these criteria, we identified the Golestan Cohort Study (Pourshams et al. 2010). This study, conducted in northeastern Iran comprising approximately 50,000 individuals, primarily investigates the risk factors for upper gastrointestinal cancers in this high-risk region. Quantitative targeted liquid chromatography mass spectrometric (LC-MS/MS) data was obtained for a subset of the cohort and collected at the University of North Carolina at Chapel Hill to identify distinct biochemical alterations induced by opium consumption (Ghanbari et al. 2021; Li et al. 2020). The dataset consisted of 218 opioid users and 80 non-users. After initial data inspection and conversion to mzML format using the ProteoWizard 3.0.22155 software, files were divided into training (n = 214), validation (n = 54), and test (n = 30) sets. To evaluate the impact of image characteristics and augmentation techniques on classification performance, four datasets were prepared. One dataset contained lower quality, more pixalated images due to the bin size set to 500 × 500 using the numpy library’s histogram2d function and was named ST001618_Opium_study_LC_MS_500. Another dataset, while also using bin size of 500 x 500, it also incorporated data augmentation using the augment_images function, generating nine additional images per training set with random offsets of up to 5 units in both x and y directions. This dataset was named ST001618_Opium_study_LC_MS_500_augmented. The third dataset employed a higher bin size of 1000 × 1000 leading to sharper pseudoimages and was named ST001618_Opium_study_LC_MS_1000. The final dataset employed image augmentation technique, generating nine additional images per training set with random offsets of up to 5 units in both x and y directions and used a bin size of 1000 × 1000. It was named ST001618_Opium_study_LC_MS_1000_augmented."
  },
  {
    "objectID": "projects/DeepLCMS/DeepLCMS.html#software",
    "href": "projects/DeepLCMS/DeepLCMS.html#software",
    "title": "DeepLCMS: A framework that leverages transfer learning for the classification of pseudo images in mass spectrometry-based analysis",
    "section": "Software",
    "text": "Software\nThe code base relied on the following software packages and versions on Google Colab: PyTorch Lightning 2.1.3, Pytorch Image Models (timm) 0.9.12, torchinfo 1.8.0, Optuna 3.5.0, TorchCam 0.4.0, pandas 2.1.3, NumPy 1.26, and Matplotlib 3.7.1. The remaining packages and their respective versions can be found in the requirements.txt file."
  },
  {
    "objectID": "projects/DeepLCMS/DeepLCMS.html#creating-the-pseudo-images",
    "href": "projects/DeepLCMS/DeepLCMS.html#creating-the-pseudo-images",
    "title": "DeepLCMS: A framework that leverages transfer learning for the classification of pseudo images in mass spectrometry-based analysis",
    "section": "Creating the pseudo images",
    "text": "Creating the pseudo images\nMass spectrometry-based pseudo images are generated using the plot_2D_spectra_overview function, which creates a 2D heatmap of the LC/MS data. The function first loads the data in mzML format from the provided file path using the OpenMS library. A 2D histogram is then created, with the x-axis representing the retention time (Rt) and the y-axis representing the m/z values. The intensity values of the histogram are normalized using the highest peak at a given Rt. To enhance the visualization of the data, a Gaussian filter is applied to reduce noise, and the filtered image is scaled to the range 0-255. The histogram is then logarithmically transformed. Finally, pseudoimages are created using matplotlib, with the colormap set to ‘jet’."
  },
  {
    "objectID": "projects/DeepLCMS/DeepLCMS.html#untargeted-data-analysis-with-ms-dial",
    "href": "projects/DeepLCMS/DeepLCMS.html#untargeted-data-analysis-with-ms-dial",
    "title": "DeepLCMS: A framework that leverages transfer learning for the classification of pseudo images in mass spectrometry-based analysis",
    "section": "Untargeted data analysis with MS-DIAL",
    "text": "Untargeted data analysis with MS-DIAL\nTo assess the model’s decision-making process and enable interpretability, we analyzed two pooled samples (sp_10 and sp_20) using MS-DIAL (Tsugawa et al. 2015), a data-independent MS/MS deconvolution software, in positive mode applying the default settings. For metabolite identification, the MSP database from the MS-DIAL website was utilized, that contains ESI-MS/MS fragmentation data for 16,481 unique standards."
  },
  {
    "objectID": "projects/DeepLCMS/DeepLCMS.html#selecting-a-model-architecture-family",
    "href": "projects/DeepLCMS/DeepLCMS.html#selecting-a-model-architecture-family",
    "title": "DeepLCMS: A framework that leverages transfer learning for the classification of pseudo images in mass spectrometry-based analysis",
    "section": "Selecting a model architecture family",
    "text": "Selecting a model architecture family\nFrom the readily available model architecture families provided by Pytorch Image Models, 68 unique ones were selected as representative examples of a given class and filtered based on their parameter count, selecting those with parameters counts between 10 and 20 million to allow for an unbiased comparison. Out of the 68 selected 32 were subsequently underwent training with validation metrics recorded. According to Table 1 (arranged based on validation loss), the MobileOne S3 emerged as the top performer with an F1 score of 0.95. It was followed by DenseNet (F1 = 0.92), MobileViTV2 (F1 = 0.90), and ConvNeXt Nano (F1 = 0.84). RepVit M3 rounded out the top five with an F1 score of 0.86. To delve deeper into the performance of each architecture family, we also evaluated all individual architectures within each family (Table 2).\n\n\nCode\n(\n    pd.read_csv(\"exp-1-result.csv\")\n    .rename(\n        columns=lambda df: df.replace(\"_\", \" \").replace(\"val\", \"validation\").title()\n    )\n    .rename(columns={\"Minimal Param Model Count\": \"Parameter Count (M)\"})\n    .round(2)\n)\n\n\n\n\n\n\n\nTable 1: Validation metrics of model architectural families from the PyTorch Image Models library using models with parameter counts ranging from 10 to 20 million.\n\n\n\nModel Family\nValidation Accuracy\nValidation F1\nValidation Precision\nValidation Recall\nValidation Loss\nParameter Count (M)\n\n\n\n\n0\nmobileone_s3.apple_in1k\n0.93\n0.95\n1.00\n1.00\n0.26\n10.17\n\n\n1\ndensenet169.tv_in1k\n0.91\n0.92\n0.88\n1.00\n0.36\n14.15\n\n\n2\nmobilevitv2_150.cvnets_in22k_ft_in1k_384\n0.89\n0.90\n1.00\n0.88\n0.38\n10.59\n\n\n3\nmobilevitv2_150.cvnets_in22k_ft_in1k_384\n0.89\n0.90\n1.00\n0.88\n0.38\n10.59\n\n\n4\nconvnext_nano.in12k_ft_in1k\n0.76\n0.84\n0.74\n1.00\n0.42\n15.59\n\n\n5\nrepvit_m3.dist_in1k\n0.87\n0.86\n0.95\n1.00\n0.42\n10.68\n\n\n6\ncait_xxs24_224.fb_dist_in1k\n0.85\n0.88\n0.82\n1.00\n0.44\n11.96\n\n\n7\nrepvgg_a1.rvgg_in1k\n0.80\n0.86\n0.77\n1.00\n0.45\n14.09\n\n\n8\nefficientformer_l1.snap_dist_in1k\n0.91\n0.92\n0.86\n1.00\n0.45\n12.29\n\n\n9\nhrnet_w18_small.gluon_in1k\n0.74\n0.83\n0.73\n1.00\n0.46\n13.19\n\n\n10\ngmlp_s16_224.ra3_in1k\n0.74\n0.83\n0.73\n1.00\n0.46\n19.42\n\n\n11\nfastvit_sa12.apple_dist_in1k\n0.74\n0.83\n0.73\n1.00\n0.48\n11.58\n\n\n12\nlambda_resnet26t.c1_in1k\n0.78\n0.84\n0.76\n1.00\n0.49\n10.96\n\n\n13\nefficientvit_m5.r224_in1k\n0.74\n0.83\n1.00\n1.00\n0.49\n12.47\n\n\n14\neca_resnext26ts.ch_in1k\n0.76\n0.82\n0.75\n1.00\n0.49\n10.30\n\n\n15\nghostnetv2_160.in1k\n0.72\n0.82\n0.72\n1.00\n0.50\n12.39\n\n\n16\nnf_regnet_b1.ra2_in1k\n0.89\n0.91\n0.88\n1.00\n0.50\n10.22\n\n\n17\ncoatnet_nano_rw_224.sw_in1k\n0.76\n0.84\n0.77\n1.00\n0.50\n15.14\n\n\n18\nlegacy_seresnet18.in1k\n0.72\n0.82\n0.72\n1.00\n0.51\n11.78\n\n\n19\nhaloregnetz_b.ra3_in1k\n0.74\n0.83\n0.74\n1.00\n0.51\n11.68\n\n\n20\nxcit_tiny_24_p8_384.fb_dist_in1k\n0.78\n0.84\n0.77\n1.00\n0.52\n12.11\n\n\n21\nresmlp_12_224.fb_distilled_in1k\n0.72\n0.82\n0.72\n1.00\n0.52\n15.35\n\n\n22\ngcvit_xxtiny.in1k\n0.72\n0.82\n0.83\n1.00\n0.53\n12.00\n\n\n23\nresnest14d.gluon_in1k\n0.78\n0.84\n0.82\n1.00\n0.53\n10.61\n\n\n24\nresnest14d.gluon_in1k\n0.78\n0.84\n0.82\n1.00\n0.53\n10.61\n\n\n25\nedgenext_base.in21k_ft_in1k\n0.72\n0.82\n0.72\n1.00\n0.54\n18.51\n\n\n26\nmaxvit_nano_rw_256.sw_in1k\n0.78\n0.84\n0.78\n1.00\n0.54\n15.45\n\n\n27\ncoat_mini.in1k\n0.72\n0.82\n0.72\n1.00\n0.58\n10.34\n\n\n28\ntf_efficientnetv2_b2.in1k\n0.80\n0.84\n1.00\n0.90\n0.68\n10.10\n\n\n29\nfbnetv3_d.ra2_in1k\n0.65\n0.76\n0.76\n0.82\n0.82\n10.31\n\n\n30\nrexnet_200.nav_in1k\n0.70\n0.78\n0.80\n0.82\n0.91\n16.37\n\n\n31\nmixnet_xl.ra_in1k\n0.72\n0.82\n0.76\n1.00\n1.11\n11.90\n\n\n\n\n\n\n\n\nAs shown in Table 2, the top four positions based on the validation metrics were occupied by models belonging to the ConvNeXt family. These models, particularly the first two (convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384 with validation loss 0.19, convnext_large_mlp.clip_laion2b_augreg_ft_in1k with validation loss 0.22), were pre-trained on the extensive LAION-2B dataset and fine-tuned on ImageNet-1k, enabling them to learn complex patterns that generalize well to unseen data. It is plausible that this extensive dataset and sophisticated model architecture played a pivotal role in their exceptional performance in this task. The dominance of the ConvNeXt family is noteworthy, suggesting their effectiveness in handling complex data such as mass spectrometry pseudoimages. Apple’s MobileOne also demonstrated remarkable results (validation loss = 0.27), ranking fourth in terms of validation loss. Finally, MobileViT (validation loss = 0.27), a lightweight network, secured the seventh position. While the ConvNeXt models consistently outperformed other models in terms of validation metrics, it is essential to recognize that their larger size and higher parameter count translate into increased computational demands for training and heightened susceptibility to overfitting. Their complexity demands careful hyperparameter tuning and regularization techniques to mitigate overfitting and achieve optimal performance.\n\n\nCode\n(\n    pd.read_csv(\"exp-1-best_models.csv\")\n    .rename(\n        columns=lambda df: df.replace(\"_\", \" \").replace(\"val\", \"validation\").title()\n    )\n    .round(2)\n    .head(10)\n)\n\n\n\n\n\n\n\nTable 2: Top 10 models based on evaluation metrics of all model architectures in families from the PyTorch Image Models library, regardless of their parameter counts.\n\n\n\nEpoch\nExperiment\nParam Count\nTrain Accuracy\nTrain F1\nTrain Loss\nTrain Precision\nTrain Recall\nValidation Accuracy\nValidation F1\nValidation Loss\nValidation Precision\nValidation Recall\n\n\n\n\n0\n35\nconvnext_large_mlp.clip_laion2b_augreg_ft_in1k...\n200.13\n0.92\n0.94\n0.23\n0.94\n0.94\n0.94\n0.95\n0.19\n0.96\n0.93\n\n\n1\n42\nconvnext_large_mlp.clip_laion2b_augreg_ft_in1k\n200.13\n0.86\n0.90\n0.31\n0.90\n0.89\n0.93\n0.93\n0.22\n0.91\n0.97\n\n\n2\n41\nconvnext_large_mlp.clip_laion2b_soup_ft_in12k_...\n200.13\n0.90\n0.93\n0.25\n0.91\n0.96\n0.94\n0.95\n0.22\n0.96\n0.93\n\n\n3\n42\nconvnext_large_mlp.clip_laion2b_soup_ft_in12k_...\n200.13\n0.88\n0.92\n0.25\n0.92\n0.92\n0.93\n0.94\n0.23\n0.96\n0.91\n\n\n4\n39\nmobileone_s3.apple_in1k\n10.17\n0.87\n0.92\n0.35\n0.88\n0.96\n0.89\n0.91\n0.27\n0.88\n0.95\n\n\n5\n45\nconvnext_base.clip_laion2b_augreg_ft_in12k_in1...\n88.59\n0.88\n0.92\n0.31\n0.89\n0.95\n0.83\n0.87\n0.27\n0.84\n0.93\n\n\n6\n35\nconvnextv2_large.fcmae_ft_in22k_in1k_384\n197.96\n0.86\n0.91\n0.35\n0.87\n0.94\n0.94\n0.96\n0.27\n0.97\n0.95\n\n\n7\n49\nmobilevitv2_200.cvnets_in22k_ft_in1k_384\n18.45\n0.91\n0.94\n0.34\n0.95\n0.93\n0.94\n0.95\n0.27\n1.00\n0.91\n\n\n8\n44\nconvnext_large.fb_in22k_ft_in1k_384\n197.77\n0.86\n0.90\n0.35\n0.87\n0.94\n0.85\n0.88\n0.30\n0.85\n0.91\n\n\n9\n37\nconvnextv2_base.fcmae_ft_in22k_in1k_384\n88.72\n0.81\n0.87\n0.37\n0.84\n0.91\n0.91\n0.90\n0.30\n0.96\n0.86\n\n\n\n\n\n\n\n\nTo further assess the suitability of these models and the consistency of their performance, we trained the top three performing models from each family five consecutive times and calculated the median and standard deviation of their validation metrics (Figure 1). This approach allowed us to identify the models that exhibited the most consistent validation performance across multiple training runs. Statistical significance across replicates was evaluated using Dunn’s test with Benjamini-Hochberg correction. The results revealed that the MobileViT2 family performed similarly to the ConvNeXt across all five training runs, except for the validation losses where ConvNeXt exhibited the lowest values with 0.21. According to the replication study, the ConvNeXt emerged as the most effective model, surpassing both MobileOne and MobileViT2. Despite exhibiting similar patterns, ConvNeXt exhibited a statistically significant advantage over MobileViT2 in validation loss (p = 0.01) and achieved significantly better validation recall (p &lt; 0.05). Among the three models tested, MobileOne consistently underperformed its counterparts in all performance metrics, except for validation recall, where it narrowly outperformed MobileViT2 (p &lt; 0.05). In light of these findings, we opted to use the ConvNeXt large model in further experiments.\n\n\nCode\nPIL.Image.open(\"exp-2-replicates_result.png\").convert(\"RGB\")\n\n\n\n\n\nFigure 1: Median and standard deviation of best validation metrics achieved during consecutive trainings"
  },
  {
    "objectID": "projects/DeepLCMS/DeepLCMS.html#selecting-image-properties-as-hyperparameters",
    "href": "projects/DeepLCMS/DeepLCMS.html#selecting-image-properties-as-hyperparameters",
    "title": "DeepLCMS: A framework that leverages transfer learning for the classification of pseudo images in mass spectrometry-based analysis",
    "section": "Selecting image properties as hyperparameters",
    "text": "Selecting image properties as hyperparameters\nNext, we explored the impact of image sharpness and offsets along the x and y axis as a method for image augmentation to replicate the effects of Rt shifts and incidental mass calibration drifts. Figure 2 illustrates how the number of bins employed in the np.histogram2d function within the plot_2D_spectra_overview function—which serves to transform LC/MS files into pseudo images—influences the image sharpness. Here, the bins parameter signifies the number of bins in both the x and y dimensions, which are represented by nx and ny, respectively. This results in the formation of an nx by ny bin grid across the data for the histogram. By increasing the number of bins (from 500, 500 to 1000, 1000), one could obtain a sharper image, but it could also lead to increased noise and computational complexity.\nFurthermore, we sought to evaluate the impact of image augmentation, specifically the generation of multiple, modified images prior to image training, on the validation metrics. The augment_images function applies random offsets to the specified image, producing a designated number— 9 in our case—of augmented images with a maximum horizontal offset of five and a maximum vertical offset of five pixels.\n\n\nCode\nimg_paths = list(Path.cwd().glob(\"exp_3_U_2*\"))\n\nfig = plt.figure(figsize=(9, 9))\nrows, cols = 2, 2\n\nfor idx, img_path in zip(range(1, rows * cols + 1), img_paths):\n    fig.add_subplot(rows, cols, idx)\n    img = PIL.Image.open(img_path).convert(\"RGB\")\n    plt.imshow(img)\n    plt.title(img_path.stem[6:])\n    plt.axis(False)\n\n\n\n\n\nFigure 2: Examples of images obtained after applying different bin sizes and offsets as imitation of Rt shift. The amount of offset along the x and y axes is given in the image name. Image quality as a function of the number of bins is set to either 500, indicating a lower-quality image, or 1000, indicating a higher-quality image.\n\n\n\n\nFigure 3 illustrates, image sharpness in combination with image augmentation had a significant impact on the validation loss. Setting the bin size to 500 and subsequent augmentation resulted in the lowest validation loss (0.129) and the highest validation F1 (0.982). Based on these findings, less image sharpness and more training images, obtained through data augmentation prior to training, had a beneficial effect on the validation metrics. Taken together, the final training and fine-tuning is conducted on images with bin size set to 500 and subsequent augmentation applied resulting in the generation of nine extra samples per training image.\n\n\nCode\nPIL.Image.open(\"exp_3_experiment_result_1.png\").convert(\"RGB\")\n\n\n\n\n\nFigure 3: Training and validation metrics of experiments involving the selection of image properties as hyperparameters"
  },
  {
    "objectID": "projects/DeepLCMS/DeepLCMS.html#hyperparameter-tuning-with-optuna",
    "href": "projects/DeepLCMS/DeepLCMS.html#hyperparameter-tuning-with-optuna",
    "title": "DeepLCMS: A framework that leverages transfer learning for the classification of pseudo images in mass spectrometry-based analysis",
    "section": "Hyperparameter tuning with Optuna",
    "text": "Hyperparameter tuning with Optuna\nOptuna, an open-source hyperparameter optimization framework, delivers an automated and efficient approach for optimizing hyperparameters for machine learning and deep learning models. The framework operates by defining an objective function that could be either maximized or minimized. This function encapsulates the model and the metric to be optimized. Within the objective function, hyperparameters are suggested using a trial object. Optuna supports various types of hyperparameters, including float, integer, and categorical. The core of Optuna’s operation lies in its optimization process, which employs state-of-the-art algorithms to search the hyperparameter space efficiently. It can also prunes unpromising trials to conserve computational resources. To optimize the model’s performance, we employed Optuna to identify the most effective optimizer from a pool of four: Adam, AdamW, Adamax, and RMSprop. Additionally, we sought an optimal learning rate scheduler, choosing between CosineAnnealingLR and ReduceLROnPlateau, based on the validation loss achieved during training. After extensive optimization, the combination of Adamax optimizer and CosineAnnealingLR learning rate scheduler yielded the lowest validation loss (0.23), resulting in its selection for subsequent experiments."
  },
  {
    "objectID": "projects/DeepLCMS/DeepLCMS.html#evaluating-the-final-model-on-the-test-set",
    "href": "projects/DeepLCMS/DeepLCMS.html#evaluating-the-final-model-on-the-test-set",
    "title": "DeepLCMS: A framework that leverages transfer learning for the classification of pseudo images in mass spectrometry-based analysis",
    "section": "Evaluating the final model on the test set",
    "text": "Evaluating the final model on the test set\nThe final model training was conducted using the optimal settings determined during the optimization experiments, employing transfer learning of a pre-trained ConvNeXt large model, with all of its layers frozen except for the last classification head. The learning rate was automatically determined to be 0.006 based on the learning rate finder tool implemented by PyTorch Lightning. For the training, we utilized the augmented dataset (Figure 4) that featured bin size set to 500 for both dimensions. To prevent overfitting and promote better generalization capabilities, random erasing (probability=1.00) and color jitter (probability=0.25) were used. This technique effectively augments the training data, prompting the model to minimize its dependence on specific regions of the image thus preveting the model from memorizing limited patterns and encourages it to generalize better to unseen data. The underlying concept resembles a dropout layer, a common component of neural network architectures. The optimized model achieved validation metrics with loss of 0.138, validation precision of 0.963, F1 0.946, validation accuracy exceeding 0.944, and validation recall reaching 0.93.\nDuring training, we employed early stopping to prevent overfitting, terminating training if the validation loss fails to improve for 10 consecutive epochs. Additionally, we utilized PyTorch Lightning’s built-in ModelCheckpoint to save the best-performing model, ensuring that the model that achieved the highest validation metrics was preserved and evaluated later on.\n\n\nCode\nPIL.Image.open(\"exp-5-transformed_grid.png\").convert(\"RGB\")\n\n\n\n\n\nFigure 4: Example of augmented dataloader images used during training time, demonstrating the application of random erasing and color jitter to reduce overfitting\n\n\n\n\nAfter the training was completed, the best-performing model was reloaded, and test metrics were calculated using the 30 test samples present in our dataset. According to the final evaluation, the model produced 20 True Positives, 7 True Negatives, 2 False Negatives, and 1 False Positive.\nThese results translate to: Precision = 0.952, Recall = 0.909, F1 = 0.930, Accuracy = 0.90.\nWhile the model achieved impressive validation metrics, the slight drop of performance on the test set compared to the validation metrics indicates potential overfitting. Further optimization of the model, particularly during the method optimization phase, could be achieved through cross-validation strategies. However, this approach might necessitate excessive computational resources."
  },
  {
    "objectID": "projects/DeepLCMS/DeepLCMS.html#model-interpretability",
    "href": "projects/DeepLCMS/DeepLCMS.html#model-interpretability",
    "title": "DeepLCMS: A framework that leverages transfer learning for the classification of pseudo images in mass spectrometry-based analysis",
    "section": "Model interpretability",
    "text": "Model interpretability\n\nEstimating probability\nTo assess the model’s confidence in its predictions, we can transform the model’s raw output, which is the binary cross-entropy loss derived from a set of logits obtained using PyTorch’s BCEWithLogitsLoss function, into prediction probabilities using a sigmoid function. These converted values reflect the probability that the model assigns to a given prediction belonging to Class 1 (Opioid User). Figure 5 illustrates this concept, showing that in the first instance, the model estimates a 43% probability of the instance belonging to Class 1, suggesting a higher likelihood of it being a Non-Opioid User (and this prediction proved correct).\n\n\nCode\nPIL.Image.open(\"exp-5-prediction_matrix.png\").convert(\"RGB\")\n\n\n\n\n\nFigure 5: Set of images from the test set, along with their corresponding prediction probabilities for the Class 1 (Opidid User). The true labels are displayed beside the predictions. Green labels indicate successful predictions, whereas red labels denote unsuccessful ones\n\n\n\n\n\n\nLooking at Layer Class Activation Maps\nTo gain insights into the regions of an image our model focuses on when making a prediction, we employed LayerCAM, a tool from the TorchCam package. LayerCAM (Jiang et al. 2021), short for Layer Class Activation Maps, is a technique for generating class activation maps from distinct layers of a CNN. These regions of interests highlight key features the model utilizes to differentiate between opioid users and non-opioid users. Class activation maps are typically derived from the final convolutional layer of a CNN, highlighting discriminative object regions for the class of interest. Such a tool could be particularly valuable for validating the insights derived from our deep learning model, particularly when compared to more targeted or untargeted metabolomics data.\n\n\nCode\nPIL.Image.open(\"exp-5-plot_activation.png\").convert(\"RGB\")\n\n\n\n\n\nFigure 6: Regions of interest identified by LayerCAM, demonstrating the specific areas of the image that our model deems most relevant for making predictions.\n\n\n\n\n\n\nOverlaying the untargeted metabolomics data with the class activation map\nTo gain a better understanding of the specific metabolic features that influence the model’s decision-making process in classifying individuals into either group, we analyzed two examples of the pooled samples using MS-DIAL (Tsugawa et al. 2015), an open-source software pipeline designed for data-independent MS/MS deconvolution and identification of small molecules. Subsequently, metabolite annotation was performed using the MSP database, which houses ESI-MS/MS fragmentation data for 16,481 unique authentic standards. The MSP database served as the basis for assigning compound identification levels: 999 denoted instances where MS-DIAL could not identify the feature due to missing MS/MS fragmentation spectrum or non-matching library entries. A tentative identification was assigned with 530, indicating a match based on accurate mass comparison. A more confident tentative identification was marked with 430, indicating spectral correspondence between the feature and the library entry’s MS/MS spectrum.\nTo leverage the untargeted metabolomics data, we can, for example, construct a function similar to overlay_untargeted_data, which facilitates the extraction of specific regions from the feature map derived from MS-DIAL based on retention time and accurate mass. These extracted features can be overlaid as a scatter plot alongside our activation map. This approach, in theory, should provide a deeper understanding of the features that play a crucial role in differentiating between the various classes.\n\n\nCode\ndef overlay_untargeted_data(\n    query_str: str,\n    ms_dial_location: Path,\n    background_image_location: Path,\n    crop_area: Tuple[int, int, int, int],\n):\n    \"\"\"\n    Overlay untargeted metabolomics data on a background image.\n\n    Parameters:\n    - query_str (str): Query string for filtering the data.\n    - ms_dial_location (str): Location of the mass spectrometry data file.\n    - background_image_location (str): Location of the background image.\n    - crop_area (tuple): Coordinates for cropping the background image (left, upper, right, lower).\n\n    Returns:\n    - composite_img (PIL.Image.Image): Overlay of data on the background image.\n    - df (pd.DataFrame): Filtered and processed DataFrame.\n    \"\"\"\n    # Read and preprocess the mass spectrometry data\n    df = (\n        pd.read_csv(\n            ms_dial_location,\n            skiprows=4,\n            usecols=[\n                \"Average Rt(min)\",\n                \"Average Mz\",\n                \"Metabolite name\",\n                \"Ontology\",\n                \"Annotation tag (VS1.0)\",\n                \"MS/MS matched\",\n                \"SP_10\",\n                \"SP_20\",\n            ],\n        )\n        .rename(columns=lambda x: re.sub(r\"\\W|[%/]\", \"_\", x.lower()))\n        .query(\"annotation_tag__vs1_0_ == '430' or annotation_tag__vs1_0_ == '530'\")\n        .query(query_str)\n        .drop_duplicates(subset=\"metabolite_name\")\n    )\n\n    # Create scatterplot\n    sns.scatterplot(\n        data=df,\n        x=\"average_rt_min_\",\n        y=\"average_mz\",\n        alpha=0.5,\n        s=100,\n        color=\"black\",\n    )\n\n    # Set plot limits and turn off axis\n    plt.ylim(50, 750)\n    plt.xlim(0, 20)\n    plt.axis(False)\n\n    # Create the foreground image\n    buf = io.BytesIO()\n    plt.savefig(buf, format=\"jpeg\", bbox_inches=\"tight\", pad_inches=0)\n    buf.seek(0)\n    foreground = PIL.Image.open(buf)\n    plt.close()\n\n    # Create the background image\n    background = (\n        PIL.Image.open(background_image_location)\n        .convert(\"RGB\")\n        .crop(crop_area)\n        .resize(foreground.size)\n    )\n\n    # Create a mask\n    mask = PIL.Image.new(\"L\", foreground.size, 128)\n\n    # Composite the images\n    composite_img = PIL.Image.composite(background, foreground, mask)\n\n    return composite_img, df\n\n\nAs illustrated in Figure 7, and Figure 8, the most notable features identified by MS-DIAL with levels 430 and 530 are overlaid with two examples of the activation map generated by TorchCam. Importantly, morphine-3-glucuronide (Level 530), a major metabolite of morphine, stands out as a key feature influencing the model’s decision-making process. Additionally, EDDP (Level 430), a major metabolite of methadone, a synthetic opioid agonist used for chronic pain and opioid use disorder, is also considered by the model. While we observed tentative evidence suggesting that the metabolic pathways involving Kreb’s cycle, purine metabolism, central carbon metabolism, histone modification, and acetylation are also considered by the model, as it was pointed out by Li et al. (2020), the level of identification (530) indicates that these identifications are based solely on accurate mass measurement alone thus further investigation is warranted to confirm the identities of these potential biomarkers (Table 3).\n\n\nCode\ncomposite_img_pattern_1, pattern_df_1 = overlay_untargeted_data(\n    query_str=\"3.5&lt;average_rt_min_&lt;5 and 460&lt;average_mz&lt;500 or 5&lt;average_rt_min_&lt;6.5 and 575&lt;average_mz&lt;650 or 9.5&lt;average_rt_min_&lt;10 and 280&lt;average_mz&lt;320\",\n    ms_dial_location=\"PeakID_0_2023_12_25_17_39_09.csv\",\n    background_image_location=\"exp-5-plot_activation.png\",\n    crop_area=(25, 90, 850, 910),\n)\nfont = ImageFont.truetype(\"arial.ttf\", 16)\n\n# Create a Draw object\ndraw = ImageDraw.Draw(composite_img_pattern_1)\n\n# Draw the texts at different locations\ndraw.text((25, 100), \"Morphine-3-Glucuronide\", (0, 0, 0), font=font)\ndraw.text((100, 200), \"EDDP, Octanoyl-L-Carnitine, Aspirine\", (0, 0, 0), font=font)\n\ncomposite_img_pattern_1\n\n\n\n\n\nFigure 7: Example-1 of interest overlayed with proposed feature names tentativelyi identified by MS-DIAL.\n\n\n\n\n\n\nCode\ncomposite_img_pattern_2, pattern_df_2 = overlay_untargeted_data(\n    query_str=\"(16.5&lt;average_rt_min_&lt;18 and 180&lt;average_mz&lt;250) or (4&lt;average_rt_min_&lt;5.5 and 50&lt;average_mz&lt;100) or (8&lt;average_rt_min_&lt;12 and 50&lt;average_mz&lt;100) or (8.5&lt;average_rt_min_&lt;9 and 250&lt;average_mz&lt;300) or (12&lt;average_rt_min_&lt;14 and 200&lt;average_mz&lt;220) or (18&lt;average_rt_min_&lt;20 and 280&lt;average_mz&lt;350)\",\n    ms_dial_location=\"PeakID_0_2023_12_25_17_39_09.csv\",\n    background_image_location=\"exp-5-plot_activation.png\",\n    crop_area=(20, 1070, 850, 1890),\n)\n\nImageDraw.Draw(composite_img_pattern_2).text(\n    (100, 210), \"EDDP, Octanoyl-L-Carnitine, Aspirine\", (0, 0, 0), font=font\n)\ncomposite_img_pattern_2\n\n\n\n\n\nFigure 8: Example-2 of interest overlayed with proposed feature names tentativelyi identified by MS-DIAL.\n\n\n\n\n\n\nCode\npd.concat([pattern_df_1, pattern_df_2], axis=0).drop(columns=[\"sp_10\", \"sp_20\"]).head(\n    50\n)\n\n\n\n\n\n\n\nTable 3: List of 50 metabolites residing at regions of interests that were tentatively identified by MS-DIAL following the overlay of the activation map and the feature map\n\n\n\naverage_rt_min_\naverage_mz\nmetabolite_name\nontology\nannotation_tag__vs1_0_\nms_ms_matched\n\n\n\n\n3076\n9.998\n283.24149\nw/o MS2: 2-(N-butylacetamido)-N-cyclohexylbuta...\nTertiary carboxylic acid amides\n530\nFalse\n\n\n3297\n9.938\n297.06689\nw/o MS2: clausenin methyl ether\nLinear pyranocoumarins\n530\nFalse\n\n\n3379\n9.938\n301.06268\nw/o MS2: 3(2'-Chlorophenyl)-7-ethoxycoumarin\nIsoflav-3-enones\n530\nFalse\n\n\n3459\n9.599\n305.09894\nheraclenol\nPsoralens\n430\nTrue\n\n\n3510\n9.917\n308.22070\nw/o MS2: Dihydrocapsaicin\nMethoxyphenols\n530\nFalse\n\n\n3571\n9.543\n312.15945\nw/o MS2: atropine\nTropane alkaloids\n530\nFalse\n\n\n3587\n9.622\n313.06686\nw/o MS2: (-)Catechin\nCatechins\n530\nFalse\n\n\n5424\n3.893\n460.15945\nw/o MS2: Villol\nRotenones\n530\nFalse\n\n\n5425\n4.167\n460.19565\nw/o MS2: Apixaban\nPhenylpiperidines\n530\nFalse\n\n\n5444\n4.369\n462.17502\nw/o MS2: Morphine_3_Glucuronide\nMorphinans\n530\nFalse\n\n\n5557\n4.553\n474.17270\nw/o MS2: Raloxifene\nAryl-phenylketones\n530\nFalse\n\n\n5611\n3.744\n480.09711\nw/o MS2: FOLIC ACID\nGlutamic acid and derivatives\n530\nFalse\n\n\n5655\n4.720\n486.19623\nw/o MS2: Smenathiazole B\nN-acyl-alpha amino acids and derivatives\n530\nFalse\n\n\n5668\n4.802\n488.15448\nw/o MS2: Dasatinib (BMS-354825)\nAromatic anilides\n530\nFalse\n\n\n5686\n4.539\n490.07681\nw/o MS2: Vemurafenib (PLX4032)\nAryl-phenylketones\n530\nFalse\n\n\n5709\n4.486\n492.24979\nw/o MS2: Hypaconine\nAconitane-type diterpenoid alkaloids\n530\nFalse\n\n\n5731\n4.002\n496.15677\nw/o MS2: NCGC00380381-01_C24H27NO9_\nSaccharolipids\n530\nFalse\n\n\n5743\n4.409\n498.17252\nw/o MS2: Tebipenem pivoxil (L-084)\nThienamycins\n530\nFalse\n\n\n6249\n5.595\n591.15735\nw/o MS2: Mulberroside A\nStilbene glycosides\n530\nFalse\n\n\n6264\n5.595\n595.13904\nw/o MS2: Tiliroside\nFlavonoid 3-O-p-coumaroyl glycosides\n530\nFalse\n\n\n6292\n5.613\n604.17505\nw/o MS2: SSR126768\nDimethoxybenzenes\n530\nFalse\n\n\n6294\n5.595\n605.17346\nw/o MS2: Naringin dihydrochalcone\nFlavonoid O-glycosides\n530\nFalse\n\n\n6359\n6.428\n630.22260\nw/o MS2: NCGC00347814-02_C29H37NO13_\nBenzylisoquinolines\n530\nFalse\n\n\n72\n4.042\n72.08070\nw/o MS2: Pyrrolidine; CE10; RWRDLPDLKQPQOW-UHF...\nPyrrolidines\n530\nFalse\n\n\n75\n4.985\n72.08073\nw/o MS2: Pyrrolidine; CE0; RWRDLPDLKQPQOW-UHFF...\nPyrrolidines\n530\nFalse\n\n\n78\n4.943\n73.06469\nw/o MS2: BUTANAL\nAlpha-hydrogen aldehydes\n530\nFalse\n\n\n94\n5.348\n76.03080\nw/o MS2: Glycine\nAlpha amino acids\n530\nFalse\n\n\n104\n4.844\n78.03378\n2-Aminoethanethiol\nAlkylthiols\n430\nTrue\n\n\n173\n4.943\n89.05956\nw/o MS2: Isobutyric acid\nCarboxylic acids\n530\nFalse\n\n\n218\n5.358\n95.04900\nw/o MS2: Phenol\n1-hydroxy-4-unsubstituted benzenoids\n530\nFalse\n\n\n222\n4.071\n95.06035\nw/o MS2: 4-Methylpyrimidine\nPyrimidines and pyrimidine derivatives\n530\nFalse\n\n\n226\n4.844\n96.04424\n3-Hydroxypyridine\nHydroxypyridines\n430\nTrue\n\n\n252\n8.776\n98.98405\nw/o MS2: Ortophosphate\nNon-metal phosphates\n530\nFalse\n\n\n253\n4.658\n99.04393\nw/o MS2: alpha-Methylene-gamma-butyrolactone\nGamma butyrolactones\n530\nFalse\n\n\n1393\n16.813\n181.04948\nAspirin\nAcylsalicylic acids\n430\nTrue\n\n\n1566\n17.420\n191.98239\nw/o MS2: 2-Chlorobenzenesulfonamide\nBenzenesulfonamides\n530\nFalse\n\n\n1613\n17.092\n194.97618\nw/o MS2: Orotic Acid\nPyrimidinecarboxylic acids\n530\nFalse\n\n\n1689\n17.184\n198.99387\nw/o MS2: Tioconazole\nBenzylethers\n530\nFalse\n\n\n1690\n17.155\n199.01105\nw/o MS2: PUTRESCINE DIHYDROCHLORIDE\nMonoalkylamines\n530\nFalse\n\n\n1701\n17.223\n199.13049\nw/o MS2: Tacrine\nAcridines\n530\nFalse\n\n\n1720\n12.290\n200.23743\nw/o MS2: N-Methyldodecylamine\nDialkylamines\n530\nFalse\n\n\n1784\n12.903\n204.08629\nw/o MS2: alpha-oxo-1h-indole-3-propanoic acid\nIndolyl carboxylic acids and derivatives\n530\nFalse\n\n\n1802\n17.464\n205.00497\nw/o MS2: Tilt(TM)\nDichlorobenzenes\n530\nFalse\n\n\n1862\n17.487\n209.00053\nw/o MS2: Kynurenine\nAlkyl-phenylketones\n530\nFalse\n\n\n1872\n12.139\n209.11678\nw/o MS2: Primin\nP-benzoquinones\n530\nFalse\n\n\n1897\n17.206\n210.97739\nw/o MS2: AZELAIC ACID\nMedium-chain fatty acids\n530\nFalse\n\n\n1923\n16.999\n212.97305\nw/o MS2: Aconitic Acid\nTricarboxylic acids and derivatives\n530\nFalse\n\n\n2034\n13.514\n219.19563\nw/o MS2: Tributylphosphine oxide\nOrganophosphine oxides\n530\nFalse\n\n\n2107\n17.939\n224.99295\nw/o MS2: 3-Hydroxykynurenine\nAlkyl-phenylketones\n530\nFalse\n\n\n2119\n17.230\n225.99634\nw/o MS2: Cyprodinil\nAniline and substituted anilines\n530\nFalse\n\n\n\n\n\n\n\n\nAlthough this is not the only approach to understanding the principles upon which the model bases its decisions, we believe that combining CNNs with MS/MS fragmentation-based untargeted metabolomics can be used as an example to gain a better understanding of the underlying mechanisms. Conversely, the model can also guide researchers in directing resources and efforts towards identifying specific regions of interest more effectively by performing more targeted measurements, such as MS/MS experiments, since we already know which features play significant roles in distinguishing between the classes. In particular, we recognize the potential benefits of such a model in high-throughput settings where rapid decisions are required, although thorough validation remains essential. This approach could be highly valuable in conjunction with techniques such as untargeted metabolomics and lipidomics, where an unbiased assessment of the phenotype is desirable, especially, when combined with simple sample preparation techniques such as protein precipitation.\nWhile the use of CNNs in high-throughput settings holds immense potential, it is crucial to acknowledge potential drawbacks. Further research is essential to understand how image preprocessing techniques can influence the decision-making process. Currently, we only implemented basic techniques, such as scaling, Gaussian noise reduction, and logarithmic transformation. The number of training examples could also pose a challenge. While transfer learning, as we demonstrated, does not demand a plethora of training examples, we still required over 200 samples for training. Nonetheless, once sufficient number of images are obtained for the initial training, one can retrain the model as new data becomes available. Additionally, it is essential to exercise caution against overfitting. While we implemented several strategies to mitigate overfitting and enhance generalizability, our testing metrics indicate a slight overfitting compared to the validation metrics. However, employing cross-validation may help address this issue. Finally, GPU availability is crucial for neural network training. While transfer learning does not require as much computational resources as training a network from scratch, utilizing a GPU is still recommended. Hence, having access to a GPU is highly advised. This project utilized the freely available GPU (NVIDIA Tesla T4) as part of the Google Colab. Once experimentation is complete, depending on the size of the model selected, inference may be able to be performed using a CPU."
  },
  {
    "objectID": "posts/Top 10 things I learned from Effective Pandas/2023_08_02-Top_10_things_I_learned_from_Effective_Pandas.html",
    "href": "posts/Top 10 things I learned from Effective Pandas/2023_08_02-Top_10_things_I_learned_from_Effective_Pandas.html",
    "title": "Top 10 things I learned from the book Effective Pandas by Matt Harrison",
    "section": "",
    "text": "Effective Pandas by Matt Harrison is a guide to the Pandas library, a powerful Python tool for data manipulation and analysis. The book covers a wide range of topics, from basic data structures to advanced techniques for data cleaning, transformation, and visualization.\nI have found Effective Pandas to be a captivating and thought-provoking read. The book offers a genuinely unique take on data wrangling, putting a great emphasis on the utility of chaining methods and utilizing the lambda function. I have found these ideas to be so useful and practical that I have revisited the book multiple times just to make sure I keep them fresh in my memory. I must have read the book from back to back at least 3-4 times.\nIn this article, I will share the top 10 (+1) things I learned from Effective Pandas. These are the concepts and techniques that I found to be the most useful and practical.\nWe will use the Real World Smartphone’s Dataset by Abhijit Dahatonde from Kaggle. Let’s get to it.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\npd.set_option('display.max_columns', None)\n\n\n\nLoad the dataset\n\n\nCode\ndf=pd.read_csv('smartphones.csv')\n# some info about the dataframe, such as dimensions and dtypes of columns\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 980 entries, 0 to 979\nData columns (total 22 columns):\n #   Column                     Non-Null Count  Dtype  \n---  ------                     --------------  -----  \n 0   brand_name                 980 non-null    object \n 1   model                      980 non-null    object \n 2   price                      980 non-null    int64  \n 3   avg_rating                 879 non-null    float64\n 4   5G_or_not                  980 non-null    int64  \n 5   processor_brand            960 non-null    object \n 6   num_cores                  974 non-null    float64\n 7   processor_speed            938 non-null    float64\n 8   battery_capacity           969 non-null    float64\n 9   fast_charging_available    980 non-null    int64  \n 10  fast_charging              769 non-null    float64\n 11  ram_capacity               980 non-null    int64  \n 12  internal_memory            980 non-null    int64  \n 13  screen_size                980 non-null    float64\n 14  refresh_rate               980 non-null    int64  \n 15  num_rear_cameras           980 non-null    int64  \n 16  os                         966 non-null    object \n 17  primary_camera_rear        980 non-null    float64\n 18  primary_camera_front       975 non-null    float64\n 19  extended_memory_available  980 non-null    int64  \n 20  resolution_height          980 non-null    int64  \n 21  resolution_width           980 non-null    int64  \ndtypes: float64(8), int64(10), object(4)\nmemory usage: 168.6+ KB\n\n\n\n\nTip #1: Use pd.assign more extensively\nThe pd.assign method in Pandas is a very powerful tool that can be used to create new columns, modify existing columns, or both. It is a very versatile method that can be used in a variety of ways.\nOne of the most important benefits of using the assign method is that it can be incorporated into method chaining. This means that you can chain multiple assign methods together to create a more concise and readable code. Another benefit of using the assign method is that it completely sidesteps the infamous SettingWithCopyWarning. This warning is often triggered when you try to modify an existing column in a DataFrame. However, the assign method creates a new DataFrame, so there is no need to worry about this warning.\nProblem statement: Let’s say we would like to capitalize the brand names located in the brand_name column as well as calculate the Pixels Per Inch (PPI). PPI can be calculated following the equation described by the Pixel density page on Wikipedia.\n\n\nCode\n(df\n .assign(brand_name=lambda df: df.brand_name.str.capitalize(), # capitalizes the brand names \n         PPI=lambda df: (np.sqrt(np.square(df.resolution_height) + np.square(df.resolution_width))\n                         .div(df.screen_size)\n                         .round(1)\n                        )\n        )\n .loc[:, ['brand_name','model','PPI']]\n .sort_values(by='PPI',ascending=False)\n .head(5)\n)\n\n\n\n\n\n\n\n\n\nbrand_name\nmodel\nPPI\n\n\n\n\n689\nSony\nSony Xperia 1 IV (12GB RAM + 512GB)\n642.6\n\n\n696\nSony\nSony Xperia Pro-I\n642.6\n\n\n688\nSony\nSony Xperia 1 II\n642.6\n\n\n655\nSamsung\nSamsung Galaxy S20\n566.0\n\n\n656\nSamsung\nSamsung Galaxy S20 5G\n566.0\n\n\n\n\n\n\n\n\n\nTip #2: Simplify the management of multiple if-else conditions using np.select\nIf our goal is to incorporate if-else logic seamlessly into our code, we can effortlessly achieve this using either pd.mask or pd.where. Yet, what approach should we adopt when we need to evaluate multiple conditions instead of just two? In such situations, we have two options: we can either employ successive pd.mask or pd.where calls, or we can take advantage of the np.select function as an alternative solution.\nProblem statement: We want to identify the top 3 and top 5 most popular processor brands in smartphones. To do this, we will first create two lists, one for the top 3 brands and one for the top 5 brands. Any processor brand that is not in either of these lists will be categorized as “Other”.\n\n\nCode\n# Let's create the two lists that contain the top3 and top5 brand names\ntop3=df.processor_brand.value_counts().head(3).index\ntop5=df.processor_brand.value_counts().head(5).index\nprint(f'Top 3 most popular processors: {top3.tolist()}')\nprint(f'Top 5 most popular processors: {top5.tolist()}')\n\n\nTop 3 most popular processors: ['snapdragon', 'helio', 'dimensity']\nTop 5 most popular processors: ['snapdragon', 'helio', 'dimensity', 'exynos', 'bionic']\n\n\n\n\nCode\n'''\nHere's an example that employs two successive pd.where calls:\nIn the first pd.where call, it checks whether the brand is in the top 3; if not, it assigns the label \"Top5\" to it.\nThen, in the second call, it checks if the value is in the top 5; if not, it appends the category \"Other\".\nAs you can see, the logic can become intricate and difficult to grasp, especially when dealing with numerous conditions, \nmaking the code cumbersome and hard to manage.\n'''\n(df\n .assign(frequency=lambda df: df.processor_brand\n         .where(df.processor_brand.isin(top3), other = 'Top5')\n         .where(df.processor_brand.isin(top5), other = 'Other')\n        )\n .loc[:, 'frequency']\n .value_counts()\n)\n\n\nsnapdragon    413\nhelio         201\ndimensity     177\nTop5           95\nOther          94\nName: frequency, dtype: int64\n\n\n\n\nCode\n'''\n Now Let's see np.select!\n It simplifies the process significantly. By providing a list of conditions we want to evaluate and their \n corresponding values if the condition evaluates to True, we can handle multiple conditions effortlessly. \n Additionally, we can specify a default value if none of the conditions evaluates to True, making the code \n much more straightforward and easier to manage. \n'''\n(df\n .assign(frequency=lambda df: np.select(condlist=[df.processor_brand.isin(top3), df.processor_brand.isin(top5)],\n                                        choicelist=[df.processor_brand,'Top5'],\n                                        default='Other'\n                                       )\n        )\n .loc[:, 'frequency']\n .value_counts()\n)\n\n\nsnapdragon    413\nhelio         201\ndimensity     177\nTop5           95\nOther          94\nName: frequency, dtype: int64\n\n\n\n\nTip #3 Filter rows or columns with the combination of pd.loc and lambda\nSome experienced Pandas users might consider the following concept trivial, but it was an eye-opener for me after reading the book. It turns out that combining pd.loc and lambda (or any custom functions) allows us to filter both rows and columns, depending on our specific needs.\nProblem statement: We are interested in identifying phones with a battery capacity greater than 5000mAh.\n\n\nCode\n(df\n .loc[lambda df: df.battery_capacity.gt(5000),['model', 'battery_capacity']] # here we use pd.gt() to select values greater than 5000\n .sort_values(by='battery_capacity')\n)\n\n\n\n\n\n\n\n\n\nmodel\nbattery_capacity\n\n\n\n\n70\nGoogle Pixel 6 Pro (12GB RAM + 256GB)\n5003.0\n\n\n69\nGoogle Pixel 6 Pro\n5003.0\n\n\n977\nXiaomi Redmi Note 9 Pro Max\n5020.0\n\n\n922\nXiaomi Redmi Note 10 Lite\n5020.0\n\n\n923\nXiaomi Redmi Note 10 Lite (4GB RAM + 128GB)\n5020.0\n\n\n...\n...\n...\n\n\n624\nSamsung Galaxy F63\n7000.0\n\n\n411\nOukitel WP9\n8000.0\n\n\n410\nOukitel WP21\n9800.0\n\n\n409\nOukitel WP19\n21000.0\n\n\n58\nDoogee V Max\n22000.0\n\n\n\n\n113 rows × 2 columns\n\n\n\n\n\nTip #4 Rename multiple columns effortlessly with rename and replace\nOK. This is a big one for me. I used this one multiple times already. The title pretty much says it all. Let’s say our column names contain spaces, which makes column selection by attribute access pretty much impossible. Now, what do we do? Well…Let’s see.\nProblem statement: We would like to remove all the underscores from our column names.\n\n\nCode\ndf.columns # original column names for reference\n\n\nIndex(['brand_name', 'model', 'price', 'avg_rating', '5G_or_not',\n       'processor_brand', 'num_cores', 'processor_speed', 'battery_capacity',\n       'fast_charging_available', 'fast_charging', 'ram_capacity',\n       'internal_memory', 'screen_size', 'refresh_rate', 'num_rear_cameras',\n       'os', 'primary_camera_rear', 'primary_camera_front',\n       'extended_memory_available', 'resolution_height', 'resolution_width'],\n      dtype='object')\n\n\n\n\nCode\n# column names after replacing underscores\n(df\n .rename(columns = lambda x: x.replace('_', ''))\n .columns\n)\n\n\nIndex(['brandname', 'model', 'price', 'avgrating', '5Gornot', 'processorbrand',\n       'numcores', 'processorspeed', 'batterycapacity',\n       'fastchargingavailable', 'fastcharging', 'ramcapacity',\n       'internalmemory', 'screensize', 'refreshrate', 'numrearcameras', 'os',\n       'primarycamerarear', 'primarycamerafront', 'extendedmemoryavailable',\n       'resolutionheight', 'resolutionwidth'],\n      dtype='object')\n\n\n\n\nTip #5: Use pd.clip to easily remove outliers\n\n\nCode\n# First, we'll identify the phone brands with the most number of handsets present in our dataset.\"\ntop10_brand_names = (df\n                     .brand_name\n                     .value_counts()\n                     .head(10)\n                     .index\n                     .tolist()\n                    )\nprint(top10_brand_names)\n\n# Then we will sort them based on median price\ntop10_brand_names_ordered = (df\n                             .loc[lambda x: x.brand_name.isin(top10_brand_names),['brand_name', 'price']]\n                             .groupby('brand_name')\n                             .median()\n                             .sort_values(by='price')\n                             .index\n                             .to_list()\n                            )\nprint(top10_brand_names_ordered)\n\n\n['xiaomi', 'samsung', 'vivo', 'realme', 'oppo', 'motorola', 'apple', 'oneplus', 'poco', 'tecno']\n['tecno', 'poco', 'realme', 'xiaomi', 'motorola', 'vivo', 'oppo', 'samsung', 'oneplus', 'apple']\n\n\n\n\nCode\nfig, axs = plt.subplots(1,2, figsize=(10,5))\n\n# For reference, this is what our box plot looks if we leave in the outlier values\n(df\n .loc[lambda x: x.brand_name.isin(top10_brand_names),['brand_name', 'price']] # filter rows based on top10_brand_names and select columns\n .pivot(columns='brand_name',values='price') # pivot to get the brand names on the x axis later on\n .loc[:, top10_brand_names_ordered] # order the columns based on median price\n .plot\n .box(title='Top 10 most popular smartphone brands \\n -outlier values included-', \n      rot=90,\n      ax=axs[0]\n     )\n)\n\n(df\n .loc[lambda x: x['brand_name'].isin(top10_brand_names), ['brand_name', 'price']]\n .pivot(columns='brand_name', values='price')\n .loc[:, top10_brand_names_ordered]\n .pipe(lambda df: df.assign(**{col : df[col].clip(lower=df[col].quantile(0.05), # this is called dictionary unpacking\n                                                  upper=df[col].quantile(0.95))\n                              for col in df.columns}))\n .plot\n .box(title='Top 10 most popular smartphone brands \\n -only values between 5th and 95th percentiles included-',\n      rot=90,\n      ax=axs[1]\n     )\n)\naxs[0].set(ylabel='Price in local currency')\nplt.tight_layout()\n\n\n\n\n\n\n\nTip #6: Find corrupted entries with str.extract\nHow often have you encountered the situation where, for some reason, a column that is expected to only contain numerical values displays object as its dtype? This often indicates the presence of some string values mixed within the column. It would be beneficial to promptly identify all the erroneous values, correct them, and proceed with our analysis smoothly. Let’s explore what we can do in such scenarios.\nProblem statement: We would like to identify any cells in a specific column that contain non-numerical values.\n\n\nCode\ndf_bad_values = df.copy(deep=True) # let's prepare a copy of the original dataframe \n\n# let's modify some of the values in the price column randomly:\ndf_bad_values.loc[np.random.randint(0, high=df_bad_values.shape[0], size=10), 'price'] = '.'\ndf_bad_values.loc[np.random.randint(0, high=df_bad_values.shape[0], size=10), 'price'] = '-'\ndf_bad_values.loc[np.random.randint(0, high=df_bad_values.shape[0], size=10), 'price'] = '*'\ndf_bad_values.info() # the modified dataframe's price column now returns object as dtype\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 980 entries, 0 to 979\nData columns (total 22 columns):\n #   Column                     Non-Null Count  Dtype  \n---  ------                     --------------  -----  \n 0   brand_name                 980 non-null    object \n 1   model                      980 non-null    object \n 2   price                      980 non-null    object \n 3   avg_rating                 879 non-null    float64\n 4   5G_or_not                  980 non-null    int64  \n 5   processor_brand            960 non-null    object \n 6   num_cores                  974 non-null    float64\n 7   processor_speed            938 non-null    float64\n 8   battery_capacity           969 non-null    float64\n 9   fast_charging_available    980 non-null    int64  \n 10  fast_charging              769 non-null    float64\n 11  ram_capacity               980 non-null    int64  \n 12  internal_memory            980 non-null    int64  \n 13  screen_size                980 non-null    float64\n 14  refresh_rate               980 non-null    int64  \n 15  num_rear_cameras           980 non-null    int64  \n 16  os                         966 non-null    object \n 17  primary_camera_rear        980 non-null    float64\n 18  primary_camera_front       975 non-null    float64\n 19  extended_memory_available  980 non-null    int64  \n 20  resolution_height          980 non-null    int64  \n 21  resolution_width           980 non-null    int64  \ndtypes: float64(8), int64(9), object(5)\nmemory usage: 168.6+ KB\n\n\n\n\nCode\n# let's find the corrupted values easily:\n(df_bad_values\n .price\n .str.extract(r'([^a-zA-Z])') # returns NON-matching alphabetical characters\n .value_counts()\n)\n\n\n*    10\n-    10\n.    10\ndtype: int64\n\n\n\n\nTip #7: Sort values based on the key parameter\nThe pd.sort_values function surprised me with its versatility. Previously, I had only used its by, axis, and ascending parameters for sorting. However, Matt’s book introduced me to its key parameter, which allows us to apply any function to sort the values. The only constraint is that the function must return a Series.\nProblem statement: For whatever reason, we would like to sort the phone model names in ascending order based on their second letter.\n\n\nCode\n(df\n .iloc[:, 1:3]\n .sort_values(by='model',\n              key = lambda x: x.str[1],\n              ascending = True\n             )\n .head(10)\n)\n\n\n\n\n\n\n\n\n\nmodel\nprice\n\n\n\n\n55\nCAT S22 Flip\n14999\n\n\n697\nTCL Ion X\n8990\n\n\n195\nLG V60 ThinQ\n79990\n\n\n196\nLG Velvet 5G\n54999\n\n\n197\nLG Wing 5G\n54999\n\n\n108\niKall Z19 Pro\n8099\n\n\n107\niKall Z19\n7999\n\n\n106\niKall Z18\n6799\n\n\n54\nBLU F91 5G\n14990\n\n\n413\nPOCO C31 (4GB RAM + 64GB)\n7499\n\n\n\n\n\n\n\n\n\nTip #8: Reference an existing variable inside pd.query with @\nThis resembles Tip #4, as it’s a technique I frequently use. Since reading Matt’s book, I have started using pd.query extensively to filter rows based on values, instead of relying on .loc or .iloc. In case you choose to adopt pd.query as well, it’s essential to be aware of its capability to use “@” to reference variables in the environment. This feature enhances its flexibility and makes it even more convenient to apply in various data filtering scenarios.\nProblem statement: Our objective is to identify phones that meet three specific criteria: being priced below the average market price, having more processor cores than the average, and possessing a battery capacity greater than the average.\n\n\nCode\naverage_price=df.price.mean()\naverage_cores=df.num_cores.mean()\naverage_battery=df.battery_capacity.mean()\n\n(df\n .query(\"(price &lt;= @average_price) and (num_cores &gt;= @average_cores) and (battery_capacity &gt;= @average_battery)\")\n .iloc[:,:3]\n .sort_values(by='price')\n .head()\n)\n\n\n\n\n\n\n\n\n\nbrand_name\nmodel\nprice\n\n\n\n\n498\nrealme\nRealme C30\n5299\n\n\n179\nitel\nitel Vision 3 (2GB RAM + 32GB)\n5785\n\n\n202\nmicromax\nMicromax IN 2C\n5999\n\n\n729\ntecno\nTecno Spark Go 2022\n6249\n\n\n499\nrealme\nRealme C30 (3GB RAM + 32GB)\n6299\n\n\n\n\n\n\n\n\n\nTip #9: Gain more insights by using style.background_gradient\nBeing a visual creature, I often struggle to comprehend data quickly just by examining the raw table alone. Fortunately, with the help of style.background_gradient, similar to the heatmap function in the seaborn library, we can represent cells, in terms of color gradient, based on their values. This enables us to identify trends and patterns in our data swiftly, making data analysis more intuitive and insightful.\nProblem statement: Our goal is to identify the overall trends related to key descriptors found among the Top 10 smartphone brands, aiming to determine which brand offers the most value for our money.\n\n\nCode\n(df\n .query(\"brand_name.isin(@top10_brand_names_ordered)\") # filter rows based on top10 brands\n .groupby(['brand_name',])\n [['avg_rating', 'processor_speed', 'ram_capacity', \n   'screen_size', 'battery_capacity', 'price']]\n .mean()\n .sort_values(by='avg_rating')\n .transpose()\n .rename(columns=str.capitalize) # capitalize brand names\n .style\n .set_caption(\"Key descriptors of the Top 10 smartphone brands\")\n .format(precision=1)\n .background_gradient(cmap = 'vlag', axis = 1)\n .set_table_styles([\n    {'selector': 'td', 'props': 'text-align: center;'},\n     {'selector': 'caption','props': 'font-size:1.5em; font-weight:bold;'}\n     ,]\n )\n)\n\n\n\n\n\nKey descriptors of the Top 10 smartphone brands\n\n\nbrand_name\nTecno\nRealme\nApple\nVivo\nPoco\nSamsung\nOppo\nXiaomi\nMotorola\nOneplus\n\n\n\n\navg_rating\n7.4\n7.6\n7.7\n7.7\n7.9\n7.9\n7.9\n7.9\n8.0\n8.2\n\n\nprocessor_speed\n2.1\n2.3\n3.1\n2.4\n2.5\n2.4\n2.5\n2.4\n2.5\n2.7\n\n\nram_capacity\n5.4\n5.7\n5.3\n6.7\n6.1\n6.5\n7.5\n6.4\n6.1\n8.2\n\n\nscreen_size\n6.7\n6.5\n6.1\n6.5\n6.6\n6.6\n6.6\n6.6\n6.6\n6.6\n\n\nbattery_capacity\n5333.9\n4903.1\n3527.2\n4703.7\n5009.4\n4917.4\n4667.2\n4957.6\n4863.1\n4759.5\n\n\nprice\n14545.4\n17461.4\n95966.5\n26782.4\n18479.2\n36843.0\n29650.0\n27961.1\n24099.9\n35858.6\n\n\n\n\n\n\n\nTip #10: Use pd.pipe to include any functions in our chain\nOne of the most valuable lessons I learned from Effective Pandas is the importance of arranging my code in a chain. Although it may feel somewhat restrictive at first, once you overcome the initial hurdles, you’ll realize that your code becomes more readable and easier to understand. The need to invent unique names for temporary variables is completely eliminated, making coding a much happier experience.\nIn the chaining world, you often find yourself wanting to use various functions that are not explicitly designed for chaining. However, there’s good news! You can still achieve this. The pd.pipe function comes to the rescue, allowing you to use any function as long as it returns a Series or DataFrame. It’s a flexible solution that empowers you to seamlessly integrate different functions into your chaining workflow, making your data manipulation more efficient and enjoyable.\nProblem statement: We aim to visualize the impact of RAM capacity on user satisfaction. To achieve this, we will utilize the sns.lmplot function, which plots the data and corresponding regression models for the Top 5 phone brands.\n\n\nCode\ntop5_brand_names_ordered = df.brand_name.value_counts().head().index\n\nwith sns.axes_style(\"darkgrid\"):\n    g = (df\n         .query(\"brand_name.isin(@top5_brand_names_ordered)\") # filter rows based on top10 brands\n         [['brand_name', 'avg_rating', 'ram_capacity']]\n         .assign(brand_name=lambda df: df.brand_name.str.capitalize())\n         .rename(columns={'brand_name':'Brand name'})\n         .pipe(lambda df: sns.lmplot(data=df,\n                                     x='ram_capacity',\n                                     y='avg_rating',\n                                     hue='Brand name',\n                                     # height=4,\n                                     # aspect=1.2\n                                    )\n              )\n        )\n\n    g.set(title='Customer satisfaction correlates with RAM capacity', \n          xlabel='RAM capacity',\n          ylabel='User rating'\n         )\nplt.tight_layout()\n\n\n\n\n\n\n\nTip #10 + 1: Use the “margin” parameter of pd.crosstab to easily calculate row/column subtotals\nDespite primarily using the pandas groupby function for data aggregation, the pd.crosstab function has an enticing feature: the margin parameter. This option enables us to effortlessly calculate subtotals across rows and columns. Moreover, by normalizing our data, we can gain even more intuition about the questions we want to answer.\nProblem statement: Our objective is to evaluate how RAM capacity impacts user satisfaction across the Top 5 brands. Additionally, we will normalize our data to compare values comprehensively across the entire dataset.\n\n\nCode\n(df\n .query(\"brand_name.isin(@top5_brand_names_ordered)\") # filter rows based on top10 brands\n .assign(brand_name=lambda df: df.brand_name.str.capitalize())\n .pipe(lambda df: pd.crosstab(index=df['ram_capacity'],\n                              columns=df['brand_name'],\n                              values=df['avg_rating'],\n                              aggfunc='mean',\n                              margins=True,\n                              normalize='all'\n                             )\n      )\n .mul(100)\n .round(1)\n)\n\n\n\n\n\n\n\n\nbrand_name\nOppo\nRealme\nSamsung\nVivo\nXiaomi\nAll\n\n\nram_capacity\n\n\n\n\n\n\n\n\n\n\n2\n0.0\n2.7\n2.8\n2.7\n2.7\n11.5\n\n\n3\n2.9\n2.8\n2.9\n2.9\n2.8\n12.1\n\n\n4\n3.1\n3.2\n3.2\n3.2\n3.2\n13.5\n\n\n6\n3.3\n3.5\n3.5\n3.4\n3.5\n14.8\n\n\n8\n3.7\n3.7\n3.8\n3.7\n3.8\n15.7\n\n\n12\n3.8\n3.9\n3.9\n3.8\n3.9\n16.3\n\n\n16\n3.8\n0.0\n0.0\n0.0\n0.0\n16.2\n\n\nAll\n20.2\n19.6\n20.2\n19.8\n20.2\n100.0\n\n\n\n\n\n\n\nI hope this article has convinced you to pick up Matt Harrison’s Effective Pandas! There are plenty more exciting ideas in the book beyond the Top 10 I’ve shared here (I didn’t even get into the fascinating time series part!). I hope you found these insights helpful and inspiring.\nHappy coding 🐼💻🚀"
  },
  {
    "objectID": "posts/spam/spam.html",
    "href": "posts/spam/spam.html",
    "title": "Tidy Tuesday: Spam E-mail",
    "section": "",
    "text": "Photo by Stephen Phillips - Hostreviews.co.uk on UnSplash\nWelcome to this week’s Tidy Tuesday! Today, we’re delving into the intriguing yet bothersome realm of email spams. You know, those unsolicited messages that flood our inboxes? They go by many names like junk email or spam mail, and they’re sent in bulk. The term “spam” got its name from a Monty Python sketch where the word “Spam” was everywhere, just like these emails. Starting from the early 1990s, these spam messages have been on a steady rise, making up around 90% of all email traffic by 2014.\nWe’ve got our data from the Tidy Tuesday treasure trove over at GitHub! This dataset, from Vincent Arel-Bundock’s Rdatasets package, was initially gathered at Hewlett-Packard Labs. They later kindly shared it with the UCI Machine Learning Repository.\nThis treasure trove of information consists of 4601 emaily sorted into spam and non-spam categorie"
  },
  {
    "objectID": "posts/spam/spam.html#visualize-all-the-principal-components",
    "href": "posts/spam/spam.html#visualize-all-the-principal-components",
    "title": "Tidy Tuesday: Spam E-mail",
    "section": "Visualize all the principal components",
    "text": "Visualize all the principal components\n\n\nCode\nX = df.select(pl.all().exclude(\"yesno\")).to_pandas()\ny = df.select(pl.col(\"yesno\")).to_pandas()\n\n\n\n\nCode\n# Before PCA we need to scale and transform our dataset\n\nscaler = preprocessing.PowerTransformer().set_output(transform=\"pandas\")\n\nX = scaler.fit_transform(X)\n\n\n\n\nCode\npca = decomposition.PCA()\ncomponents = pca.fit_transform(X)\nlabels = {\n    str(i): f\"PC {i+1} ({var:.1f}%)\"\n    for i, var in enumerate(pca.explained_variance_ratio_ * 100)\n}\n\nfig = px.scatter_matrix(\n    components,\n    opacity=0.2,\n    labels=labels,\n    dimensions=range(6),\n    color=y.squeeze().map({0: \"Not a Spam\", 1: \"Spam\"}),\n    width=1000,\n    height=1000,\n)\nfig.update_traces(diagonal_visible=False)\nfig.show()\n\n\n\n\n                                                \nFigure 4: Visualization of All Principal Components\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs evident, the first and second principal components manage to account for 62% of the variance within the data. A distinct separation of classes is noticeable, particularly with the spam group exhibiting a broader distribution, indicating greater diversity."
  },
  {
    "objectID": "posts/spam/spam.html#visualize-loadings",
    "href": "posts/spam/spam.html#visualize-loadings",
    "title": "Tidy Tuesday: Spam E-mail",
    "section": "Visualize Loadings",
    "text": "Visualize Loadings\nFor a deeper comprehension of how each characteristic influences our principal components, we can delve into examining the loadings.\n\n\nCode\npca = decomposition.PCA(n_components=2)\ncomponents = pca.fit_transform(X)\nloadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n\n\nfig = px.scatter(\n    components,\n    x=0,\n    y=1,\n    color=y.squeeze().map({0: \"Not a Spam\", 1: \"Spam\"}),\n    labels={\n        \"0\": f\"PC1 ({pca.explained_variance_ratio_[0]:.0%})\",\n        \"1\": f\"PC2 ({pca.explained_variance_ratio_[1]:.0%})\",\n    },\n    template=\"plotly_dark\",\n    color_discrete_sequence=[\n        \"red\",\n        \"green\",\n    ],\n    opacity=0.4,\n    width=700,\n    height=700,\n)\nfor i, feature in enumerate(X.columns):\n    fig.add_annotation(\n        ax=0,\n        ay=0,\n        axref=\"x\",\n        ayref=\"y\",\n        x=loadings[i, 0],\n        y=loadings[i, 1],\n        showarrow=True,\n        arrowsize=1,\n        arrowhead=2,\n        xanchor=\"right\",\n        yanchor=\"top\",\n    )\n    fig.add_annotation(\n        x=loadings[i, 0],\n        y=loadings[i, 1],\n        ax=0,\n        ay=0,\n        font=dict(\n            size=20,\n            # color='yellow'\n        ),\n        xanchor=\"left\",\n        yanchor=\"bottom\",\n        text=feature,\n        yshift=5,\n    )\nfig\n\n\n\n\n                                                \nFigure 5: Loadings Plot\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe loadings plot reveals a strong correlation among the variables “crl.tot,” “money,” and “n000”. If we were to construct a machine learning model for spam email recognition, we could opt for one of these variables to streamline our dataset. Furthermore, the disassociation of “bang” from this trio is evident, its vector positioned at a 90° angle.\n\n\nAlright folks, our journey into the intriguing realm of spam emails has left us with some valuable insights. It’s clear that words like “bang” and “n000” are key indicators to watch out for. Luckily, modern machine learning models are here to do the hard work on our behalf.\nStay vigilant out there and take care! See you in the next week’s adventure! 👋📧🛡️"
  },
  {
    "objectID": "posts/polars speed/polars_speed.html",
    "href": "posts/polars speed/polars_speed.html",
    "title": "The Ultimate Speed Test: Pandas vs Polars",
    "section": "",
    "text": "Once I shared my first article about “Getting Started with Polars” I started thinking about something exciting: comparing the speed of Polars and Pandas. This curiosity was sparked by all the buzz around the brand-new Pandas 2.0 release, promising lightning-fast performance. Pandas 2.0 was announced to come packed with cool features, including the addition of Apache Arrow (pyarrow) as its backing memory format. The big perk of Apache Arrow is that it makes operations speedier and more memory-friendly. Naturally, this got me wondering: how does Pandas 2.0 measure up against Polars? Let’s dive in and find out!"
  },
  {
    "objectID": "posts/polars speed/polars_speed.html#csv",
    "href": "posts/polars speed/polars_speed.html#csv",
    "title": "The Ultimate Speed Test: Pandas vs Polars",
    "section": "CSV",
    "text": "CSV\nReading CSV files from disk is a task that data scientists often find themselves doing. Now, let’s see how these two libraries compare for this particular job. To maximize the blazing-fast data handling capabilities of PyArrow, we’ll equip Pandas with the engine=\"pyarrow\" and dtype_backend=\"pyarrow\" arguments. Let’s see how these choices shape the performance!\n\n\nCode\ntest_dict = {\n    \"Pandas\": lambda: pd.read_csv(\n        \"sample.csv\", engine=\"pyarrow\", dtype_backend=\"pyarrow\"\n    ),\n    \"Polars\": lambda: pl.read_csv(\"sample.csv\"),\n}\nread_csv = run_test(test_dict, \"Read csv\")\n\n\n\n\n                                                \nFigure 1: Reading in data From a CSV File\n\n\n\nFor the sake of comparison, we’ll also demonstrate the timeit function invoked using Jupyter cell magic. You’ll notice that the numbers generated this way are quite closely aligned with ours.\n\n\nCode\n%%timeit\npd.read_csv(\"sample.csv\", engine=\"pyarrow\", dtype_backend=\"pyarrow\")\n\n\n6.33 ms ± 234 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\n\nCode\n%%timeit\npl.read_csv(\"sample.csv\")\n\n\n3.01 ms ± 456 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)"
  },
  {
    "objectID": "posts/polars speed/polars_speed.html#parquet",
    "href": "posts/polars speed/polars_speed.html#parquet",
    "title": "The Ultimate Speed Test: Pandas vs Polars",
    "section": "parquet",
    "text": "parquet\nNow, let’s read the data in Parquet format.\n\n\nCode\ntest_dict = {\n    \"Pandas\": lambda: pd.read_parquet(\n        \"sample.parquet\", engine=\"pyarrow\", dtype_backend=\"pyarrow\"\n    ),\n    \"Polars\": lambda: pl.read_parquet(\"sample.parquet\"),\n}\nread_parquet = run_test(test_dict, \"Read parquet\")\n\n\n\n\n                                                \nFigure 2: Reading in data From a parquet File\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nPolars unquestionably wins this round, it can boast a speed advantage of 2 to 4 times over Pandas."
  },
  {
    "objectID": "posts/polars speed/polars_speed.html#files-awaiting-in-memory-reading",
    "href": "posts/polars speed/polars_speed.html#files-awaiting-in-memory-reading",
    "title": "The Ultimate Speed Test: Pandas vs Polars",
    "section": "Files awaiting in-memory reading",
    "text": "Files awaiting in-memory reading\nA clever approach to conserve memory and enhance speed involves reading only the columns essential for operations. Consider a scenario where we’re interested in displaying just the names from this dataset. The big question now: how do these libraries measure up in terms of speed? Let’s find out!\n\n\nCode\ntest_dict = {\n    \"Pandas\": lambda: pd.read_csv(\n        \"sample.csv\", engine=\"pyarrow\", dtype_backend=\"pyarrow\", usecols=[\"name\"]\n    ),\n    \"Polars\": lambda: pl.scan_csv(\"sample.csv\").select(pl.col(\"name\")).collect(),\n}\nselect_col_not_in_memory = run_test(test_dict, \"Select column (not in memory)\")\n\n\n\n\n                                                \nFigure 3: Selecting Columns from a File Not Yet in Memory"
  },
  {
    "objectID": "posts/polars speed/polars_speed.html#file-is-in-memory",
    "href": "posts/polars speed/polars_speed.html#file-is-in-memory",
    "title": "The Ultimate Speed Test: Pandas vs Polars",
    "section": "File is in memory",
    "text": "File is in memory\nAs anticipated, Polars continues to showcase its swiftness. It’s worth highlighting the usage of the lazy and collect methods in Polars. These nifty tools grant us access to the library’s clever query optimization techniques, which play a pivotal role in significantly enhancing performance. OK, one step further: suppose our files are already loaded into memory. Would there still be a distinction in performance under this circumstance?\n\n\nCode\ndf_pandas = pd.read_csv(\"sample.csv\", engine=\"pyarrow\", dtype_backend=\"pyarrow\")\ndf_polars = pl.read_csv(\"sample.csv\")\n\n\n\n\nCode\ntest_dict = {\n    \"Pandas\": lambda: df_pandas.loc[:, \"name\"],\n    \"Polars\": lambda: df_polars.lazy().select(pl.col(\"name\")).collect(),\n}\nselect_col_in_memory = run_test(test_dict, \"Select column\")\n\n\n\n\n                                                \nFigure 4: Selecting Columns from a File Already in Memory\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhile Polars showed a significant speed advantage for tasks involving pre-read files, both libraries perform similarly when the files are already in memory."
  },
  {
    "objectID": "posts/polars speed/polars_speed.html#based-on-one-condition",
    "href": "posts/polars speed/polars_speed.html#based-on-one-condition",
    "title": "The Ultimate Speed Test: Pandas vs Polars",
    "section": "Based on one condition",
    "text": "Based on one condition\nFor our simple scenario, we’ll be narrowing down our focus to filter data based on individuals with the name “David”.\n\n\nCode\ntest_dict = {\n    \"Pandas\": lambda: (df_pandas.query(\"name=='David'\")),\n    \"Polars\": lambda: (df_polars.lazy().filter((pl.col(\"name\") == \"David\")).collect()),\n}\nfilter_row_one_condition = run_test(test_dict, \"Filter (simple)\")\n\n\n\n\n                                                \nFigure 5: Filtering Rows Based on One Condition"
  },
  {
    "objectID": "posts/polars speed/polars_speed.html#based-on-multiple-conditions",
    "href": "posts/polars speed/polars_speed.html#based-on-multiple-conditions",
    "title": "The Ultimate Speed Test: Pandas vs Polars",
    "section": "Based on multiple conditions",
    "text": "Based on multiple conditions\nNow, for a more intricate challenge, we’re going to dive into querying the data to extract individuals who meet specific criteria: those named David, born after 1980, residing in a city other than London, married, and with three children.\n\n\nCode\ntest_dict = {\n    \"Pandas\": lambda: (\n        df_pandas.query(\n            \"name=='David' and born&gt;1980 and city != 'London' or is_married == True and children &gt;= 3\"\n        )\n    ),\n    \"Polars\": lambda: (\n        df_polars.lazy()\n        .filter(\n            (pl.col(\"name\") == \"David\")\n            & (pl.col(\"born\") &gt; 1980)\n            & (pl.col(\"city\") != \"London\")\n            | (pl.col(\"is_married\") == True) & (pl.col(\"children\") &gt;= 3)\n        )\n        .collect()\n    ),\n}\nfilter_row_multiple_condition = run_test(test_dict, \"Filter (complex)\")\n\n\n\n\n                                                \nFigure 6: Filtering Rows Based on Multiple Condition\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nBoth libraries tackled this challenge quite well, yet Pandas struggled to keep pace with Polars. It’s intriguing to observe that while Pandas required nearly twice the time for the more intricate task, Polars managed to complete it in almost the same amount of time. Parallelization in action."
  },
  {
    "objectID": "posts/polars speed/polars_speed.html#single-operation",
    "href": "posts/polars speed/polars_speed.html#single-operation",
    "title": "The Ultimate Speed Test: Pandas vs Polars",
    "section": "Single operation",
    "text": "Single operation\nAs a single operation, we’ll simply calculate the century in which these individuals were born.\n\n\nCode\ntest_dict = {\n    \"Pandas\": lambda: (df_pandas.assign(born=lambda df: df.born.div(100).round())),\n    \"Polars\": lambda: (\n        df_polars.lazy().with_columns((pl.col(\"born\") / 100).round()).collect()\n    ),\n}\noperate_one_column = run_test(test_dict, \"Operate (one column)\")\n\n\n\n\n                                                \nFigure 7: Performing a Singme Operation on a Column"
  },
  {
    "objectID": "posts/polars speed/polars_speed.html#multiple-operations",
    "href": "posts/polars speed/polars_speed.html#multiple-operations",
    "title": "The Ultimate Speed Test: Pandas vs Polars",
    "section": "Multiple operations",
    "text": "Multiple operations\nLet’s also explore what happens when performing multiple operations on the columns. We’ll mix things up with some string operations, mapping, and math calculations to see how these libraries handle it!\n\n\nCode\ntest_dict = {\n    \"Pandas\": lambda: (\n        df_pandas.assign(\n            born=lambda df: df.born.div(100).round(),\n            name=lambda df: df.name.str.lower(),\n            city=lambda df: df.city.str.upper(),\n            zip_code=lambda df: df.zip_code.mul(2),\n            income=lambda df: df.income.div(10).astype(\"str\"),\n            is_married=lambda df: df.is_married.map({False: 0, True: 1}),\n            children=lambda df: df.children.astype(\"bool\"),\n            car=lambda df: df.car.str[0],\n        )\n    ),\n    \"Polars\": lambda: (\n        df_polars.lazy()\n        .with_columns(\n            [\n                (pl.col(\"born\") / 100).round(),\n                pl.col(\"name\").str.to_lowercase(),\n                pl.col(\"city\").str.to_uppercase(),\n                pl.col(\"zip_code\") * 2,\n                (pl.col(\"income\") / 10).cast(pl.Utf8),\n                pl.col(\"is_married\").map_dict({False: 0, True: 1}),\n                pl.col(\"children\").cast(pl.Boolean),\n                pl.col(\"car\").str.slice(0, length=1),\n            ]\n        )\n        .collect()\n    ),\n}\noperate_multiple_column = run_test(test_dict, \"Operate (more columns)\")\n\n\n\n\n                                                \nFigure 8: Perfrming a Multiple Operation on Columns\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nOnce again, Polars takes the lead. While both libraries required more time for the task involving multiple operations, Polars demonstrated superior scalability in this scenario."
  },
  {
    "objectID": "posts/polars speed/polars_speed.html#simple",
    "href": "posts/polars speed/polars_speed.html#simple",
    "title": "The Ultimate Speed Test: Pandas vs Polars",
    "section": "Simple",
    "text": "Simple\nTime to shift our attention to aggregation. First up, a simple task: let’s calculate the mean income based on names. Then, for a bit more complexity, we’ll dive into computing statistics involving the income, children, and car columns. Things are about to get interesting!\n\n\nCode\ntest_dict = {\n    \"Pandas\": lambda: (df_pandas.groupby(\"name\").income.mean()),\n    \"Polars\": lambda: (df_polars.lazy().groupby(\"name\").mean().collect()),\n}\naggregate_simple = run_test(test_dict, \"Aggregate (simple)\")\n\n\n\n\n                                                \nFigure 10: Performing a simple aggregation"
  },
  {
    "objectID": "posts/polars speed/polars_speed.html#more-complex",
    "href": "posts/polars speed/polars_speed.html#more-complex",
    "title": "The Ultimate Speed Test: Pandas vs Polars",
    "section": "More complex",
    "text": "More complex\n\n\nCode\ntest_dict = {\n    \"Pandas\": lambda: (\n        df_pandas.groupby([\"name\", \"car\", \"is_married\"]).agg(\n            born_min=(\"born\", min),\n            born_max=(\"born\", max),\n            income_mean=(\"income\", np.mean),\n            income_median=(\"income\", np.median),\n            children_mean=(\"children\", np.mean),\n            car_count=(\"car\", \"count\"),\n        )\n    ),\n    \"Polars\": lambda: (\n        df_polars.lazy()\n        .groupby([\"name\", \"car\", \"is_married\"])\n        .agg(\n            [\n                pl.col(\"born\").min().alias(\"born_min\"),\n                pl.col(\"born\").max().alias(\"born_max\"),\n                pl.col(\"income\").mean().alias(\"income_mean\"),\n                pl.col(\"income\").median().alias(\"income_median\"),\n                pl.col(\"children\").mean().alias(\"children_mean\"),\n                pl.col(\"car\").count().alias(\"car_count\"),\n            ]\n        )\n        .collect()\n    ),\n}\naggregate_complex = run_test(test_dict, \"Aggregate (complex)\")\n\n\n\n\n                                                \nFigure 11: Performing a complex aggregation\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhile Pandas showcased noteworthy speed for the simple aggregation, the more intricate task exposed significant disparities between the two libraries. Polars took a commanding lead in this scenario, presenting a considerably faster performance compared to Pandas."
  },
  {
    "objectID": "posts/polars speed/polars_speed.html#using-pl.read_csv",
    "href": "posts/polars speed/polars_speed.html#using-pl.read_csv",
    "title": "The Ultimate Speed Test: Pandas vs Polars",
    "section": "Using pl.read_csv",
    "text": "Using pl.read_csv\n\n\nCode\ntest_dict = {\n    \"Pandas\": lambda: (\n        df_pandas.loc[:, lambda df: ~df.columns.isin([\"is_married\"])]\n        .query(\"name=='David' and born&gt;1980 and city != 'London' and children &gt;= 3\")\n        .assign(\n            born=lambda df: df.born.div(100).round(),\n            name=lambda df: df.name.str.lower(),\n            city=lambda df: df.city.str.upper(),\n            zip_code=lambda df: df.zip_code.mul(2),\n            income=lambda df: df.income.div(10),\n            children=lambda df: df.children.astype(\"bool\"),\n            car=lambda df: df.car.str[0],\n        )\n        .groupby(\n            [\n                \"name\",\n                \"car\",\n            ]\n        )\n        .agg(\n            born_min=(\"born\", min),\n            born_max=(\"born\", max),\n            income_mean=(\"income\", np.mean),\n            income_median=(\"income\", np.median),\n            children_mean=(\"children\", np.mean),\n            car_count=(\"car\", \"count\"),\n        )\n    ),\n    \"Polars\": lambda: (\n        df_polars.lazy()\n        .select(cs.all() - cs.ends_with(\"married\"))\n        .filter(\n            (pl.col(\"name\") == \"David\")\n            & (pl.col(\"born\") &gt; 1980)\n            & (pl.col(\"city\") != \"London\")\n            & (pl.col(\"children\") &gt;= 3)\n        )\n        .with_columns(\n            [\n                (pl.col(\"born\") / 100).round(),\n                pl.col(\"name\").str.to_lowercase(),\n                pl.col(\"city\").str.to_uppercase(),\n                pl.col(\"zip_code\") * 2,\n                (pl.col(\"income\") / 10),\n                pl.col(\"children\").cast(pl.Boolean),\n                pl.col(\"car\").str.slice(0, length=1),\n            ]\n        )\n        .groupby(\n            [\n                \"name\",\n                \"car\",\n            ]\n        )\n        .agg(\n            [\n                pl.col(\"born\").min().alias(\"born_min\"),\n                pl.col(\"born\").max().alias(\"born_max\"),\n                pl.col(\"income\").mean().alias(\"income_mean\"),\n                pl.col(\"income\").median().alias(\"income_median\"),\n                pl.col(\"children\").mean().alias(\"children_mean\"),\n                pl.col(\"car\").count().alias(\"car_count\"),\n            ]\n        )\n        .collect()\n    ),\n}\nwhole_workflow_read_csv = run_test(test_dict, \"Whole workflow\")\n\n\n\n\n                                                \nFigure 12: Performing a representative workflow"
  },
  {
    "objectID": "posts/polars speed/polars_speed.html#using-pl.scan_csv",
    "href": "posts/polars speed/polars_speed.html#using-pl.scan_csv",
    "title": "The Ultimate Speed Test: Pandas vs Polars",
    "section": "Using pl.scan_csv",
    "text": "Using pl.scan_csv\n\n\nCode\ntest_dict = {\n    \"Pandas\": lambda: (\n        df_pandas.loc[:, lambda df: ~df.columns.isin([\"is_married\"])]\n        .query(\"name=='David' and born&gt;1980 and city != 'London' and children &gt;= 3\")\n        .assign(\n            born=lambda df: df.born.div(100).round(),\n            name=lambda df: df.name.str.lower(),\n            city=lambda df: df.city.str.upper(),\n            zip_code=lambda df: df.zip_code.mul(2),\n            income=lambda df: df.income.div(10),\n            children=lambda df: df.children.astype(\"bool\"),\n            car=lambda df: df.car.str[0],\n        )\n        .groupby(\n            [\n                \"name\",\n                \"car\",\n            ]\n        )\n        .agg(\n            born_min=(\"born\", min),\n            born_max=(\"born\", max),\n            income_mean=(\"income\", np.mean),\n            income_median=(\"income\", np.median),\n            children_mean=(\"children\", np.mean),\n            car_count=(\"car\", \"count\"),\n        )\n    ),\n    \"Polars\": lambda: (\n        pl.scan_csv(\"sample.csv\")\n        .select(cs.all() - cs.ends_with(\"married\"))\n        .filter(\n            (pl.col(\"name\") == \"David\")\n            & (pl.col(\"born\") &gt; 1980)\n            & (pl.col(\"city\") != \"London\")\n            & (pl.col(\"children\") &gt;= 3)\n        )\n        .with_columns(\n            [\n                (pl.col(\"born\") / 100).round(),\n                pl.col(\"name\").str.to_lowercase(),\n                pl.col(\"city\").str.to_uppercase(),\n                pl.col(\"zip_code\") * 2,\n                (pl.col(\"income\") / 10),\n                pl.col(\"children\").cast(pl.Boolean),\n                pl.col(\"car\").str.slice(0, length=1),\n            ]\n        )\n        .groupby(\n            [\n                \"name\",\n                \"car\",\n            ]\n        )\n        .agg(\n            [\n                pl.col(\"born\").min().alias(\"born_min\"),\n                pl.col(\"born\").max().alias(\"born_max\"),\n                pl.col(\"income\").mean().alias(\"income_mean\"),\n                pl.col(\"income\").median().alias(\"income_median\"),\n                pl.col(\"children\").mean().alias(\"children_mean\"),\n                pl.col(\"car\").count().alias(\"car_count\"),\n            ]\n        )\n        .collect()\n    ),\n}\nwhole_workflow_scan_csv = run_test(test_dict, \"Whole workflow (scan_csv)\")\n\n\n\n\n                                                \nFigure 13: Performing a representative workflow (using pl_scan_csv for Polars)\n\n\n\nAs evident, the utilization of scan_csv increased the required time by about 3-4 times. However, even with this increase, Polars still manages to maintain a substantial advantage of around 5 times faster than the entire workflow executed using Pandas.\n\n\n\n\n\n\nNote\n\n\n\nWhen we consider the entirety of the data processing pipeline, irrespective of the file reading approach, Polars emerges as the victor. It consistently exhibits a considerable speed advantage compared to Pandas."
  },
  {
    "objectID": "posts/pandas_styler/pandas_style.html",
    "href": "posts/pandas_styler/pandas_style.html",
    "title": "Mastering Pandas DataFrame Styling for a Stunning Presentation",
    "section": "",
    "text": "Image by Mika Baumeister\nAs you wrap up your data analysis journey, you face a fun decision: how to share your discoveries effectively. Tables and graphs both have their moments to shine. Tables work great when you want your audience to spot exact values and compare them. But, here’s the twist: tables can look overwhelming at first, especially if they’re crammed with data.\nBut don’t worry! Styling and formatting are here to help. Pandas comes to the rescue. It lets you turn your data into stylish tables effortlessly.\nIn this article, we’ll explore some tricks to make your Pandas DataFrames look awesome and tell a clear data story. For the demonstrations, we’ll dive into Seaborn’s built-in “tips” dataset. This dataset is a nifty data frame with 244 rows and 7 variables, offering a peek into the world of tipping. This comprehensive collection includes variables like the tip amount in dollars, the bill total in dollars, the gender of the bill payer, the presence of smokers in the party, the day of the week, the time of day, and the party size. Ready to roll? Let’s jump right in!"
  },
  {
    "objectID": "posts/pandas_styler/pandas_style.html#highlighting-minimum-and-maximum-values",
    "href": "posts/pandas_styler/pandas_style.html#highlighting-minimum-and-maximum-values",
    "title": "Mastering Pandas DataFrame Styling for a Stunning Presentation",
    "section": "Highlighting minimum and maximum values",
    "text": "Highlighting minimum and maximum values\nCheck out these handy helpers: highlight_min and highlight_max. They’re your go-to for spotting the lowest and highest values in each column. It’s a speedy way to emphasize the most important data points in each category.\nAlso, please note the format(precision=1, thousands=\",\", decimal=\".\") snippet, this is not exactly a built-in style feature but has everything to do with keeping those float numbers tidy. Pandas tends to display more decimal places than we often require, which can be a bit distracting. To tone it down a notch, we can lean on the format() and format_index() methods to fine-tune the precision. Trust me, it’s super useful!\n\n\nCode\n(\n    grouped.style.format(precision=1, thousands=\",\", decimal=\".\")\n    .highlight_max(\n        axis=0, props=\"color:white; font-weight:bold; background-color:green;\"\n    )\n    .highlight_min(axis=0, props=\"color:white; font-weight:bold; background-color:red;\")\n)\n\n\n\n\n\n\nTable 3: Highlighting Maximum and Minimum Values with Pandas Styler\n\n\n \n \n \ntotal_bill\ntip\nsize\n\n\nday\nsmoker\ntime\n \n \n \n\n\n\n\nFri\nNo\nDinner\n22.5\n3.2\n2.0\n\n\nLunch\n16.0\n3.0\n3.0\n\n\nYes\nDinner\n16.3\n3.0\n2.0\n\n\nLunch\n12.8\n2.1\n2.0\n\n\nSat\nNo\nDinner\n17.8\n2.8\n2.0\n\n\nYes\nDinner\n20.4\n2.7\n2.0\n\n\nSun\nNo\nDinner\n18.4\n3.0\n3.0\n\n\nYes\nDinner\n23.1\n3.5\n2.0\n\n\nThur\nNo\nDinner\n18.8\n3.0\n2.0\n\n\nLunch\n15.2\n2.1\n2.0\n\n\nYes\nLunch\n16.5\n2.6\n2.0"
  },
  {
    "objectID": "posts/pandas_styler/pandas_style.html#emphasizing-values-within-a-range",
    "href": "posts/pandas_styler/pandas_style.html#emphasizing-values-within-a-range",
    "title": "Mastering Pandas DataFrame Styling for a Stunning Presentation",
    "section": "Emphasizing Values Within a Range",
    "text": "Emphasizing Values Within a Range\nImagine you’re want to find the days when tips fell between 3 and 5 dollars. In this scenario, the highlight_between method comes to the rescue. Don’t forget to use the subset argument; it’s your trusty sidekick when you only want to work with selected columns.\n\n\nCode\n(\n    grouped.reset_index()\n    .style.format(precision=1, thousands=\",\", decimal=\".\")\n    .highlight_between(\n        left=3,\n        right=5,\n        subset=[\"tip\"],\n        axis=1,\n        props=\"color:white; font-weight:bold; background-color:purple;\",\n    )\n)\n\n\n\n\n\n\nTable 4: Highlighting Data within a Specified Range Using Pandas Styler\n\n\n \nday\nsmoker\ntime\ntotal_bill\ntip\nsize\n\n\n\n\n0\nFri\nNo\nDinner\n22.5\n3.2\n2.0\n\n\n1\nFri\nNo\nLunch\n16.0\n3.0\n3.0\n\n\n2\nFri\nYes\nDinner\n16.3\n3.0\n2.0\n\n\n3\nFri\nYes\nLunch\n12.8\n2.1\n2.0\n\n\n4\nSat\nNo\nDinner\n17.8\n2.8\n2.0\n\n\n5\nSat\nYes\nDinner\n20.4\n2.7\n2.0\n\n\n6\nSun\nNo\nDinner\n18.4\n3.0\n3.0\n\n\n7\nSun\nYes\nDinner\n23.1\n3.5\n2.0\n\n\n8\nThur\nNo\nDinner\n18.8\n3.0\n2.0\n\n\n9\nThur\nNo\nLunch\n15.2\n2.1\n2.0\n\n\n10\nThur\nYes\nLunch\n16.5\n2.6\n2.0"
  },
  {
    "objectID": "posts/pandas_styler/pandas_style.html#highlight-column-wise-outliers",
    "href": "posts/pandas_styler/pandas_style.html#highlight-column-wise-outliers",
    "title": "Mastering Pandas DataFrame Styling for a Stunning Presentation",
    "section": "Highlight Column-Wise Outliers",
    "text": "Highlight Column-Wise Outliers\nUtilizing parameters such as q_left=0.05, q_right=0.95, axis=0, and defining props='opacity: 10%;', we can highlight values residing outside the 5-95 percentile range.\n\n\nCode\n(\n    grouped.style.format(precision=1, thousands=\",\", decimal=\".\").highlight_quantile(\n        q_left=0.05, q_right=0.95, axis=0, props=\"opacity: 10%;\"\n    )\n)\n\n\n\n\n\n\nTable 5: Highlighting Outliers\n\n\n \n \n \ntotal_bill\ntip\nsize\n\n\nday\nsmoker\ntime\n \n \n \n\n\n\n\nFri\nNo\nDinner\n22.5\n3.2\n2.0\n\n\nLunch\n16.0\n3.0\n3.0\n\n\nYes\nDinner\n16.3\n3.0\n2.0\n\n\nLunch\n12.8\n2.1\n2.0\n\n\nSat\nNo\nDinner\n17.8\n2.8\n2.0\n\n\nYes\nDinner\n20.4\n2.7\n2.0\n\n\nSun\nNo\nDinner\n18.4\n3.0\n3.0\n\n\nYes\nDinner\n23.1\n3.5\n2.0\n\n\nThur\nNo\nDinner\n18.8\n3.0\n2.0\n\n\nLunch\n15.2\n2.1\n2.0\n\n\nYes\nLunch\n16.5\n2.6\n2.0"
  },
  {
    "objectID": "posts/pandas_styler/pandas_style.html#spot-trends-using-color-gradients",
    "href": "posts/pandas_styler/pandas_style.html#spot-trends-using-color-gradients",
    "title": "Mastering Pandas DataFrame Styling for a Stunning Presentation",
    "section": "Spot trends Using Color Gradients",
    "text": "Spot trends Using Color Gradients\nWe can show data trends using both the background_gradient and text_gradient methods. These methods introduce gradient-style background colors and text shading to our visualizations. To optimize their impact, it’s advisable to first arrange your data with the sort_values method before applying the background_gradient.\n\n\nCode\n(\n    grouped.reset_index()\n    .sort_values(by=\"total_bill\")\n    .style.format(precision=1, thousands=\",\", decimal=\".\")\n    .background_gradient(cmap=\"viridis\", axis=0)\n)\n\n\n\n\n\n\nTable 6: Unveiling Data Trends with Pandas Styler’s background_gradient\n\n\n \nday\nsmoker\ntime\ntotal_bill\ntip\nsize\n\n\n\n\n3\nFri\nYes\nLunch\n12.8\n2.1\n2.0\n\n\n9\nThur\nNo\nLunch\n15.2\n2.1\n2.0\n\n\n1\nFri\nNo\nLunch\n16.0\n3.0\n3.0\n\n\n2\nFri\nYes\nDinner\n16.3\n3.0\n2.0\n\n\n10\nThur\nYes\nLunch\n16.5\n2.6\n2.0\n\n\n4\nSat\nNo\nDinner\n17.8\n2.8\n2.0\n\n\n6\nSun\nNo\nDinner\n18.4\n3.0\n3.0\n\n\n8\nThur\nNo\nDinner\n18.8\n3.0\n2.0\n\n\n5\nSat\nYes\nDinner\n20.4\n2.7\n2.0\n\n\n0\nFri\nNo\nDinner\n22.5\n3.2\n2.0\n\n\n7\nSun\nYes\nDinner\n23.1\n3.5\n2.0\n\n\n\n\n\n\n\n\nCode\n(\n    grouped.reset_index()\n    .sort_values(by=\"total_bill\")\n    .style.format(precision=1, thousands=\",\", decimal=\".\")\n    .text_gradient(cmap=\"bwr\", axis=0)\n)\n\n\n\n\n\n\nTable 7: Unveiling Data Trends with Pandas Styler’s text_gradient\n\n\n \nday\nsmoker\ntime\ntotal_bill\ntip\nsize\n\n\n\n\n3\nFri\nYes\nLunch\n12.8\n2.1\n2.0\n\n\n9\nThur\nNo\nLunch\n15.2\n2.1\n2.0\n\n\n1\nFri\nNo\nLunch\n16.0\n3.0\n3.0\n\n\n2\nFri\nYes\nDinner\n16.3\n3.0\n2.0\n\n\n10\nThur\nYes\nLunch\n16.5\n2.6\n2.0\n\n\n4\nSat\nNo\nDinner\n17.8\n2.8\n2.0\n\n\n6\nSun\nNo\nDinner\n18.4\n3.0\n3.0\n\n\n8\nThur\nNo\nDinner\n18.8\n3.0\n2.0\n\n\n5\nSat\nYes\nDinner\n20.4\n2.7\n2.0\n\n\n0\nFri\nNo\nDinner\n22.5\n3.2\n2.0\n\n\n7\nSun\nYes\nDinner\n23.1\n3.5\n2.0"
  },
  {
    "objectID": "posts/pandas_styler/pandas_style.html#display-bar-charts-within-your-table",
    "href": "posts/pandas_styler/pandas_style.html#display-bar-charts-within-your-table",
    "title": "Mastering Pandas DataFrame Styling for a Stunning Presentation",
    "section": "Display Bar Charts within Your Table",
    "text": "Display Bar Charts within Your Table\nLet’s explore a technique for highlighting the significance of values by embedding bar charts right within the cells. The blend of bar heights and color gradients can pack a powerful punch in your data storytelling arsenal. Don’t forget to experiment with the ‘align’ option, a handy tool that helps you position these bars within the cells just right, giving your visuals a polished look. Feel free to play around with the settings and find what clicks best with your data tales.\n\n\nCode\n(\n    grouped.reset_index()\n    .sort_values(by=\"total_bill\")\n    .style.format(precision=1, thousands=\",\", decimal=\".\")\n    .bar(\n        align=\"mean\",\n        cmap=\"bwr\",\n        height=50,\n        width=60,\n        props=\"width: 120px; border-right: 1px solid black;\",\n    )\n)\n\n\n\n\n\n\nTable 8: Bar Charts in Your Table with Alignment Set to ‘Mean’\n\n\n \nday\nsmoker\ntime\ntotal_bill\ntip\nsize\n\n\n\n\n3\nFri\nYes\nLunch\n12.8\n2.1\n2.0\n\n\n9\nThur\nNo\nLunch\n15.2\n2.1\n2.0\n\n\n1\nFri\nNo\nLunch\n16.0\n3.0\n3.0\n\n\n2\nFri\nYes\nDinner\n16.3\n3.0\n2.0\n\n\n10\nThur\nYes\nLunch\n16.5\n2.6\n2.0\n\n\n4\nSat\nNo\nDinner\n17.8\n2.8\n2.0\n\n\n6\nSun\nNo\nDinner\n18.4\n3.0\n3.0\n\n\n8\nThur\nNo\nDinner\n18.8\n3.0\n2.0\n\n\n5\nSat\nYes\nDinner\n20.4\n2.7\n2.0\n\n\n0\nFri\nNo\nDinner\n22.5\n3.2\n2.0\n\n\n7\nSun\nYes\nDinner\n23.1\n3.5\n2.0\n\n\n\n\n\n\n\n\nCode\n(\n    grouped.reset_index()\n    .sort_values(by=\"total_bill\")\n    .style.format(precision=1, thousands=\",\", decimal=\".\")\n    .bar(\n        align=0,\n        cmap=\"bwr\",\n        height=50,\n        width=60,\n        props=\"width: 120px; border-right: 1px solid black;\",\n    )\n)\n\n\n\n\n\n\nTable 9: Bar Charts in Your Table with Alignment Set using a float number\n\n\n \nday\nsmoker\ntime\ntotal_bill\ntip\nsize\n\n\n\n\n3\nFri\nYes\nLunch\n12.8\n2.1\n2.0\n\n\n9\nThur\nNo\nLunch\n15.2\n2.1\n2.0\n\n\n1\nFri\nNo\nLunch\n16.0\n3.0\n3.0\n\n\n2\nFri\nYes\nDinner\n16.3\n3.0\n2.0\n\n\n10\nThur\nYes\nLunch\n16.5\n2.6\n2.0\n\n\n4\nSat\nNo\nDinner\n17.8\n2.8\n2.0\n\n\n6\nSun\nNo\nDinner\n18.4\n3.0\n3.0\n\n\n8\nThur\nNo\nDinner\n18.8\n3.0\n2.0\n\n\n5\nSat\nYes\nDinner\n20.4\n2.7\n2.0\n\n\n0\nFri\nNo\nDinner\n22.5\n3.2\n2.0\n\n\n7\nSun\nYes\nDinner\n23.1\n3.5\n2.0"
  },
  {
    "objectID": "posts/pandas_styler/pandas_style.html#formatting",
    "href": "posts/pandas_styler/pandas_style.html#formatting",
    "title": "Mastering Pandas DataFrame Styling for a Stunning Presentation",
    "section": "Formatting",
    "text": "Formatting\nBelow is an illustrative example of the myriad possibilities when it comes to fine-tuning the style of a DataFrame. In this code, we’ve undertaken various stylistic enhancements:\n\nBackground Gradient: We’ve applied a nice background color gradient to the columns along the vertical axis.\nCaption: We’ve set a descriptive caption for the DataFrame.\nVisual Styling: We’ve specified text alignment and introduced hovering options to make highlighted values pop even more.\nColumn Names Formatting: We’ve reformatted our column names for clarity and aesthetics.\nColumn Hiding: We’ve concealed the ‘smoker’ column, decluttering our view.\nIndex Hiding: We’ve also hidden the index labels for a cleaner look.\nNumerical Formatting: We’ve individually specified the numerical representation, including adding a dollar sign for currency values.\n\nThis demonstration underscores the virtually endless possibilities for customizing the appearance of your DataFrame. However, it’s important to note that the physical attributes set by the set_table_styles method won’t be exported to Excel, should you choose to do so. Just a handy tidbit to keep in mind.\n\n\nCode\n# Start by resetting the index and renaming columns with underscores\n# Replace underscores with spaces for better readability\n(\n    grouped.reset_index().rename(columns=lambda x: x.replace(\"_\", \" \"))\n    # Sort the DataFrame by the 'total bill' column\n    .sort_values(by=\"total bill\")\n    # Apply Pandas Styler to format the table\n    .style\n    # Apply background color gradient to columns along the vertical axis (axis=0)\n    .background_gradient(cmap=\"viridis\", axis=0)\n    # Set a caption for the table\n    .set_caption(\"Exploring Dining Trends: Bill Amounts, Tips, and Party Sizes\")\n    # Customize the table's visual styling\n    .set_table_styles(\n        [\n            {\n                \"selector\": \"th.col_heading\",\n                \"props\": \"text-align: center; font-size: 1.5em;\",\n            },\n            {\"selector\": \"td\", \"props\": \"text-align: center;\"},\n            {\n                \"selector\": \"td:hover\",\n                \"props\": \"font-style: italic; color: black; font-weight:bold; background-color : #ffffb3;\",\n            },\n        ],\n        overwrite=False,\n    )\n    # Apply custom formatting to the index labels (convert to uppercase)\n    .format_index(str.upper, axis=1)\n    # Hide the 'smoker' column from the table\n    .hide(subset=[\"smoker\"], axis=1)\n    # Hide the index label (row numbers)\n    .hide(axis=\"index\")\n    # Format specific columns with dollar signs and one decimal place\n    .format(\n        {\n            \"total bill\": \"$ {:.1f}\",\n            \"tip\": \"$ {:.1f}\",\n            \"size\": \"{:.0f}\",\n        }\n    )\n)\n\n\n\n\n\n\nTable 10: Formatting tables\n\n\nDAY\nTIME\nTOTAL BILL\nTIP\nSIZE\n\n\n\n\nFri\nLunch\n$ 12.8\n$ 2.1\n2\n\n\nThur\nLunch\n$ 15.2\n$ 2.1\n2\n\n\nFri\nLunch\n$ 16.0\n$ 3.0\n3\n\n\nFri\nDinner\n$ 16.3\n$ 3.0\n2\n\n\nThur\nLunch\n$ 16.5\n$ 2.6\n2\n\n\nSat\nDinner\n$ 17.8\n$ 2.8\n2\n\n\nSun\nDinner\n$ 18.4\n$ 3.0\n3\n\n\nThur\nDinner\n$ 18.8\n$ 3.0\n2\n\n\nSat\nDinner\n$ 20.4\n$ 2.7\n2\n\n\nFri\nDinner\n$ 22.5\n$ 3.2\n2\n\n\nSun\nDinner\n$ 23.1\n$ 3.5\n2"
  },
  {
    "objectID": "posts/pandas_styler/pandas_style.html#get-a-clearer-overview-with-set_sticky",
    "href": "posts/pandas_styler/pandas_style.html#get-a-clearer-overview-with-set_sticky",
    "title": "Mastering Pandas DataFrame Styling for a Stunning Presentation",
    "section": "Get a Clearer Overview with set_sticky",
    "text": "Get a Clearer Overview with set_sticky\nWhat if you find yourself facing a DataFrame with more columns than can comfortably fit on your screen, yet you still wish to inspect each column individually? In the past, you might have resorted to using pd.set_option('display.max_columns', xyz) to expand the display. However, there’s a much more elegant solution: set_sticky.\nset_sticky introduces a clever CSS trick that permanently pins the index or column headers within a scrolling frame. In our case, although the ‘tips’ DataFrame doesn’t have an excessive number of columns, we’ve concatenated 10 DataFrames together to showcase the remarkable utility of set_sticky. As you scroll horizontally, you’ll notice that you can now conveniently inspect all the columns while the index remains firmly in place, thanks to the magic of set_sticky. Let’s explore this feature below.\n\n\nCode\n(pd.concat([df for i in range(10)], axis=1).head().style.set_sticky(axis=\"index\"))\n\n\n\n\n\n\nTable 12: Demonstrating set_sticky\n\n\n \ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.990000\n1.010000\nFemale\nNo\nSun\nDinner\n2\n16.990000\n1.010000\nFemale\nNo\nSun\nDinner\n2\n16.990000\n1.010000\nFemale\nNo\nSun\nDinner\n2\n16.990000\n1.010000\nFemale\nNo\nSun\nDinner\n2\n16.990000\n1.010000\nFemale\nNo\nSun\nDinner\n2\n16.990000\n1.010000\nFemale\nNo\nSun\nDinner\n2\n16.990000\n1.010000\nFemale\nNo\nSun\nDinner\n2\n16.990000\n1.010000\nFemale\nNo\nSun\nDinner\n2\n16.990000\n1.010000\nFemale\nNo\nSun\nDinner\n2\n16.990000\n1.010000\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.340000\n1.660000\nMale\nNo\nSun\nDinner\n3\n10.340000\n1.660000\nMale\nNo\nSun\nDinner\n3\n10.340000\n1.660000\nMale\nNo\nSun\nDinner\n3\n10.340000\n1.660000\nMale\nNo\nSun\nDinner\n3\n10.340000\n1.660000\nMale\nNo\nSun\nDinner\n3\n10.340000\n1.660000\nMale\nNo\nSun\nDinner\n3\n10.340000\n1.660000\nMale\nNo\nSun\nDinner\n3\n10.340000\n1.660000\nMale\nNo\nSun\nDinner\n3\n10.340000\n1.660000\nMale\nNo\nSun\nDinner\n3\n10.340000\n1.660000\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.010000\n3.500000\nMale\nNo\nSun\nDinner\n3\n21.010000\n3.500000\nMale\nNo\nSun\nDinner\n3\n21.010000\n3.500000\nMale\nNo\nSun\nDinner\n3\n21.010000\n3.500000\nMale\nNo\nSun\nDinner\n3\n21.010000\n3.500000\nMale\nNo\nSun\nDinner\n3\n21.010000\n3.500000\nMale\nNo\nSun\nDinner\n3\n21.010000\n3.500000\nMale\nNo\nSun\nDinner\n3\n21.010000\n3.500000\nMale\nNo\nSun\nDinner\n3\n21.010000\n3.500000\nMale\nNo\nSun\nDinner\n3\n21.010000\n3.500000\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.680000\n3.310000\nMale\nNo\nSun\nDinner\n2\n23.680000\n3.310000\nMale\nNo\nSun\nDinner\n2\n23.680000\n3.310000\nMale\nNo\nSun\nDinner\n2\n23.680000\n3.310000\nMale\nNo\nSun\nDinner\n2\n23.680000\n3.310000\nMale\nNo\nSun\nDinner\n2\n23.680000\n3.310000\nMale\nNo\nSun\nDinner\n2\n23.680000\n3.310000\nMale\nNo\nSun\nDinner\n2\n23.680000\n3.310000\nMale\nNo\nSun\nDinner\n2\n23.680000\n3.310000\nMale\nNo\nSun\nDinner\n2\n23.680000\n3.310000\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.590000\n3.610000\nFemale\nNo\nSun\nDinner\n4\n24.590000\n3.610000\nFemale\nNo\nSun\nDinner\n4\n24.590000\n3.610000\nFemale\nNo\nSun\nDinner\n4\n24.590000\n3.610000\nFemale\nNo\nSun\nDinner\n4\n24.590000\n3.610000\nFemale\nNo\nSun\nDinner\n4\n24.590000\n3.610000\nFemale\nNo\nSun\nDinner\n4\n24.590000\n3.610000\nFemale\nNo\nSun\nDinner\n4\n24.590000\n3.610000\nFemale\nNo\nSun\nDinner\n4\n24.590000\n3.610000\nFemale\nNo\nSun\nDinner\n4\n24.590000\n3.610000\nFemale\nNo\nSun\nDinner\n4"
  },
  {
    "objectID": "posts/pandas_styler/pandas_style.html#concatenating-dataframe-outputs",
    "href": "posts/pandas_styler/pandas_style.html#concatenating-dataframe-outputs",
    "title": "Mastering Pandas DataFrame Styling for a Stunning Presentation",
    "section": "Concatenating DataFrame Outputs",
    "text": "Concatenating DataFrame Outputs\nYou can combine two or more Stylers if they have the same columns. This is particularly handy when presenting summary statistics for a DataFrame\n\n\nCode\n(\n    df.groupby(\"day\")[[\"total_bill\", \"tip\", \"size\"]]\n    .mean()\n    .style.format(precision=1)\n    .concat(\n        df[[\"total_bill\", \"tip\", \"size\"]]\n        .agg([\"mean\", \"median\", \"sum\"])\n        .style.format(precision=1)\n        .relabel_index([\"Average\", \"Median\", \"Sum\"])\n    )\n)\n\n\n\n\n\n\nTable 13: Demonstrating Easily Concatenating Different DataFrame Outputs\n\n\n \ntotal_bill\ntip\nsize\n\n\nday\n \n \n \n\n\n\n\nFri\n17.2\n2.7\n2.1\n\n\nSat\n20.4\n3.0\n2.5\n\n\nSun\n21.4\n3.3\n2.8\n\n\nThur\n17.7\n2.8\n2.5\n\n\nAverage\n19.8\n3.0\n2.6\n\n\nMedian\n17.8\n2.9\n2.0\n\n\nSum\n4827.8\n731.6\n627.0"
  },
  {
    "objectID": "posts/labour_day/labour_day.html",
    "href": "posts/labour_day/labour_day.html",
    "title": "Tidy Tuesday: Union Membership in the United States",
    "section": "",
    "text": "Photo by Claudio Schwarz\n\n\nHappy Labor Day! 🛠️ As we celebrate the achievements of workers and the contributions they’ve made to society, what better way to delve into the world of labor and employment than with a fresh Tidy Tuesday dataset? This week’s data comes from the comprehensive “Union Membership, Coverage, and Earnings from the CPS” dataset, courtesy of Barry Hirsch from Georgia State University, David Macpherson from Trinity University, and William Even from Miami University.\nThis dataset is a rich source of information, shedding light on the intricate dynamics of union membership, coverage, and earnings. So, grab your coffee, get comfortable, and let’s embark on this Tidy Tuesday journey together, exploring the intricate tapestry of labor data!\n\nImport data\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nimport geopandas\nfrom lets_plot.geo_data import *\n\nfrom lets_plot import *\nfrom lets_plot.mapping import as_discrete\n\nLetsPlot.setup_html()\n\n\n\n            \n            \n            \n\n\nThere are three distinct datasets to investigate: one focusing on demographic information, another centered on wage data, and the third associated with various states. Let’s read them all in.\n\n\nCode\ndemographic_df = pd.read_csv(\n    \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-09-05/demographics.csv\"\n)\nwages_df = pd.read_csv(\n    \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-09-05/wages.csv\"\n)\nstates_df = pd.read_csv(\n    \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-09-05/states.csv\"\n)\n\n\n\n\nData dictionaries\nHere are the corresponding data dictionaries:\nDemographics data: Data sources:\n\n1973-1981: May Current Population Survey (CPS)\n1982: No union questions available\n1983-2022: CPS Outgoing Rotation Group (ORG) Earnings Files\n\nThe definition of union membership was expanded in 1977 to include “employee associations similar to a union”.\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nyear\ndouble\nWhen the data was collected.\n\n\nsample_size\ndouble\nThe number of wage and salary workers ages 16 and over who were surveyed.\n\n\nemployment\ndouble\nWage and salary employment in thousands.\n\n\nmembers\ndouble\nEmployed workers who are union members in thousands.\n\n\ncovered\ndouble\nWorkers covered by a collective bargaining agreement in thousands.\n\n\np_members\ndouble\nPercent of employed workers who are union members.\n\n\np_covered\ndouble\nPercent of employed workers who are covered by a collective bargaining agreement.\n\n\nfacet\ncharacter\nThe sector or demographic group contained in this row of data.\n\n\n\nWages data: Data sources:\n\n1973-1981: May Current Population Survey (CPS)\n1982: No union questions available\n1983-2022: CPS Outgoing Rotation Group (ORG) Earnings Files\n\nThe definition of union membership was expanded in 1977 to include “employee associations similar to a union”.\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nyear\ndouble\nWhen the data was collected.\n\n\nsample_size\ndouble\nThe number of wage and salary workers ages 16 and over who were surveyed and provided earnings and hours worked information.\n\n\nwage\ndouble\nMean hourly earnings in nominal dollars.\n\n\nat_cap\ndouble\nPercent of workers with weekly earnings at the top code of $999 through 1988, $1923 in 1989-97, and $2885 beginning in 1998, with individuals assigned mean earnings above the cap based on annual estimates of the gender-specific Pareto distribution.\n\n\nunion_wage\ndouble\nMean wage among union members.\n\n\nnonunion_wage\ndouble\nMean wage among nonunion workers.\n\n\nunion_wage_premium_raw\ndouble\nThe percentage difference between the union and nonunion wage.\n\n\nunion_wage_premium_adjusted\ndouble\nEstimated as exp(b)-1 where b is the regression coefficient on a union membership variable (equal to 1 if union and 0 otherwise) from a semi-logarithmic wage equation, with controls included for worker/job characteristics. Included in the all-worker wage equation are the control variables: years of schooling, potential years of experience [proxied by age minus years of schooling minus 6] and its square [both interacted with gender], and categorical variables for marital status, race and ethnicity, gender, part-time, large metropolitan area, state, public sector, broad industry, and broad occupation. Controls are omitted, as appropriate, for estimates within sectors or by demographic group [i.e., by class, gender, race, or industry sector]. Workers who do not report earnings but instead have them imputed [i.e., assigned] by the Census are removed from the estimation samples in all years, except 1994 and 1995 when imputed earners cannot be identified. Inclusion of imputed earners causes union wages to be understated, nonunion wages overstated, and union-nonunion wage differences understated. For 1994-95, the sample includes imputed earners and estimates in those years have been adjusted to remove the bias from imputation.\n\n\nfacet\ncharacter\nThe sector or demographic group contained in this row of data.\n\n\n\nStates data:\nData source: Current Population Survey (CPS) Outgoing Rotation Group (ORG) Earnings Files\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nstate_census_code\ndouble\nCensus state code used in CPS\n\n\nstate\ncharacter\nState name.\n\n\nsector\ncharacter\nEmployment sector.\n\n\nobservations\ndouble\nCPS sample size.\n\n\nemployment\ndouble\nWage and salary employment in thousands.\n\n\nmembers\ndouble\nEmployed workers who are union members in thousands.\n\n\ncovered\ndouble\nWorkers covered by a collective bargaining agreement in thousands.\n\n\np_members\ndouble\nPercent of employed workers who are union members.\n\n\np_covered\ndouble\nPercent of employed workers who are covered by a collective bargaining agreement.\n\n\nstate_abbreviation\ncharacter\nState abbreviation.\n\n\nyear\ndouble\nYear of the survey.\n\n\n\nAs in our previous Tidy Tuesday blog, I believe we can address the following questions using this dataset:\n\nWhat are the overarching trends in the labor force, particularly regarding union memberships?\nDo specific demographic groups or occupations display a higher likelihood of union membership?\nDo union members experience any financial advantages or benefits compared to non-union workers?\nWhich states have the highest number of union members or affiliated unions?\n\n\n\nWhat are the overarching trends in the labor force, particularly regarding union memberships?\nThe figure below illustrates a consistent upward trajectory in the workforce’s growth, whereas union membership and the number of individuals covered by unions have been on a declining trend over the decades.\n\n\nCode\n(\n    demographic_df.query(\"facet.str.contains('all wage')\")\n    .drop(columns=[\"sample_size\", \"facet\", \"p_members\", \"p_covered\"])\n    .melt(id_vars=\"year\", var_name=\"Description\")\n    .pipe(\n        lambda df: ggplot(df)\n        + geom_line(aes(\"year\", \"value\", color=\"Description\"), size=1.5)\n        + labs(\n            title=\"Evolution of Workforce Size and Union Membership Across the Years\",\n            subtitle=\"While employment numbers have been on the rise, the proportion of employed workers \\nwho are union members and covered by unions has been steadily decreasing.\",\n            x=\"\",\n            y=\"Number of people in thousands\",\n            caption=\"© 2023 by Barry T. Hirsch, David A. Macpherson, and William E. Even.\",\n        )\n        + theme(\n            plot_subtitle=element_text(size=12, face=\"italic\"),\n            plot_title=element_text(size=15, face=\"bold\"),\n        )\n    )\n)\n\n\n\n   \n   \nFigure 1: Evolution of Workforce Size and Union Membership Across the Years\n\n\n\n\n\nDo specific demographic groups or occupations display a higher likelihood of union membership?\nNext, our investigation will focus on identifying potential demographic disparities among workforce members and uncovering the professions with the highest likelihood of union membership. We’ll utilize the “p_members” column, which represents the percentage of employed workers who are union members, to delve into these aspects.\nAs evident in Figure 2, a substantial decrease in union membership is observable across nearly all demographic groups over the decades.\n\n\nCode\n(\n    demographic_df.query(\"facet.str.contains('demographics')\")\n    .assign(facet=lambda df: df.facet.replace({\"demographics:\": \"\"}, regex=True))\n    .pipe(\n        lambda df: ggplot(df, aes(\"year\", \"p_members\", color=\"facet\"))\n        + geom_line(\n            size=1.5,\n        )\n        + labs(\n            title=\"Demographic Trends in Union Membership as a Percentage of the Total Workforce Over Time\",\n            subtitle=\"Almost all demographic groups exhibit a significant decline in union membership over the decades.\",\n            x=\"\",\n            y=\"Percent of employed workers who are union members\",\n            caption=\"© 2023 by Barry T. Hirsch, David A. Macpherson, and William E. Even.\",\n        )\n        + theme(\n            plot_subtitle=element_text(size=12, face=\"italic\"),\n            plot_title=element_text(size=15, face=\"bold\"),\n        )\n        + ggsize(800, 500)\n    )\n)\n\n\n\n   \n   \nFigure 2: Evolution of Workforce Size and Union Membership Across the Years\n\n\n\nWhile certain professions exhibit declining trends in union memberships, it’s noteworthy that some public sector occupations, such as postal service, police, and local government jobs, maintain the highest and most consistent levels of union participation (Figure 3).\n\n\nCode\n(\n    demographic_df.query(\n        \"(~facet.str.contains('demographics')) and (~facet.str.contains('all'))\"\n    ).pipe(\n        lambda df: ggplot(df, aes(\"year\", \"p_members\", color=\"facet\"))\n        + geom_line(\n            size=1.5,\n        )\n        + labs(\n            title=\"Workforce Sector-Specific Trends in Union Membership\",\n            subtitle=\"Public sector occupations demonstrate higher union participation rates.\",\n            x=\"\",\n            y=\"Percent of employed workers who are union members\",\n            caption=\"© 2023 by Barry T. Hirsch, David A. Macpherson, and William E. Even.\",\n        )\n        + theme(\n            plot_subtitle=element_text(size=12, face=\"italic\"),\n            plot_title=element_text(size=15, face=\"bold\"),\n        )\n        + ggsize(800, 500)\n    )\n)\n\n\n\n   \n   \nFigure 3: Workforce Sector-Specific Trends in Union Membership\n\n\n\n\n\nDo union members experience any financial advantages or benefits compared to non-union workers?\nThe next figure is both straightforward and highly impactful, providing a direct comparison of average wages for union members versus non-union members.\n\n\nCode\n(\n    wages_df.query(\"facet == 'all wage and salary workers'\")\n    .drop(\n        columns=[\n            \"wage\",\n            \"sample_size\",\n            \"at_cap\",\n            \"union_wage_premium_raw\",\n            \"union_wage_premium_adjusted\",\n            \"facet\",\n        ]\n    )\n    .melt(id_vars=\"year\", var_name=\"Description\")\n    .pipe(\n        lambda df: ggplot(df, aes(\"year\", \"value\", color=\"Description\"))\n        + geom_line(\n            size=1.5,\n        )\n        + labs(\n            title=\"Differences in Mean Hourly Earnings for Union and Non-Union Workers\",\n            subtitle=\"Union workers consistently display higher mean wages in comparison to non-union members.\",\n            x=\"\",\n            y=\"Mean hourly earnings in nominal dollars\",\n            caption=\"© 2023 by Barry T. Hirsch, David A. Macpherson, and William E. Even.\",\n        )\n        + theme(\n            plot_subtitle=element_text(size=12, face=\"italic\"),\n            plot_title=element_text(size=15, face=\"bold\"),\n        )\n    )\n)\n\n\n\n   \n   \nFigure 4: Differences in Mean Hourly Earnings for Union and Non-Union Workers\n\n\n\nAs with our previous figures, we will now delve into demographic variations and job sector disparities. To unveil these potential distinctions, we will use the “union_wage_premium_raw” column, which indicates the percentage difference between union and non-union wages.\nThe most significant disparities in hourly wages (Figure 5) between unionized and non-unionized workers are observed within the construction sector. These disparities have fluctuated over the decades, ranging from below 40% to as high as 80%. It’s important to note that almost all sectors exhibit higher earnings for union members, except for federal, manufacturing, and wholesale/retail workers (particularly in recent times).\n\n\nCode\n(\n    wages_df.query(\n        \"(~facet.str.contains('demographics')) and (~facet.str.contains('all'))\"\n    ).pipe(\n        lambda df: ggplot(df, aes(\"year\", \"union_wage_premium_raw\", color=\"facet\"))\n        + geom_line(\n            size=1.5,\n        )\n        + labs(\n            title=\"Sector-specific Differences in Mean Hourly Earnings for Union and Non-Union Workers\",\n            subtitle=\"The disparities between wages range from -20% to +80%.\",\n            x=\"\",\n            y=\"The percentage difference between the union and nonunion wage\",\n            caption=\"© 2023 by Barry T. Hirsch, David A. Macpherson, and William E. Even.\",\n        )\n        + theme(\n            plot_subtitle=element_text(size=12, face=\"italic\"),\n            plot_title=element_text(size=15, face=\"bold\"),\n        )\n        + ggsize(800, 500)\n        + geom_hline(yintercept=0, linetype=5, color=\"#6c6c6c\", size=1)\n    )\n)\n\n\n\n   \n   \nFigure 5: Sector-specific Differences in Mean Hourly Earnings for Union and Non-Union Workers\n\n\n\nIn addition to gender-specific differences, it’s intriguing to observe the relationship between education levels and wage disparities between unionized and non-unionized workers. Workers with education levels below college tend to benefit more from union membership, whereas, in general, individuals with higher education levels tend to have lower wages when affiliated with a union.\n\n\nCode\n(\n    wages_df.query(\"facet.str.contains('demographics')\")\n    .assign(facet=lambda df: df.facet.replace({\"demographics:\": \"\"}, regex=True))\n    .pipe(\n        lambda df: ggplot(df, aes(\"year\", \"union_wage_premium_raw\", color=\"facet\"))\n        + geom_line(\n            size=1.5,\n        )\n        + labs(\n            title=\"Demographics-specific Differences in Mean Hourly Earnings for Union and Non-Union Workers\",\n            subtitle=\"It appears that females tend to benefit more from union membership in comparison to males, \\nparticularly in terms of the percentage difference between union and non-union wages.\",\n            x=\"\",\n            y=\"The percentage difference between the union and nonunion wage\",\n            caption=\"© 2023 by Barry T. Hirsch, David A. Macpherson, and William E. Even.\",\n        )\n        + theme(\n            plot_subtitle=element_text(size=12, face=\"italic\"),\n            plot_title=element_text(size=15, face=\"bold\"),\n        )\n        + ggsize(800, 500)\n        + geom_hline(yintercept=0, linetype=5, color=\"#6c6c6c\", size=1)\n    )\n)\n\n\n\n   \n   \nFigure 6: Demographics-specific Differences in Mean Hourly Earnings for Union and Non-Union Workers\n\n\n\n\n\nWhich states have the highest number of union members or affiliated unions?\nTo illustrate variations in union membership across states, we will leverage Lets Plot’s capabilities for visualizing spatial data. Given the extensive dataset, our focus will be on highlighting a subset of the data, specifically the most recent trends in 2022 across all sectors.\nAs evident in Figure 7, certain states like New York, Alaska, Hawaii, Washington, California, and Oregon have a significant portion of their workforce as union members. Conversely, states like Utah, South Dakota, and the Carolinas display lower percentages of union membership among their workforce.\n\n\nCode\nfiltered_data = states_df.query(\"sector == 'Total' and year == 2022\")\nboundaries = (\n    geocode_states(filtered_data[\"state\"])\n    .scope(\"USA\")\n    .inc_res(4)\n    .get_boundaries(resolution=4)\n)\n\n(\n    ggplot()\n    + geom_livemap()\n    + geom_polygon(\n        aes(color=\"p_members\", fill=\"p_members\"),\n        data=states_df.query(\"sector == 'Total' and year == 2022\"),\n        map=boundaries,\n        alpha=0.9,\n        map_join=[[\"state\"], [\"state\"]],\n        show_legend=False,\n        size=0.01,\n    )\n    + theme(\n        axis_title=\"blank\", axis_text=\"blank\", axis_ticks=\"blank\", axis_line=\"blank\"\n    )\n    + scale_fill_brewer(palette=\"Greens\")\n    + ggsize(800, 500)\n)\n\n\n\n   \n   \nFigure 7: Percentage of Union Members Across States\n\n\n\nThere you have it! I hope you found this week’s Tidy Tuesday analysis insightful. I would encourage everyone to dive deeper into this dataset as I’ve only scratched the surface; there’s a wealth of knowledge waiting to be uncovered here.\nHappy coding, and until next time, see you in our next exploration!"
  },
  {
    "objectID": "posts/human_day/human_day.html",
    "href": "posts/human_day/human_day.html",
    "title": "Tidy Tuesday: The Global Human Day",
    "section": "",
    "text": "Photo by Filip Mroz\nThis week’s Tidy Tuesday takes us to The Human Chronome Project, an initiative based at McGill University in Montreal. We’re exploring insights from their paper titled “The global human day in PNAS” and the accompanying dataset on Zenodo.\nThe publication offers a broad view of our species’ activities, shedding light on how economic activities fit into the grand scheme of life. It also highlights activities with significant potential for change. For a deeper dive into our research methodology and to explore supplementary visualizations, I encourage you to peruse the supplementary materials provided with the publication."
  },
  {
    "objectID": "posts/human_day/human_day.html#regression-diagnostic",
    "href": "posts/human_day/human_day.html#regression-diagnostic",
    "title": "Tidy Tuesday: The Global Human Day",
    "section": "Regression diagnostic",
    "text": "Regression diagnostic\nNext, we’ll perform some diagnostic procedures for our regression model. These diagnostics will serve the purpose of evaluating how well our model aligns with its underlying assumptions. This step is crucial for ensuring the reliability and validity of our regression analysis.\nAs depicted in Figure 4, our residuals exhibit a relatively normal distribution. While there are a few data points that may have contributed to some erroneous predictions, we have opted to maintain the current state of the analysis as the results appear reasonable.\n\n\nCode\n# Create a histogram plot of residuals\nhist_plot = res.resid.to_frame(name=\"residuals\").pipe(\n    lambda df: ggplot(df, aes(\"residuals\"))\n    + geom_histogram(bins=15)\n    + labs(\n        title=\"Histogram of Residuals\",\n    )\n    + theme(plot_title=element_text(size=15, face=\"bold\"))\n)\n\n# Create a residuals plot\nresiduals_plot = (\n    res.resid.to_frame(name=\"residuals\")\n    .reset_index()  # Reset the index for plotting\n    .pipe(\n        lambda df: ggplot(df, aes(\"index\", \"residuals\"))\n        + geom_point(size=5, alpha=0.05)  # Plot residuals as points\n        + labs(\n            title=\"Residuals\",  # Title for the residuals plot\n        )\n        + theme(\n            plot_title=element_text(size=15, face=\"bold\")  # Customize title appearance\n        )\n    )\n)\n\n\n# Create a Q-Q plot of residuals\nqqplot = res.resid.to_frame(name=\"residuals\").pipe(\n    lambda df: qq_plot(data=df, sample=\"residuals\", size=5, alpha=0.05)\n    + labs(\n        title=\"Q-Q Plot\",\n    )\n    + theme(plot_title=element_text(size=15, face=\"bold\"))\n)\n\n# Create a fit plot comparing predicted values and log(GDP per population)\nfit_plot = (\n    pd.concat([res.fittedvalues, df.gdp_per_population_log], axis=1)\n    .rename(\n        columns={\n            0: \"Predicted value\",\n            \"gdp_per_population_log\": \"log(GDP per population)\",\n        }\n    )\n    .pipe(\n        lambda df: ggplot(df, aes(\"Predicted value\", \"log(GDP per population)\"))\n        + geom_point(size=5, alpha=0.05)\n        + labs(\n            title=\"Fit Plot\",\n        )\n        + theme(plot_title=element_text(size=15, face=\"bold\"))\n        + geom_abline(slope=1, size=1, linetype=\"dashed\", color=\"red\")\n    )\n)\n\n# Combine the three plots into a grid with two columns\n(gggrid([hist_plot, residuals_plot, qqplot, fit_plot], ncol=2) + ggsize(800, 600))\n\n\n\n   \n   \nFigure 4: Regression diagnostic\n\n\n\nAnd there you have it, folks! We hope you’ve found this week’s exploration insightful. Happy coding to all, and I look forward to seeing you next week for more exciting discoveries and learning opportunities. Until then, stay curious and keep exploring! 👩‍💻👨‍💻"
  },
  {
    "objectID": "posts/2023-08-08-HotOnes/2023_08_08_HotOnes.html",
    "href": "posts/2023-08-08-HotOnes/2023_08_08_HotOnes.html",
    "title": "Tidy Tuesday: Hot Ones Episodes",
    "section": "",
    "text": "I’ve decided to get more involved in the Tidy Tuesday movement. I think it’s a really enjoyable way to improve my skills in working with data, like analyzing, organizing, and visualizing it. The cool datasets they provide make it even more interesting. More information on Tidy Tuesday and their past datasets can be found here.\nThis week we have a dataset related to the show Hot Ones. Hot Ones is a unique web series that combines spicy food challenges with celebrity interviews. Hosted by Sean Evans, guests tackle increasingly hot chicken wings while answering questions, leading to candid and entertaining conversations. The show’s blend of heat and honesty has turned it into a global sensation, offering a fresh take on interviews and captivating audiences worldwide.\nLet’s see what we can learn from the data 🔬🕵️‍♂️."
  },
  {
    "objectID": "posts/2023-08-08-HotOnes/2023_08_08_HotOnes.html#what-differences-can-be-observed-in-the-spiciness-of-sauces-as-we-look-across-the-various-seasons",
    "href": "posts/2023-08-08-HotOnes/2023_08_08_HotOnes.html#what-differences-can-be-observed-in-the-spiciness-of-sauces-as-we-look-across-the-various-seasons",
    "title": "Tidy Tuesday: Hot Ones Episodes",
    "section": "What differences can be observed in the spiciness of sauces as we look across the various seasons?",
    "text": "What differences can be observed in the spiciness of sauces as we look across the various seasons?\n\n\nCode\n(sauces\n .groupby('season')\n .agg(AVG_scoville=('scoville', 'mean'),\n      median_scoville=('scoville', 'median'),\n     )\n .reset_index()\n .melt(id_vars='season',\n       var_name='statistics',\n      )\n .pipe(lambda df: (ggplot(df, aes('season', 'value',fill='statistics'))\n                   + geom_bar(stat='identity', show_legend= False)\n                   + facet_wrap('statistics',nrow=1,scales='free_y')\n                   + labs(x='Seasons',\n                          y='Scoville Units'\n                         )\n                   + theme(plot_title=element_text(size=20,face='bold'))\n                  + ggsize(1000,600)\n                  )\n      )\n \n)\n\n\n\n   \n   \nFigure 1: Average and median Scoville Units across the Hot Ones’ seasons\n\n\n\nThere seems to be a shift during the season 3-5 period as can be seen in Figure 1. Both indicators – mean and median Scoville Units – show a consistent upward trend over this time frame and later on they stabilize.\nWhat about the overall spread of the data? 🤔\n\n\nCode\n(sauces\n .loc[:, ['season', 'scoville']]\n .pipe(lambda df: (ggplot(df, aes('season', 'scoville'))\n                   + geom_boxplot()\n                   + scale_y_log10()\n                   + labs(x='Seasons',\n                          y='log(Scoville Units)'\n                         )\n                   + theme(plot_title=element_text(size=20,face='bold'))\n                   + ggsize(1000,600)\n                  )\n      )\n)\n\n\n\n   \n   \nFigure 2: Spread of Scoville Units across the Hot Ones’ seasons\n\n\n\nHere are some observations: Season 5 exhibits the widest range, featuring sauces with Scoville Units spanning from 450 to 2,000,000. In addition, starting from season 6 onwards, the averages, medians, and ranges of Scoville Units appear to even out."
  },
  {
    "objectID": "posts/2023-08-08-HotOnes/2023_08_08_HotOnes.html#has-every-individual-successfully-completed-all-the-episodes",
    "href": "posts/2023-08-08-HotOnes/2023_08_08_HotOnes.html#has-every-individual-successfully-completed-all-the-episodes",
    "title": "Tidy Tuesday: Hot Ones Episodes",
    "section": "Has every individual successfully completed all the episodes?",
    "text": "Has every individual successfully completed all the episodes?\nTo answer this question we will use the episodes dataframe. Keep in mind there are 300 episodes in this dataframe. The finished column can be useful here. Just by looking for entries where finished == False we will have our answer.\n\n\nCode\n(episodes\n .query(\"finished==False\")\n [['season', 'guest', 'guest_appearance_number']]\n)\n\n\n\n\n\n\n\n\n\nseason\nguest\nguest_appearance_number\n\n\n\n\n0\n1\nTony Yayo\n1\n\n\n7\n1\nDJ Khaled\n1\n\n\n19\n2\nMike Epps\n1\n\n\n20\n2\nJim Gaffigan\n1\n\n\n24\n2\nRob Corddry\n1\n\n\n51\n3\nRicky Gervais\n1\n\n\n90\n4\nMario Batali\n1\n\n\n96\n5\nTaraji P. Henson\n1\n\n\n129\n7\nLil Yachty\n1\n\n\n130\n7\nE-40\n1\n\n\n144\n8\nShaq\n1\n\n\n171\n10\nChance the Rapper\n1\n\n\n185\n12\nEric André\n2\n\n\n218\n15\nQuavo\n1\n\n\n251\n17\nPusha T\n1\n\n\n\n\n\n\n\nTaking a closer look, it seems that around 15 participants didn’t make it through the entire Hot Ones interview challenge.Not bad out of 300 shows. And guess what? Eric André popped up on the show not just once, but twice! Now, the big question: did he conquer the hot seat in at least one of those interviews? Let’s plot the data to make it more visual…\n\n\nCode\n(episodes\n .query(\"finished==False\")\n .groupby('season')\n .finished\n .count()\n .to_frame()\n .reset_index()\n .pipe(lambda df: (ggplot(df, aes('season', 'finished'))\n                   + geom_bar(stat='identity')\n                   + labs(x='Seasons',\n                          y='Number of incomplete interviews'\n                         )\n                   + theme(plot_title=element_text(size=20,face='bold'))\n                   + ggsize(600,400)\n                  )\n      )\n\n)\n\n\n\n   \n   \nFigure 3: Number of incomplete interviews per season\n\n\n\nInterestingly, a majority of these incomplete interviews belong to season 2 (Figure 3). This fact is quite surprising, especially when you consider that the maximum Scoville value for that season was only 550,000 – nearly a quarter of the following year’s value, where only one person faced difficulty finishing the challenge."
  },
  {
    "objectID": "posts/2023-08-08-HotOnes/2023_08_08_HotOnes.html#what-is-the-completion-rate-per-season",
    "href": "posts/2023-08-08-HotOnes/2023_08_08_HotOnes.html#what-is-the-completion-rate-per-season",
    "title": "Tidy Tuesday: Hot Ones Episodes",
    "section": "What is the completion rate per season?",
    "text": "What is the completion rate per season?\nTo get to the bottom of this question, let’s start by figuring out how many episodes were in each season. We can grab this info from the season dataset. Just a heads-up, in season 9 they seem to threw in an extra episode. So, keep this in mind! Otherwise, you might end up with percentages that go beyond 100%.\n\n\nCode\n# First we need to find out the total number of episodes per season\n\nepisodes_per_season = (season\n                       [['season', 'episodes', 'note']]\n                       .set_index('season')\n                       # we need to extract the one extra episode in season 9\n                       .assign(note=lambda df: df.note\n                               .str.extract(r'([0-9.]+)')\n                               .astype(float),\n                        # add the two column together\n                               episodes=lambda df: df.episodes\n                               .add(df.note, fill_value=0)\n                              )\n                       .drop(columns='note')\n                       .squeeze()\n                      )\n\n\n\n\nCode\ncompletion_rate = (episodes\n                   .query(\"finished==True\")\n                   .groupby('season')\n                   .finished\n                   .sum()\n                   .div(episodes_per_season)\n                   .mul(100)\n                   .to_frame().reset_index()\n                   .rename(columns={0:'completion_rate'})\n                  )\n                   \n(completion_rate                  \n .pipe(lambda df: (ggplot(df, aes('season', 'completion_rate'))\n                   + geom_line(stat='identity')\n                   + labs(x='Seasons',\n                          y='% successful participants'\n                         )\n                   + theme(plot_title=element_text(size=20,face='bold'))\n                   + ggsize(600,400)\n                  )\n      )\n)\n\n\n\n   \n   \nFigure 4: Completion rate per season\n\n\n\nTaking a peek at Figure 4, it seems like the normalized completion rate hits its lowest point in season 1, closely followed by season 7. However, even in those seasons, the rate remains surprisingly high."
  },
  {
    "objectID": "posts/2023-08-08-HotOnes/2023_08_08_HotOnes.html#is-there-a-correlation-between-scoville-units-and-completion-rate",
    "href": "posts/2023-08-08-HotOnes/2023_08_08_HotOnes.html#is-there-a-correlation-between-scoville-units-and-completion-rate",
    "title": "Tidy Tuesday: Hot Ones Episodes",
    "section": "Is there a correlation between Scoville Units and completion rate?",
    "text": "Is there a correlation between Scoville Units and completion rate?\nHere’s a curious thought: could there be a link between Scoville Units and the completion rate? I’m just wondering if the spiciness level affects how well participants handle the challenge. Exploring this connection might add a spicy twist to the Hot Ones experience – let’s see where the data takes us!\n\n\nCode\n# AVG_scoville code comes from a code snippet \n# 'What differences can be observed in the spiciness \n# of sauces as we look across the various seasons?'\n\nAVG_scoville = (sauces\n                .groupby('season')\n                .agg(AVG_scoville=('scoville', 'mean'))\n                .squeeze()\n               )\n\n# Let's calculate the Pearson correlation coefficient. We have to discard the last value\n# as the completion rate is not defined for that\n\nstats.pearsonr(AVG_scoville.values[:-1],completion_rate.completion_rate.values[:-1])\n\n\nPearsonRResult(statistic=0.5039021286250108, pvalue=0.02349225734224539)\n\n\nThe Pearson correlation coefficient has its say: there’s actually a moderate positive correlation(0.5, p&lt;0.05) between Scoville Units and completion rate. Quite intriguing, isn’t it? Honestly, I was expecting the opposite outcome myself! It seems like the higher the average spiciness, the more determined the guests become. Take a look at Figure 5.\n\n\nCode\n(pd.concat([AVG_scoville,completion_rate],axis=1)\n .rename(columns={0:'success_rate'})\n .pipe(lambda df: (ggplot(df, aes('AVG_scoville', 'completion_rate'))\n                   + geom_point(size=5, alpha=0.5)\n                   + geom_smooth()\n                   + labs(x='Average Scoville units',\n                          y='% successful participants'\n                         )\n                   + theme(plot_title=element_text(size=20,face='bold'))\n                   + ggsize(600,400)\n                  )\n      )\n)\n\n\n\n   \n   \nFigure 5: Correlation between Average Scoville units and Completion rates"
  },
  {
    "objectID": "posts/2023-08-08-HotOnes/2023_08_08_HotOnes.html#are-there-any-returning-guests",
    "href": "posts/2023-08-08-HotOnes/2023_08_08_HotOnes.html#are-there-any-returning-guests",
    "title": "Tidy Tuesday: Hot Ones Episodes",
    "section": "Are there any returning guests?",
    "text": "Are there any returning guests?\nHas there been a brave soul who dared to make a return to the show for a second time? The column guest_appearance_number holds the answers you’re looking for.\n\n\nCode\n(episodes\n .query(\"guest_appearance_number &gt; 1\")\n [['guest','season', 'episode_season', 'finished']]\n)\n\n\n\n\n\n\n\n\n\nguest\nseason\nepisode_season\nfinished\n\n\n\n\n124\nEddie Huang\n6\n13\nTrue\n\n\n161\nJay Pharoah\n9\n999\nTrue\n\n\n183\nTom Segura\n12\n1\nTrue\n\n\n185\nEric André\n12\n3\nFalse\n\n\n188\nT-Pain\n12\n6\nTrue\n\n\n189\nAdam Richman\n12\n7\nTrue\n\n\n190\nAction Bronson\n12\n8\nTrue\n\n\n203\nNaN\n13\n11\nTrue\n\n\n214\nRussell Brand\n14\n11\nTrue\n\n\n215\nSteve-O\n14\n12\nTrue\n\n\n241\nGordon Ramsay\n16\n14\nTrue\n\n\n254\nPost Malone\n18\n1\nTrue\n\n\n\n\n\n\n\nIt looks like a total of 12 individuals have taken on the challenge not once, but twice. Hats off to their courage! 🎩"
  },
  {
    "objectID": "articles.html",
    "href": "articles.html",
    "title": "Articles",
    "section": "",
    "text": "Speed up your Python code 60x\n\n\n\n\n\n\n\nRust\n\n\n\n\n\n\n\n\n\n\n\nJun 7, 2024\n\n\nAdam Cseresznye\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday: The Global Human Day\n\n\n\n\n\n\n\nTidy Tuesday\n\n\n\n\n\n\n\n\n\n\n\nSep 11, 2023\n\n\nAdam Cseresznye\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday: Union Membership in the United States\n\n\n\n\n\n\n\nTidy Tuesday\n\n\n\n\n\n\n\n\n\n\n\nSep 5, 2023\n\n\nAdam Cseresznye\n\n\n\n\n\n\n  \n\n\n\n\nMastering Pandas DataFrame Styling for a Stunning Presentation\n\n\n\n\n\n\n\nPandas\n\n\n\n\n\n\n\n\n\n\n\nAug 27, 2023\n\n\nAdam Cseresznye\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday: Refugees\n\n\n\n\n\n\n\nTidy Tuesday\n\n\n\n\n\n\n\n\n\n\n\nAug 22, 2023\n\n\nAdam Cseresznye\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday: Spam E-mail\n\n\n\n\n\n\n\nTidy Tuesday\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2023\n\n\nAdam Cseresznye\n\n\n\n\n\n\n  \n\n\n\n\nThe Ultimate Speed Test: Pandas vs Polars\n\n\n\n\n\n\n\nPolars\n\n\nPandas\n\n\n\n\n\n\n\n\n\n\n\nAug 13, 2023\n\n\nAdam Cseresznye\n\n\n\n\n\n\n  \n\n\n\n\nGetting Started with Polars\n\n\n\n\n\n\n\nPolars\n\n\n\n\n\n\n\n\n\n\n\nAug 11, 2023\n\n\nAdam Cseresznye\n\n\n\n\n\n\n  \n\n\n\n\n🥳🎉 Two IBM certificates and some geospatial data\n\n\n\n\n\n\n\ncelebration\n\n\ngeospatial data\n\n\n\n\n\n\n\n\n\n\n\nAug 10, 2023\n\n\nAdam Cseresznye\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday: Hot Ones Episodes\n\n\n\n\n\n\n\nTidy Tuesday\n\n\n\n\n\n\n\n\n\n\n\nAug 9, 2023\n\n\nAdam Cseresznye\n\n\n\n\n\n\n  \n\n\n\n\nHow to use the Lets-Plot library by JetBrains\n\n\n\n\n\n\n\nLets-Plot\n\n\nggplot2\n\n\n\n\n\n\n\n\n\n\n\nAug 5, 2023\n\n\nAdam Cseresznye\n\n\n\n\n\n\n  \n\n\n\n\nTop 10 things I learned from the book Effective Pandas by Matt Harrison\n\n\n\n\n\n\n\nPandas\n\n\n\n\n\n\n\n\n\n\n\nAug 2, 2023\n\n\nAdam Cseresznye\n\n\n\n\n\n\n  \n\n\n\n\n🏆🥉3rd place at Kaggle’s Classification with a Tabular Vector Borne Disease Dataset\n\n\n\n\n\n\n\ncelebration\n\n\nKaggle\n\n\n\n\n\n\n\n\n\n\n\nJun 4, 2023\n\n\nAdam Cseresznye\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\nHi there! Welcome to my website! \n",
    "section": "",
    "text": "I am Adam, a PhD candidate at the University of Antwerp working in the field of environmental toxicology. This platform is dedicated to my pursuit of mastering data science (DS), and I invite you to join me on this journey.\n\n\n\n\n\n\n\nDiscover my scientific contributions.\n\n\n\nExplore my projects.\n\n\n\nRead interesting tidbits on DS.\n\n\n\n\n\nInterested in collaboration or have questions? Get in touch!\n\nContact Me"
  },
  {
    "objectID": "index.html#explore-my-work",
    "href": "index.html#explore-my-work",
    "title": "\nHi there! Welcome to my website! \n",
    "section": "",
    "text": "Discover my scientific contributions.\n\n\n\nExplore my projects.\n\n\n\nRead interesting tidbits on DS."
  },
  {
    "objectID": "posts/folium/2023-07-09-Folium.html",
    "href": "posts/folium/2023-07-09-Folium.html",
    "title": "🥳🎉 Two IBM certificates and some geospatial data",
    "section": "",
    "text": "I’m happy to share that I’ve recently completed both IBM’s Data Analyst and Data Science Professional Certificates within the past month. The course content was well-structured, and I learned a great deal from these programs. For instance, I’ve always been interested in learning SQL, and this was the perfect chance to start exploring it.\nIf you’re curious about these certificates, you can find more information through the links provided below. But my learning journey doesn’t stop here—I’m planning to tackle most of the courses listed in the Data Science learning path on Coursera, so there’s more to come.\nWhile I’m at it, I wanted to introduce you to a neat library called Folium, which is fantastic for working with geospatial data. I came across Folium during the capstone project of the Data Science Specialization, where we had a fun task of predicting and visualizing the success of SpaceX rocket launches.\nIn this post, I’ll briefly share what I’ve learned about this library. I hope you’ll find it useful too. Let’s dive in!\n\n\nCode\nimport folium\nimport pandas as pd\nimport os\nfrom folium import plugins\n\n\nWe’ll be utilizing the dataset made available by https://open.toronto.ca/. This dataset includes the locations of bicycles installed on sidewalks and boulevards across the City of Toronto, wherever there’s a requirement for public bicycle parking facilities. By the way, I discovered this dataset through the Awesome Public Datasets repository on GitHub. If you haven’t already, I recommend checking them out.\n\n\nCode\n# Let's read in the file\n\nfor file in os.listdir():\n    if file.endswith(\".csv\"):\n        toronto_df = pd.read_csv(file)\n\n        print(f\"{file} read in as pandas dataframe\")\n\n\nStreet bicycle parking data - 4326.csv read in as pandas dataframe\n\n\nConsidering the original dataset has over 17,300 entries, we’ll keep things light by working with just 500 rows for now. It’s all for the sake of a demonstration, after all!\n\n\nCode\ntoronto_df = toronto_df.sample(n=500)\ntoronto_df.head()\n\n\n\n\n\n\n\n\n\n_id\nOBJECTID\nID\nADDRESSNUMBERTEXT\nADDRESSSTREET\nFRONTINGSTREET\nSIDE\nFROMSTREET\nDIRECTION\nSITEID\nWARD\nBIA\nASSETTYPE\nSTATUS\nSDE_STATE_ID\nX\nY\nLONGITUDE\nLATITUDE\ngeometry\n\n\n\n\n784\n4481427\n10424\nBP-05283\n15\nDundonald St\nNaN\nNaN\nDundonald St\nNaN\nNaN\n13.0\nNaN\nRing\nTemporarily Removed\n0\nNaN\nNaN\nNaN\nNaN\n{'type': 'MultiPoint', 'coordinates': [[-79.38...\n\n\n3297\n4483940\n15253\nBP-35603\n49\nHarbour Sq\nQueens Quay W\nSouth\nHarbour Sq\nWest\nNaN\n10.0\nThe Waterfront\nRing\nExisting\n0\nNaN\nNaN\nNaN\nNaN\n{'type': 'MultiPoint', 'coordinates': [[-79.37...\n\n\n13971\n4494614\n31121\nBP-22492\n200\nElizabeth St\nElizabeth St\nWest\nLa Plante Ave\nWest\nNaN\n11.0\nNaN\nRing\nExisting\n0\nNaN\nNaN\nNaN\nNaN\n{'type': 'MultiPoint', 'coordinates': [[-79.38...\n\n\n5139\n4485782\n17465\nBP-40070\n70\nPeter St\nKing St W\nNorth\nPeter St\nWest\nNaN\n10.0\nToronto Downtown West\nRing\nExisting\n0\nNaN\nNaN\nNaN\nNaN\n{'type': 'MultiPoint', 'coordinates': [[-79.39...\n\n\n7635\n4488278\n20375\nBP-27153\n39\nPrince Arthur Ave\nPrince Arthur Ave\nSouth\nBedford Rd\nEast\nNaN\n11.0\nNaN\nRing\nExisting\n0\nNaN\nNaN\nNaN\nNaN\n{'type': 'MultiPoint', 'coordinates': [[-79.39...\n\n\n\n\n\n\n\nThe geometry column holds the longitude and latitude information, but before we dive in, we need to extract the valuable details. No worries – we’ll make use of pandas’ str.extract for this task.\n\n\nCode\npattern = r\"(-?\\d+\\.\\d+),\\s*(-?\\d+\\.\\d+)\"\n\ntoronto_df_processed = toronto_df.assign(\n    LONGITUDE=lambda df: df.geometry.str.extract(pattern)[0],\n    LATITUDE=lambda df: df.geometry.str.extract(pattern)[1],\n).loc[:, [\"ASSETTYPE\", \"STATUS\", \"LONGITUDE\", \"LATITUDE\"]]\ntoronto_df_processed.head()\n\n\n\n\n\n\n\n\n\nASSETTYPE\nSTATUS\nLONGITUDE\nLATITUDE\n\n\n\n\n784\nRing\nTemporarily Removed\n-79.38378423783222\n43.6660359833018\n\n\n3297\nRing\nExisting\n-79.3774934493851\n43.6407633657936\n\n\n13971\nRing\nExisting\n-79.386799735149\n43.6589303889453\n\n\n5139\nRing\nExisting\n-79.3926661761316\n43.6460273003346\n\n\n7635\nRing\nExisting\n-79.3973838724551\n43.6693038734947\n\n\n\n\n\n\n\n\nCreating the map and displaying it\nHere’s an example of how to create a map without any overlaid data points.\n\n\nCode\ntoronto_map = folium.Map(\n    location=[43.651070, -79.347015], zoom_start=11, tiles=\"OpenStreetMap\"\n)\ntoronto_map\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\nFigure 1: The City of Toronto\n\n\n\n\n\nSuperimposing bike locations on the map with FeatureGroup\nAfter instantiating FeatureGroup, we can easily add the bike locations using the add_child method. It is really easy!\n\n\nCode\n# let's start with a clean copy of the map of Toronto\ntoronto_map = folium.Map(\n    location=[43.651070, -79.347015], zoom_start=11, tiles=\"OpenStreetMap\"\n)\n\n# instantiate a feature group \nbike_stations = folium.map.FeatureGroup()\n\n# loop through the bike stations\nfor lat, long in zip(toronto_df_processed.LATITUDE, toronto_df_processed.LONGITUDE):\n    bike_stations.add_child(\n        folium.features.CircleMarker(\n            [lat, long],\n            radius=5,\n            color=\"red\",\n            fill=True,\n            fill_color=\"yellow\",\n            fill_opacity=1,\n        )\n    )\n# add bike stations to the map\ntoronto_map.add_child(bike_stations)\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\nFigure 2: The City of Toronto with available bike locations\n\n\n\n\n\nAdding pop-up text with relevant information\nWe can also enhance this by adding a pop-up box that displays custom text of our choice.\n\n\nCode\n# let's start with a clean copy of the map of Toronto\ntoronto_map = folium.Map(\n    location=[43.651070, -79.347015], zoom_start=11, tiles=\"OpenStreetMap\"\n)\n\n# instantiate a feature group \nbike_stations = folium.map.FeatureGroup()\n\n# loop through the bike stations\nfor lat, long in zip(toronto_df_processed.LATITUDE, toronto_df_processed.LONGITUDE):\n    bike_stations.add_child(\n        folium.features.CircleMarker(\n            [lat, long],\n            radius=5,\n            color=\"grey\",\n            fill=True,\n            fill_color=\"white\",\n            fill_opacity=1,\n        )\n    )\n\n# add pop-up text to each marker on the map\nlatitudes = list(toronto_df_processed.LATITUDE)\nlongitudes = list(toronto_df_processed.LONGITUDE)\nlabels = list(toronto_df_processed.STATUS)\n\nfor lat, lng, label in zip(latitudes, longitudes, labels):\n    folium.Marker([lat, lng], popup=label).add_to(toronto_map)\n\n# add bike stations to map\ntoronto_map.add_child(bike_stations)\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\nFigure 3: The City of Toronto with available bike locations\n\n\n\n\n\nClustering the rental locations with MarkerCluster\nAnd the best part, which happens to be my favorite, is that we can also integrate a MarkerCluster. This comes in handy when we’re dealing with numerous data points clustered closely together on the map. With a MarkerCluster, you get to see their combined values instead of each one individually. It’s a fantastic feature!\n\n\nCode\n# let's start with a clean copy of the map of Toronto\ntoronto_map = folium.Map(\n    location=[43.651070, -79.347015], zoom_start=11, tiles=\"OpenStreetMap\"\n)\n\n# instantiate a mark cluster object \nbike_stations_cluster = plugins.MarkerCluster().add_to(toronto_map)\n\n# loop through the dataframe and add each data point to the mark cluster\nfor lat, lng, label, in zip(\n    toronto_df_processed.LATITUDE,\n    toronto_df_processed.LONGITUDE,\n    toronto_df_processed.STATUS,\n):\n    folium.Marker(\n        location=[lat, lng],\n        icon=None,\n        popup=label,\n    ).add_to(bike_stations_cluster)\n\n# display map\ntoronto_map\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\nFigure 4: Aggregated Bike Locations in the City of Toronto\n\n\n\nThat’s a wrap! I hope these examples have been helpful. Feel free to use these techniques in your next data science or geospatial project. Until next time, happy exploring!"
  },
  {
    "objectID": "posts/Kaggle/Kaggle.html",
    "href": "posts/Kaggle/Kaggle.html",
    "title": "🏆🥉3rd place at Kaggle’s Classification with a Tabular Vector Borne Disease Dataset",
    "section": "",
    "text": "Greetings everyone! I am thrilled to announce that I have achieved a significant milestone in my data science journey. I am delighted to share that I secured the 3rd position in Kaggle’s “Classification with a Tabular Vector Borne Disease Dataset” competition, surpassing 931 talented contenders! If you’re interested in learning more about my approach to the competition, I invite you to check out my detailed solution on the Kaggle discussion page.\n\n\n\n\n\nIn this exciting competition, the aim was to predict the three most probable diseases out of a total of 11 vector-borne diseases, including well-known names such as Zika, Yellow Fever, Malaria, and more. Our task involved analyzing the provided symptoms and using the MPA@3 evaluation metric for accurate predictions.\nMy journey in the world of Kaggle has been a relatively short one, with this competition marking my third participation. To give you some context, let me share a bit about my previous placements:\n\nReaching this point has been a long and rewarding endeavor. Over the past two years, I have been studying this topic in my free time while also pursuing a full-time Ph.D. research. It has been an exciting balancing act!\nParticipating in Kaggle competitions has been an invaluable learning experience for me. It has allowed me to apply my knowledge, explore new techniques, and connect with fellow data enthusiasts. I am deeply grateful to the organizers of Kaggle for providing a platform that fosters growth and healthy competition.\nWhile this achievement brings a sense of accomplishment, I am already diving into new competitions and expanding my skills. Currently, I am particularly interested in reproducible data and developing robust architectures for machine learning projects to enhance code reproducibility. In fact, I am considering writing a blog on this often overlooked topic.\nStay tuned for more :)\n#DataScience #KaggleCompetition #VectorBorneDiseases #MachineLearning #ReproducibleData"
  },
  {
    "objectID": "posts/Lets-Plot/2023_08_05_How_to_use_the_Lets_Plot_library_by_JetBrains.html",
    "href": "posts/Lets-Plot/2023_08_05_How_to_use_the_Lets_Plot_library_by_JetBrains.html",
    "title": "How to use the Lets-Plot library by JetBrains",
    "section": "",
    "text": "When I embarked on my data science journey, due to my academics background I quickly gravitated towards the R programming language. Like many R novices, I began with Hadley Wickham’s R for Data Science book which introduced me to the wonderful ggplot2 library. As my interest in machine learning grew, I made the switch to Python. Nowadays, for most of my data plotting needs, I rely mainly on matplotlib or seaborn. Though I love these libraries, their multiple ways of accomplishing the same tasks can be a bit cumbersome and challenging to learn at first.\nThat’s why in this article, I’m excited to introduce you to the Lets-Plot library by JetBrains. It is the closest you can get to ggplot’s syntax while using Python. While some traditional Python users might find the syntax a bit unfamiliar initially, I’m here to make a case for this fantastic library and hopefully inspire you to incorporate it into your day-to-day data science activities.\nTo showcase (some of) the key features of Lets-Plot, we will be utilizing the penguins dataset 🐧 from Github.\nWithout further ado, let’s dive right in and discover the power and beauty of Lets-Plot! 🐍📊\n\n\nCode\n# Installation\n# pip install lets-plot \n\n\n\n\nCode\n# Import libraries\nimport numpy as np\nimport pandas as pd\nfrom lets_plot import *\nfrom lets_plot.mapping import as_discrete\nLetsPlot.setup_html()\n\n\n\n            \n            \n            \n\n\n\n\nCode\naddress='https://gist.githubusercontent.com/slopp/ce3b90b9168f2f921784de84fa445651/raw/4ecf3041f0ed4913e7c230758733948bc561f434/penguins.csv'\ndf=pd.read_csv(address)\ndf.head()\n\n\n\n\n\n\n\n\n\nrowid\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\n1\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\n2\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\n3\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\n4\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\n5\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n\n\n\n\n\n\nSyntax Similarities: Lets-Plot and ggplot2\nFor our first exercise, I thought it would be beneficial to replicate a basic plot inspired by Hadley’s book. When comparing my code here with the one presented by him, you’ll notice that there is very little difference between the two. The syntax is nearly identical, making it a smooth transition from ggplot to Lets-Plot.\nNow, let’s take a closer look at the code. In the ggplot function, we define the DataFrame we’ll be working with, and the aesthetic mappings are set at the global level. We assign the values for the x and y axes, as well as the color argument, which groups the data based on the categorical variable representing the three different penguin species: Adelie, Gentoo, and Chinstrap. This color parameter is quite similar to seaborn’s hue, making it easy for those familiar with seaborn to adapt to Lets-Plot seamlessly.\nAfter the ggplot() function sets the global aesthetic mappings, the geom_point() function comes into play and draws the points defined by the x and y parameters, effectively creating a scatter plot. These points represent the data points from the penguins dataset, with x and y coordinates corresponding to the specified variables.\nAdditionally, we enhance the plot by using geom_smooth(method=‘lm’), which adds a smoothed conditional mean. The lm method stands for ‘linear model,’ indicating that the smoothing is based on a linear regression. This smoothed line helps reveal trends and patterns in the data, making it easier to observe any overall relationships between the variables.\nLet’s continue exploring more of what Lets-Plot has in store for us! 📊🐧🌈\n\n\nCode\n(ggplot(df,\n        aes(x='flipper_length_mm',\n            y = 'body_mass_g',\n            color='species'\n           )\n       )\n + geom_point() # Draw points defined by an x and y coordinate, as for a scatter plot.\n + geom_smooth(method='lm') # Add a smoothed conditional mean. ‘lm’ stands for 'linear model' as Smoothing method\n) \n\n\n   \n   \n\n\nIn the previous example, we highlighted the importance of placing the color parameter at the global level, which grouped the data by the three penguin species and showed separate regression lines for each group. However, if we prefer to depict the regression line for the entire dataset, regardless of the group association, we can do so just as easily. All we need to do is remove the color parameter from the aesthetics of the ggplot function and place it solely in the geom_point.\nAdditionally, to enhance the plot further, we can properly label the x and y axes, add a title and subtitle. With these simple adjustments, we can achieve the same output as Hadley’s original code, with little to no modification.\n\n\nCode\n(ggplot(df, \n        aes(x='flipper_length_mm',\n            y = 'body_mass_g',\n           )\n       )\n + geom_point(aes(color='species', shape='species'))\n + geom_smooth(method='lm')\n + labs(\n    title = \"Body mass and flipper length\",\n    subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n    x = \"Flipper length (mm)\", y = \"Body mass (g)\",\n  ) \n + scale_color_viridis() # lets-plots equivalent of the scale_color_colorblind()\n) \n\n\n   \n   \n\n\n\n\nVisualizing Data Based on Categorical Variables\nLets-Plot provides numerous options to showcase our data using categorical variables. From bar plots, box plots, and violin plots to pie charts, the possibilities are diverse. You can check out their API for reference. Let’s explore some examples to demonstrate the versatility of Lets-Plot in visualizing categorical data.\n\n\nCode\npenguin_bar = (ggplot(df,aes(x='species'))\n               + geom_bar()\n              )\n\npenguin_box = (ggplot(df,aes(x = 'species', y = 'body_mass_g'))\n               + geom_boxplot()\n              )\n\npenguin_density = (ggplot(df,aes('body_mass_g', color='species', fill='species'))\n                   + geom_density(alpha=0.5)\n                  )\n\npenguin_rel_frequency = (ggplot(df,aes(x = 'island', fill = 'species'))\n                         + geom_bar(position='fill')\n                        )\ngggrid([penguin_bar, penguin_box, penguin_density, penguin_rel_frequency], ncol=2)\n\n\n   \n   \n\n\n\n\nIncorporate Multiple Variables with facet_wrap\nSo far we’ve discovered how easy it is to plot data based on a single categorical variable. However, what if we want to depict relationships involving two or more categorical variables? That’s where facet_wrap comes in handy. This versatile function bears resemblance to similar functions found in seaborn or ggplot2 libraries.\nTo unlock the potential of facet_wrap, we simply need to define aesthetics, which can either be global or local to the mapping function. Then, we can use facet_wrap with the relevant categorical variable we want to visualize. It’s as simple as that!\n\n\nCode\n(ggplot(df, aes(x = 'flipper_length_mm', y = 'body_mass_g'))  \n + geom_point(aes(color = 'species', shape = 'species'), size = 4) \n + facet_wrap('island', nrow=1)\n + labs(title = \"Body mass and flipper length based on island\",\n        subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n        x = \"Flipper length (mm)\", y = \"Body mass (g)\",\n       )\n + theme(plot_title=element_text(size=20,face='bold'))\n + ggsize(1000,500)\n)\n\n\n   \n   \n\n\n\n\n\n\nReordering Categorical Variables Based On Statistics\nWhen visualizing data, a task I frequently encounter is ordering categorical variables in either ascending or descending order, based on statistics like median or mean. In my previous point on “Visualizing Data Based on Categorical Variables,” you noticed that the boxplot displayed categories in an unordered manner. However, consider how we can present them in an ascending order, determined by the median. This not only enhances the aesthetics of the plot but also provides valuable insights into the relationships among the categories.\n\n\nCode\n(ggplot(df,aes(as_discrete('species', order=1, order_by='..middle..'), \n               y = 'body_mass_g'))\n + geom_boxplot()\n)\n\n\n   \n   \n\n\nBy incorporating the as_discrete function, specifying the column, the ordering direction (1 for ascending, -1 for descending), and setting the order_by variable to middle (representing the median), the plot has become significantly more informative. This simple addition has allowed us to organize the categorical variables in a meaningful manner, improving the visualization’s clarity and aiding in the interpretation of relationships among the categories.\n\n\nChaining Pandas Code with Lets Plot Visualization\nOne of the best features of the pandas library is its remarkable customizability. With the help of the pd.pipe function, we can seamlessly integrate any of our own functions into method chains, as long as they return a DataFrame or Series. This opens up exciting possibilities to fully incorporate Lets-Plot into our code, just like pandas’ own built-in plotting functionality.\nWhile Lets-Plot may be slightly more verbose than pandas plotting, it offers significantly more flexibility and freedom for customization. Not to mention that some may consider it visually more appealing. With Lets-Plot integrated into our pandas code, we can effortlessly create stunning and informative plots, making data analysis an even more enjoyable experience.\n\n\nCode\n(df\n .groupby('species')\n [['body_mass_g', 'flipper_length_mm']]\n .mean()\n .reset_index()\n .pipe(lambda df: (ggplot(df)\n                   + geom_pie(aes(slice='body_mass_g', fill='species'), \n                              stat='identity',size=30, hole=0.2, stroke=1.0,\n                              labels=layer_labels().line('@body_mass_g').format('@body_mass_g', '{.0f}').size(20)\n                             )\n                   + labs(title = \"Body mass based on species\",\n                          subtitle = \"Representing how Lets-Plot can be used with pd. pipe\",\n                          x = \"\", y = \"\",\n                         )\n                   + theme(axis='blank',\n                          plot_title=element_text(size=20,face='bold'))\n                   + ggsize(500,400)\n                  )\n )\n)\n\n\n   \n   \n\n\nThat’s a wrap on the Lets-Plot library! There’s so much more to explore and learn about this powerful tool. I hope you found this introduction helpful and consider integrating Lets-Plot into your daily data analysis routine.\nHappy coding 🐍🖥️🔍🚀"
  },
  {
    "objectID": "posts/polars/Polars_revised.html",
    "href": "posts/polars/Polars_revised.html",
    "title": "Getting Started with Polars",
    "section": "",
    "text": "In the ever-evolving field of data science, effective data manipulation tools are essential. Enter Polars, a Rust-based library garnering attention within the data community. Boasting impressive speed and versatile capabilities, Polars is redefining our data management practices. In this blog, we delve into Polars’ core functions and practical applications, shedding light on how it empowers data professionals to efficiently tackle complex tasks.\nFor those well-versed in Pandas, Polars offers a blend of familiarity and innovation. Although this document is not designed to substitute the official documentation, it serves as a tool to provide you with insights into the capabilities that polars offers. Our goal is to ensure the continuous updating of this document.\nOur exploration of Polars is guided by insights from the Polars User Guide, Kevin Heavey’s perspectives in Modern Polars and Matt Harrison’s engaging tutorial on Polars at PyCon. We kickstart our exploration with Matt Harrison’s shared dataset, the US Department of Energy’s Automobile Fuel Economy data. Let’s begin this journey!"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#use-take-for-series-or-expressions",
    "href": "posts/polars/Polars_revised.html#use-take-for-series-or-expressions",
    "title": "Getting Started with Polars",
    "section": "Use take for Series or Expressions",
    "text": "Use take for Series or Expressions\nAn interesting approach is through Polars’ take method. This method can be used with Expr, Series, or np.ndarray. However, it’s worth noting that if you intend to utilize it with DataFrames, a preliminary conversion to Series is necessary.\n\n\nCode\ndf.select(pl.col(\"make\")).to_series().take(list(range(0, 5)))\n\n\n\n\nshape: (5,)\n\n\n\nmake\n\n\ncat\n\n\n\n\n\"Alfa Romeo\"\n\n\n…\n\n\n\"Subaru\""
  },
  {
    "objectID": "posts/polars/Polars_revised.html#using-filter",
    "href": "posts/polars/Polars_revised.html#using-filter",
    "title": "Getting Started with Polars",
    "section": "Using filter",
    "text": "Using filter\nSimilar to Pandas, Polars features a filter method that assists in the process of selectively filtering rows based on one or multiple conditions.\n\nEvaluate a single condition\n\n\nCode\ndf.filter(pl.col(\"model\") == \"Testarossa\")\n\n\n\nshape: (14, 11)\n\n\n\nmake\nmodel\ndrive\ncylinders\ndispl\nfuelType\nmpgData\ncity08\nhighway08\ncomb08\nyear\n\n\ncat\ncat\ncat\ni8\nf32\ncat\nbool\ni8\ni8\ni8\ndate\n\n\n\n\n\"Ferrari\"\n\"Testarossa\"\n\"Rear-Wheel Dri…\n12\n4.9\n\"Regular\"\nfalse\n9\n14\n11\n1985-01-01\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Ferrari\"\n\"Testarossa\"\n\"Rear-Wheel Dri…\n12\n4.9\n\"Premium\"\nfalse\n10\n15\n11\n1992-01-01\n\n\n\n\n\n\n\n\nCombine multiple conditions\n\n\nCode\ndf.filter((pl.col(\"model\") == \"Testarossa\") & (pl.col(\"make\") == \"Ferrari\"))\n\n\n\nshape: (9, 11)\n\n\n\nmake\nmodel\ndrive\ncylinders\ndispl\nfuelType\nmpgData\ncity08\nhighway08\ncomb08\nyear\n\n\ncat\ncat\ncat\ni8\nf32\ncat\nbool\ni8\ni8\ni8\ndate\n\n\n\n\n\"Ferrari\"\n\"Testarossa\"\n\"Rear-Wheel Dri…\n12\n4.9\n\"Regular\"\nfalse\n9\n14\n11\n1985-01-01\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Ferrari\"\n\"Testarossa\"\n\"Rear-Wheel Dri…\n12\n4.9\n\"Premium\"\nfalse\n10\n15\n11\n1992-01-01"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#imitate-pandas-index-using-with_row_count",
    "href": "posts/polars/Polars_revised.html#imitate-pandas-index-using-with_row_count",
    "title": "Getting Started with Polars",
    "section": "Imitate pandas’ index using with_row_count",
    "text": "Imitate pandas’ index using with_row_count\nEven in the absence of a traditional index, you can make use of the with_row_count method in Polars. This clever method comes to the rescue for tasks like indexing and filtering, providing an alternative approach.\n\n\nCode\n(df.with_row_count().filter(pl.col(\"row_nr\") == 5))\n\n\n\nshape: (1, 12)\n\n\n\nrow_nr\nmake\nmodel\ndrive\ncylinders\ndispl\nfuelType\nmpgData\ncity08\nhighway08\ncomb08\nyear\n\n\nu32\ncat\ncat\ncat\ni8\nf32\ncat\nbool\ni8\ni8\ni8\ndate\n\n\n\n\n5\n\"Subaru\"\n\"Loyale\"\n\"Front-Wheel Dr…\n4\n1.8\n\"Regular\"\nfalse\n21\n24\n22\n1993-01-01\n\n\n\n\n\n\n\nUse is_in\nThe is_in function can be employed to verify whether elements of a given expression are present within another Series. We can use this to filter our rows.\n\n\nCode\ndf.filter(pl.col(\"cylinders\").is_in([i for i in range(6, 8)]))\n\n\n\nshape: (14_284, 11)\n\n\n\nmake\nmodel\ndrive\ncylinders\ndispl\nfuelType\nmpgData\ncity08\nhighway08\ncomb08\nyear\n\n\ncat\ncat\ncat\ni8\nf32\ncat\nbool\ni8\ni8\ni8\ndate\n\n\n\n\n\"Audi\"\n\"100\"\n\"Front-Wheel Dr…\n6\n2.8\n\"Premium\"\ntrue\n17\n22\n19\n1993-01-01\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Pontiac\"\n\"Grand Am\"\n\"Front-Wheel Dr…\n6\n3.3\n\"Regular\"\nfalse\n18\n26\n21\n1993-01-01"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#difference-between-select-and-with_columns",
    "href": "posts/polars/Polars_revised.html#difference-between-select-and-with_columns",
    "title": "Getting Started with Polars",
    "section": "Difference between select and with_columns`",
    "text": "Difference between select and with_columns`\nAs evident from the examples below, both select and with_column can be utilized for both column selection and column assignment. However, there’s a crucial distinction between the two. After performing column operations, the select method drops the unselected columns and retains only the columns that underwent operations. Conversely, the with_column method appends the new columns to the dataframe, preserving the original ones.\n\n\nCode\n# everything else is dropped\n\n(\n    df.select(\n        (pl.col(\"city08\") + 10).alias(\"added\"),\n        (pl.col(\"city08\") - 10).alias(\"substracted\"),\n    )\n)\n\n\n\n\nshape: (41_144, 2)\n\n\n\nadded\nsubstracted\n\n\ni8\ni8\n\n\n\n\n29\n9\n\n\n…\n…\n\n\n26\n6\n\n\n\n\n\n\n\nCode\n# columns are kept\n\n(\n    df.with_columns(\n        (pl.col(\"city08\") + 10).alias(\"added\"),\n        (pl.col(\"city08\") - 10).alias(\"substracted\"),\n    )\n)\n\n\n\nshape: (41_144, 13)\n\n\n\nmake\nmodel\ndrive\ncylinders\ndispl\nfuelType\nmpgData\ncity08\nhighway08\ncomb08\nyear\nadded\nsubstracted\n\n\ncat\ncat\ncat\ni8\nf32\ncat\nbool\ni8\ni8\ni8\ndate\ni8\ni8\n\n\n\n\n\"Alfa Romeo\"\n\"Spider Veloce …\n\"Rear-Wheel Dri…\n4\n2.0\n\"Regular\"\ntrue\n19\n25\n21\n1985-01-01\n29\n9\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Subaru\"\n\"Legacy AWD Tur…\n\"4-Wheel or All…\n4\n2.2\n\"Premium\"\nfalse\n16\n21\n18\n1993-01-01\n26\n6\n\n\n\n\n\n\nNow that we’ve clarified this point, let’s proceed to explore the fundamental methods for selecting columns."
  },
  {
    "objectID": "posts/polars/Polars_revised.html#select-all-columns",
    "href": "posts/polars/Polars_revised.html#select-all-columns",
    "title": "Getting Started with Polars",
    "section": "Select all columns",
    "text": "Select all columns\nA particularly handy tool is pl.all(), which provides the current state of the dataframe—similar to pd.assign(foo=lambda df) in Pandas. This can prove useful, particularly when dealing with operations like groupby and aggregation.\n\n\nCode\n# this is analogous to df.select(pl.col(\"*\")), where * represents the wildcard component\ndf.select(pl.all())\n\n\n\nshape: (41_144, 11)\n\n\n\nmake\nmodel\ndrive\ncylinders\ndispl\nfuelType\nmpgData\ncity08\nhighway08\ncomb08\nyear\n\n\ncat\ncat\ncat\ni8\nf32\ncat\nbool\ni8\ni8\ni8\ndate\n\n\n\n\n\"Alfa Romeo\"\n\"Spider Veloce …\n\"Rear-Wheel Dri…\n4\n2.0\n\"Regular\"\ntrue\n19\n25\n21\n1985-01-01\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Subaru\"\n\"Legacy AWD Tur…\n\"4-Wheel or All…\n4\n2.2\n\"Premium\"\nfalse\n16\n21\n18\n1993-01-01"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#select-all-columns-except-for",
    "href": "posts/polars/Polars_revised.html#select-all-columns-except-for",
    "title": "Getting Started with Polars",
    "section": "Select all columns except for…",
    "text": "Select all columns except for…\n\n\nCode\ndf.select(pl.col(\"*\").exclude(\"year\", \"comb08\", \"highway08\"))\n\n\n\nshape: (41_144, 8)\n\n\n\nmake\nmodel\ndrive\ncylinders\ndispl\nfuelType\nmpgData\ncity08\n\n\ncat\ncat\ncat\ni8\nf32\ncat\nbool\ni8\n\n\n\n\n\"Alfa Romeo\"\n\"Spider Veloce …\n\"Rear-Wheel Dri…\n4\n2.0\n\"Regular\"\ntrue\n19\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Subaru\"\n\"Legacy AWD Tur…\n\"4-Wheel or All…\n4\n2.2\n\"Premium\"\nfalse\n16"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#select-a-subset-of-columns",
    "href": "posts/polars/Polars_revised.html#select-a-subset-of-columns",
    "title": "Getting Started with Polars",
    "section": "Select a subset of columns",
    "text": "Select a subset of columns\n\n\nCode\ndf.select(pl.col(\"year\", \"comb08\", \"highway08\"))\n\n\n\n\nshape: (41_144, 3)\n\n\n\nyear\ncomb08\nhighway08\n\n\ndate\ni8\ni8\n\n\n\n\n1985-01-01\n21\n25\n\n\n…\n…\n…\n\n\n1993-01-01\n18\n21"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#select-columns-based-on-regular-expression",
    "href": "posts/polars/Polars_revised.html#select-columns-based-on-regular-expression",
    "title": "Getting Started with Polars",
    "section": "Select columns based on regular expression",
    "text": "Select columns based on regular expression\n\n\nCode\ndf.select(pl.col(\"^.*(mo|ma).*$\"))\n\n\n\n\nshape: (41_144, 2)\n\n\n\nmake\nmodel\n\n\ncat\ncat\n\n\n\n\n\"Alfa Romeo\"\n\"Spider Veloce …\n\n\n…\n…\n\n\n\"Subaru\"\n\"Legacy AWD Tur…"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#select-columns-based-on-dtypes",
    "href": "posts/polars/Polars_revised.html#select-columns-based-on-dtypes",
    "title": "Getting Started with Polars",
    "section": "Select columns based on dtypes",
    "text": "Select columns based on dtypes\nThis may remind you of pandas’ select_dtypes method.\n\n\nCode\ndf.select(pl.col(pl.Int8, pl.Boolean))\n\n\n\nshape: (41_144, 5)\n\n\n\ncylinders\nmpgData\ncity08\nhighway08\ncomb08\n\n\ni8\nbool\ni8\ni8\ni8\n\n\n\n\n4\ntrue\n19\n25\n21\n\n\n…\n…\n…\n…\n…\n\n\n4\nfalse\n16\n21\n18"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#using-selectors",
    "href": "posts/polars/Polars_revised.html#using-selectors",
    "title": "Getting Started with Polars",
    "section": "Using selectors",
    "text": "Using selectors\nSelectors offer a more intuitive approach to selecting columns from DataFrame or LazyFrame objects, taking into account factors like column names, data types, and other properties. They consolidate and enhance the functionality that’s accessible through the col() expression. More on them here.\n\nBy dtypes\n\n\nCode\ndf.select(cs.integer(), cs.float(), cs.temporal())\n\n\n\nshape: (41_144, 6)\n\n\n\ncylinders\ncity08\nhighway08\ncomb08\ndispl\nyear\n\n\ni8\ni8\ni8\ni8\nf32\ndate\n\n\n\n\n4\n19\n25\n21\n2.0\n1985-01-01\n\n\n…\n…\n…\n…\n…\n…\n\n\n4\n16\n21\n18\n2.2\n1993-01-01\n\n\n\n\n\n\n\n\nCode\n# all columns except for the ones containing float\ndf.select(cs.all() - cs.numeric())\n\n\n\nshape: (41_144, 6)\n\n\n\nmake\nmodel\ndrive\nfuelType\nmpgData\nyear\n\n\ncat\ncat\ncat\ncat\nbool\ndate\n\n\n\n\n\"Alfa Romeo\"\n\"Spider Veloce …\n\"Rear-Wheel Dri…\n\"Regular\"\ntrue\n1985-01-01\n\n\n…\n…\n…\n…\n…\n…\n\n\n\"Subaru\"\n\"Legacy AWD Tur…\n\"4-Wheel or All…\n\"Premium\"\nfalse\n1993-01-01\n\n\n\n\n\n\n\n\nCode\n# same as the one above but it uses the tilde\ndf.select(~cs.numeric())\n\n\n\nshape: (41_144, 6)\n\n\n\nmake\nmodel\ndrive\nfuelType\nmpgData\nyear\n\n\ncat\ncat\ncat\ncat\nbool\ndate\n\n\n\n\n\"Alfa Romeo\"\n\"Spider Veloce …\n\"Rear-Wheel Dri…\n\"Regular\"\ntrue\n1985-01-01\n\n\n…\n…\n…\n…\n…\n…\n\n\n\"Subaru\"\n\"Legacy AWD Tur…\n\"4-Wheel or All…\n\"Premium\"\nfalse\n1993-01-01\n\n\n\n\n\n\n\n\nBy column names\n\n\nCode\ndf.select(cs.contains(\"08\"))\n\n\n\n\nshape: (41_144, 3)\n\n\n\ncity08\nhighway08\ncomb08\n\n\ni8\ni8\ni8\n\n\n\n\n19\n25\n21\n\n\n…\n…\n…\n\n\n16\n21\n18\n\n\n\n\n\n\n\nCode\ndf.select(cs.starts_with(\"d\"))\n\n\n\n\nshape: (41_144, 2)\n\n\n\ndrive\ndispl\n\n\ncat\nf32\n\n\n\n\n\"Rear-Wheel Dri…\n2.0\n\n\n…\n…\n\n\n\"4-Wheel or All…\n2.2\n\n\n\n\n\nThe possibilities here are incredibly vast! I’m pretty sure you’ll find a function that suits your needs just right."
  },
  {
    "objectID": "posts/polars/Polars_revised.html#basic-operations",
    "href": "posts/polars/Polars_revised.html#basic-operations",
    "title": "Getting Started with Polars",
    "section": "Basic operations",
    "text": "Basic operations\n\n\nCode\n(\n    df.select(\n        pl.col(\"make\").cast(pl.Utf8).str.contains(\"Hyundai|Kia\").alias(\"contains\"),\n        pl.col(\"make\").cast(pl.Utf8).str.starts_with(\"Alfa\").alias(\"starts_with\"),\n        pl.col(\"make\").cast(pl.Utf8).str.ends_with(\"ari\").alias(\"ends_with\"),\n    )\n)\n\n\n\nshape: (41_144, 3)\n\n\n\ncontains\nstarts_with\nends_with\n\n\nbool\nbool\nbool\n\n\n\n\nfalse\ntrue\nfalse\n\n\n…\n…\n…\n\n\nfalse\nfalse\nfalse\n\n\n\n\n\n\nWe can extract the numbers from the different car models:\n\n\nCode\n(\n    df.select(\n        pl.col(\"model\")\n        .cast(pl.Utf8)\n        .str.extract(r\"(\\d+)\", group_index=1)\n        .cast(pl.Int32)\n        .alias(\"extracted_number\"),\n    )\n)\n\n\n\n\nshape: (41_144, 1)\n\n\n\nextracted_number\n\n\ni32\n\n\n\n\n2000\n\n\n…\n\n\nnull\n\n\n\n\n\nAs per usual, we can replace values in a given column:\n\n\nCode\n(\n    df.select(\n        pl.col(\"make\").cast(pl.Utf8).str.replace(\"r\", 0).alias(\"replaced\"),\n    )\n)\n\n\n\n\nshape: (41_144, 1)\n\n\n\nreplaced\n\n\nstr\n\n\n\n\n\"Alfa Romeo\"\n\n\n…\n\n\n\"Suba0u\""
  },
  {
    "objectID": "posts/polars/Polars_revised.html#changing-column-names",
    "href": "posts/polars/Polars_revised.html#changing-column-names",
    "title": "Getting Started with Polars",
    "section": "Changing column names",
    "text": "Changing column names\nAltering column names is quite reminiscent of the process in Pandas:\n\n\nCode\ndf.rename({\"make\": \"car maker\", \"model\": \"car model\"})\n\n\n\nshape: (41_144, 11)\n\n\n\ncar maker\ncar model\ndrive\ncylinders\ndispl\nfuelType\nmpgData\ncity08\nhighway08\ncomb08\nyear\n\n\ncat\ncat\ncat\ni8\nf32\ncat\nbool\ni8\ni8\ni8\ndate\n\n\n\n\n\"Alfa Romeo\"\n\"Spider Veloce …\n\"Rear-Wheel Dri…\n4\n2.0\n\"Regular\"\ntrue\n19\n25\n21\n1985-01-01\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Subaru\"\n\"Legacy AWD Tur…\n\"4-Wheel or All…\n4\n2.2\n\"Premium\"\nfalse\n16\n21\n18\n1993-01-01\n\n\n\n\n\n\nIn case you would like to alter multiple column names all at once:\n\n\nCode\ndf.select(pl.all().map_alias(lambda name: name.upper().replace(\"I\", \"0000\")))\n\n\n\nshape: (41_144, 11)\n\n\n\nMAKE\nMODEL\nDR0000VE\nCYL0000NDERS\nD0000SPL\nFUELTYPE\nMPGDATA\nC0000TY08\nH0000GHWAY08\nCOMB08\nYEAR\n\n\ncat\ncat\ncat\ni8\nf32\ncat\nbool\ni8\ni8\ni8\ndate\n\n\n\n\n\"Alfa Romeo\"\n\"Spider Veloce …\n\"Rear-Wheel Dri…\n4\n2.0\n\"Regular\"\ntrue\n19\n25\n21\n1985-01-01\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Subaru\"\n\"Legacy AWD Tur…\n\"4-Wheel or All…\n4\n2.2\n\"Premium\"\nfalse\n16\n21\n18\n1993-01-01\n\n\n\n\n\n\n\nstr.split with expand='all'\nI often find myself using the str.split method, and it caught me off guard that there isn’t a direct equivalent in Polars. Fingers crossed, we might come across an implementation like expand=True from Pandas, which would be a real game-changer here too!\n\n\nCode\n(\n    df.select(\n        pl.col(\"drive\")\n        .cast(pl.Utf8)\n        .str.split_exact(\"-\", n=1)\n        .struct.rename_fields([\"first_part\", \"second_part\"])\n        .alias(\"fields\"),\n    ).unnest(\"fields\")\n)\n\n\n\n\nshape: (41_144, 2)\n\n\n\nfirst_part\nsecond_part\n\n\nstr\nstr\n\n\n\n\n\"Rear\"\n\"Wheel Drive\"\n\n\n…\n…\n\n\n\"4\"\n\"Wheel or All\""
  },
  {
    "objectID": "posts/polars/Polars_revised.html#a-simple-split-apply-combine-operation",
    "href": "posts/polars/Polars_revised.html#a-simple-split-apply-combine-operation",
    "title": "Getting Started with Polars",
    "section": "A simple split-apply-combine operation",
    "text": "A simple split-apply-combine operation\n\n\nCode\n(df.groupby(\"make\").count().sort(by=\"count\", descending=True).limit(5))\n\n\n\n\nshape: (5, 2)\n\n\n\nmake\ncount\n\n\ncat\nu32\n\n\n\n\n\"Chevrolet\"\n4003\n\n\n…\n…\n\n\n\"Toyota\"\n2071"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#using-agg",
    "href": "posts/polars/Polars_revised.html#using-agg",
    "title": "Getting Started with Polars",
    "section": "Using agg",
    "text": "Using agg\nYou can use agg to calculate statistics over either a single or multiple columns all at once.\n\n\nCode\n(df.groupby(\"make\").agg(pl.count(), pl.col(\"model\")))\n\n\n\nshape: (136, 3)\n\n\n\nmake\ncount\nmodel\n\n\ncat\nu32\nlist[cat]\n\n\n\n\n\"TVR Engineerin…\n4\n[\"TVR 280i/350i Convertible\", \"TVR 280i/350i Coupe\", … \"TVR 280i Coupe\"]\n\n\n…\n…\n…\n\n\n\"Grumman Allied…\n1\n[\"LLV\"]\n\n\n\n\n\n\nHere’s an illustration of how to utilize the is_not_nan method. In Polars, unknown values are represented with floating-point precision, similar to how pandas handles them. It’s important not to mix this up with missing values, which are indicated by Null in Polars. Additionally, take note of the use of limit instead of head, a concept quite akin to SQL.\n\n\nCode\n(\n    df.groupby(\"make\")\n    .agg(\n        pl.col(\"city08\").mean().alias(\"mean_cyl\"),\n        pl.col(\"displ\").mean().alias(\"mean_disp\"),\n    )\n    .filter(pl.col(\"mean_cyl\").is_not_nan())\n    .sort(by=[\"mean_cyl\", \"mean_disp\"], descending=[True, True])\n    .limit(5)\n)\n\n\n\nshape: (5, 3)\n\n\n\nmake\nmean_cyl\nmean_disp\n\n\ncat\nf64\nf64\n\n\n\n\n\"Tesla\"\n94.925373\nNaN\n\n\n…\n…\n…\n\n\n\"Azure Dynamics…\n62.0\nNaN\n\n\n\n\n\n\nAggregated columns can be renamed immediately with the alias method.\n\n\nCode\n(\n    df.groupby(\"make\", \"fuelType\")\n    .agg(pl.col(\"comb08\").mean().alias(\"filtered\"))\n    .filter((pl.col(\"fuelType\") == \"CNG\") | (pl.col(\"fuelType\") == \"Diesel\"))\n)\n\n\n\nshape: (38, 3)\n\n\n\nmake\nfuelType\nfiltered\n\n\ncat\ncat\nf64\n\n\n\n\n\"Mazda\"\n\"Diesel\"\n29.714286\n\n\n…\n…\n…\n\n\n\"Ford\"\n\"Diesel\"\n29.542857\n\n\n\n\n\n\nHere’s a scenario for performing aggregation based on the year. In this case, the “year” column holds the year-related data, which can be extracted using the dt.year attribute. Keep in mind that Polars doesn’t have native plotting capabilities like pandas. To visualize the data, as a final step, you can convert the dataframe to pandas and make use of its .plot method.\n\n\nCode\n(\n    df.groupby(\n        [\n            pl.col(\"year\").dt.year().alias(\"year\"),\n        ]\n    )\n    .agg([pl.col(\"comb08\").mean(), pl.col(\"city08\").mean()])\n    .sort(\n        [\n            \"year\",\n        ]\n    )\n    .to_pandas()\n    .set_index(\"year\")\n    .plot()\n)\n\n\n&lt;Axes: xlabel='year'&gt;\n\n\n\n\n\nHere’s another fantastic option for you: the groupby_dynamic feature. It’s really impressive, by the way. You can employ this when grouping based on years or any other time-related information. Additionally, you can make use of the every parameter for resampling, which closely resembles what you can do with pandas.\n\n\nCode\n(\n    df.sort(by=\"year\")\n    .groupby_dynamic(index_column=\"year\", every=\"2y\")\n    .agg([pl.col(\"comb08\").mean(), pl.col(\"city08\").mean()])\n    .sort(\n        [\n            \"year\",\n        ]\n    )\n    .to_pandas()\n    .set_index(\"year\")\n    .plot()\n)\n\n\n&lt;Axes: xlabel='year'&gt;"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#pivot",
    "href": "posts/polars/Polars_revised.html#pivot",
    "title": "Getting Started with Polars",
    "section": "Pivot",
    "text": "Pivot\nYou can also accomplish split-apply-combine tasks seamlessly using the pivot function. It’s remarkably straightforward.\n\n\nCode\n(\n    df.with_columns(\n        pl.col(\"fuelType\").cast(pl.Utf8)\n    )  # conversion to str is needed for the next step\n    .filter(pl.col(\"fuelType\").is_in([\"Regular\", \"Premium\"]))\n    .pivot(values=\"city08\", index=\"make\", columns=\"fuelType\", aggregate_function=\"mean\")\n)\n\n\n\nshape: (126, 3)\n\n\n\nmake\nRegular\nPremium\n\n\ncat\nf64\nf64\n\n\n\n\n\"Alfa Romeo\"\n17.142857\n18.902439\n\n\n…\n…\n…\n\n\n\"PAS Inc - GMC\"\nnull\n14.0"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#getting-missing-value-count-per-column",
    "href": "posts/polars/Polars_revised.html#getting-missing-value-count-per-column",
    "title": "Getting Started with Polars",
    "section": "Getting missing value count per column",
    "text": "Getting missing value count per column\nThe null_count function showcases the count of missing values in the dataframe. Remember, these are not the same as np.nans; it’s important to be aware of this distinction. This functionality is akin to the pandas df.isna().sum() operation if you’re familiar with pandas.\n\n\nCode\ndf.null_count()\n\n\n\nshape: (1, 11)\n\n\n\nmake\nmodel\ndrive\ncylinders\ndispl\nfuelType\nmpgData\ncity08\nhighway08\ncomb08\nyear\n\n\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\n\n\n\n\n0\n0\n1189\n206\n204\n0\n0\n27\n0\n7\n0"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#getting-boolean-array-of-missing-values",
    "href": "posts/polars/Polars_revised.html#getting-boolean-array-of-missing-values",
    "title": "Getting Started with Polars",
    "section": "Getting boolean array of missing values",
    "text": "Getting boolean array of missing values\nIf you ever need to manipulate a series based on its null values, you can also obtain a boolean mask using the is_null function. This can be really handy for targeted data manipulation.\n\n\nCode\ndf.select(pl.col(\"city08\").is_null())\n\n\n\n\nshape: (41_144, 1)\n\n\n\ncity08\n\n\nbool\n\n\n\n\nfalse\n\n\n…\n\n\nfalse"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#filling-missing-values",
    "href": "posts/polars/Polars_revised.html#filling-missing-values",
    "title": "Getting Started with Polars",
    "section": "Filling missing values",
    "text": "Filling missing values\nYou have a variety of options at your disposal for filling in missing values. Here’s a list of some of the most common ones, although it’s not an exhaustive rundown.\n\nWith literals\n\n\nCode\n(df.with_columns(pl.col(\"cylinders\").fill_null(pl.lit(2))))\n\n\n\nshape: (41_144, 11)\n\n\n\nmake\nmodel\ndrive\ncylinders\ndispl\nfuelType\nmpgData\ncity08\nhighway08\ncomb08\nyear\n\n\ncat\ncat\ncat\ni8\nf32\ncat\nbool\ni8\ni8\ni8\ndate\n\n\n\n\n\"Alfa Romeo\"\n\"Spider Veloce …\n\"Rear-Wheel Dri…\n4\n2.0\n\"Regular\"\ntrue\n19\n25\n21\n1985-01-01\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Subaru\"\n\"Legacy AWD Tur…\n\"4-Wheel or All…\n4\n2.2\n\"Premium\"\nfalse\n16\n21\n18\n1993-01-01\n\n\n\n\n\n\n\n\nCode\n(df.with_columns(pl.col(\"cylinders\").fill_null(strategy=\"zero\")))\n\n\n\nshape: (41_144, 11)\n\n\n\nmake\nmodel\ndrive\ncylinders\ndispl\nfuelType\nmpgData\ncity08\nhighway08\ncomb08\nyear\n\n\ncat\ncat\ncat\ni8\nf32\ncat\nbool\ni8\ni8\ni8\ndate\n\n\n\n\n\"Alfa Romeo\"\n\"Spider Veloce …\n\"Rear-Wheel Dri…\n4\n2.0\n\"Regular\"\ntrue\n19\n25\n21\n1985-01-01\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Subaru\"\n\"Legacy AWD Tur…\n\"4-Wheel or All…\n4\n2.2\n\"Premium\"\nfalse\n16\n21\n18\n1993-01-01\n\n\n\n\n\n\n\n\nCode\n(df.with_columns(pl.col(\"cylinders\").fill_null(strategy=\"forward\")))\n\n\n\nshape: (41_144, 11)\n\n\n\nmake\nmodel\ndrive\ncylinders\ndispl\nfuelType\nmpgData\ncity08\nhighway08\ncomb08\nyear\n\n\ncat\ncat\ncat\ni8\nf32\ncat\nbool\ni8\ni8\ni8\ndate\n\n\n\n\n\"Alfa Romeo\"\n\"Spider Veloce …\n\"Rear-Wheel Dri…\n4\n2.0\n\"Regular\"\ntrue\n19\n25\n21\n1985-01-01\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Subaru\"\n\"Legacy AWD Tur…\n\"4-Wheel or All…\n4\n2.2\n\"Premium\"\nfalse\n16\n21\n18\n1993-01-01\n\n\n\n\n\n\n\n\nWith an expression\nThis can be very useful for machine learning pipelines. When addressing missing values, you can set them to the mode, median, mean, or any other value that suits your needs.\n\n\nCode\n(df.with_columns(pl.col(\"cylinders\").fill_null(pl.col(\"cylinders\").mode())))\n\n\n\nshape: (41_144, 11)\n\n\n\nmake\nmodel\ndrive\ncylinders\ndispl\nfuelType\nmpgData\ncity08\nhighway08\ncomb08\nyear\n\n\ncat\ncat\ncat\ni8\nf32\ncat\nbool\ni8\ni8\ni8\ndate\n\n\n\n\n\"Alfa Romeo\"\n\"Spider Veloce …\n\"Rear-Wheel Dri…\n4\n2.0\n\"Regular\"\ntrue\n19\n25\n21\n1985-01-01\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Subaru\"\n\"Legacy AWD Tur…\n\"4-Wheel or All…\n4\n2.2\n\"Premium\"\nfalse\n16\n21\n18\n1993-01-01\n\n\n\n\n\n\n\n\nUsing the over function\nThe over function in Polars is used to perform window operations, akin to what you’d achieve with window functions in SQL or the transform function in pandas. This function enables you to compute aggregations over a specified window or range of rows defined by a window specification. It’s perfect for tasks like calculating rolling averages, cumulative sums, and other operations that involve working with a specific subset of rows within the dataset.\nLet’s begin by filtering the car makes to encompass only the top 5 car brands. Following that, we can utilize the over function to derive several statistics.\n\n\nCode\ntop5 = (\n    df.select(pl.col(\"make\"))\n    .to_series()\n    .value_counts()\n    .sort(by=\"counts\", descending=True)\n    .limit(3)\n    .select(pl.col(\"make\"))\n    .to_series()\n)\n\n(\n    df.filter(pl.col(\"make\").is_in(top5)).select(\n        \"make\",\n        \"model\",\n        pl.col(\"city08\").mean().over(\"make\").alias(\"avg_city_mpg_by_make\"),\n        pl.col(\"city08\").mean().over(\"model\").alias(\"avg_city_mpg_by_model\"),\n        pl.col(\"comb08\").mean().over([\"make\", \"model\"]).alias(\"avg_comb_mpg_by_model\"),\n    )\n)\n\n\n\nshape: (9_957, 5)\n\n\n\nmake\nmodel\navg_city_mpg_by_make\navg_city_mpg_by_model\navg_comb_mpg_by_model\n\n\ncat\ncat\nf64\nf64\nf64\n\n\n\n\n\"Dodge\"\n\"Charger\"\n15.462253\n18.197531\n21.320988\n\n\n…\n…\n…\n…\n…\n\n\n\"Dodge\"\n\"B150/B250 Wago…\n15.462253\n11.654321\n12.925926"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#operating-on-multiple-columns-and-renaming-them",
    "href": "posts/polars/Polars_revised.html#operating-on-multiple-columns-and-renaming-them",
    "title": "Getting Started with Polars",
    "section": "Operating on multiple columns and renaming them",
    "text": "Operating on multiple columns and renaming them\nYou have the flexibility to conduct column operations and then easily rename them. Additionally, you can make use of the prefix and suffix functions to streamline your workflow.\n\n\nCode\n(df.with_columns((cs.numeric() * 2).prefix(\"changed_\")))\n\n\n\nshape: (41_144, 16)\n\n\n\nmake\nmodel\ndrive\ncylinders\ndispl\nfuelType\nmpgData\ncity08\nhighway08\ncomb08\nyear\nchanged_cylinders\nchanged_displ\nchanged_city08\nchanged_highway08\nchanged_comb08\n\n\ncat\ncat\ncat\ni8\nf32\ncat\nbool\ni8\ni8\ni8\ndate\ni8\nf32\ni8\ni8\ni8\n\n\n\n\n\"Alfa Romeo\"\n\"Spider Veloce …\n\"Rear-Wheel Dri…\n4\n2.0\n\"Regular\"\ntrue\n19\n25\n21\n1985-01-01\n8\n4.0\n38\n50\n42\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\"Subaru\"\n\"Legacy AWD Tur…\n\"4-Wheel or All…\n4\n2.2\n\"Premium\"\nfalse\n16\n21\n18\n1993-01-01\n8\n4.4\n32\n42\n36"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#count-unique-values",
    "href": "posts/polars/Polars_revised.html#count-unique-values",
    "title": "Getting Started with Polars",
    "section": "Count unique values",
    "text": "Count unique values\nAbsolutely, you can also count the unique values within a series in Polars, much like you would with the unique function in pandas. This can be really useful for understanding the distribution and diversity of data within a specific column.\n\n\nCode\ndf.select(pl.col(\"make\")).n_unique()\n\n\n136"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#value_counts-to-get-number-of-occurrences-per-item",
    "href": "posts/polars/Polars_revised.html#value_counts-to-get-number-of-occurrences-per-item",
    "title": "Getting Started with Polars",
    "section": "Value_counts to get number of occurrences per item",
    "text": "Value_counts to get number of occurrences per item\n\nOn a single column\n\n\nCode\n(df.select(pl.col(\"make\")).to_series().value_counts(sort=True))\n\n\n\n\nshape: (136, 2)\n\n\n\nmake\ncounts\n\n\ncat\nu32\n\n\n\n\n\"Chevrolet\"\n4003\n\n\n…\n…\n\n\n\"Isis Imports L…\n1\n\n\n\n\n\n\n\nCode\n(df.select(pl.col(\"make\")).groupby(\"make\").count().sort(by=\"count\", descending=True))\n\n\n\n\nshape: (136, 2)\n\n\n\nmake\ncount\n\n\ncat\nu32\n\n\n\n\n\"Chevrolet\"\n4003\n\n\n…\n…\n\n\n\"Panoz Auto-Dev…\n1\n\n\n\n\n\n\n\nOn multiple columns\nI noticed that Polars lacks the capability to perform value counts on multiple columns, unlike pandas’ value_counts function which operates only on series. However, I’ve discovered that combining a groupby operation with a count function can effectively achieve the same outcome for multiple columns. It’s all about finding alternative approaches that get the job done!\n\n\nCode\n(\n    df.select(pl.col(\"make\", \"model\"))\n    .groupby([\"make\", \"model\"])\n    .count()\n    .sort(by=\"count\", descending=True)\n)\n\n\n\n\nshape: (4_127, 3)\n\n\n\nmake\nmodel\ncount\n\n\ncat\ncat\nu32\n\n\n\n\n\"Ford\"\n\"F150 Pickup 2W…\n214\n\n\n…\n…\n…\n\n\n\"Mercedes-Benz\"\n\"400SE\"\n1"
  },
  {
    "objectID": "posts/polars/Polars_revised.html#conditionals-if-else-statements",
    "href": "posts/polars/Polars_revised.html#conditionals-if-else-statements",
    "title": "Getting Started with Polars",
    "section": "Conditionals/ if-else statements",
    "text": "Conditionals/ if-else statements\nIf you’re looking to integrate conditional if-else statements into your Polars chain, you can make use of the when, then, and otherwise functions. However, keep in mind that a series of chained when, then statements should be interpreted as if, elif, … elif, rather than if, if, … if. In other words, the first condition that evaluates to True will be selected. It’s crucial to understand this distinction when crafting your conditions.\n\n\nCode\n(\n    df.select(\n        pl.col(\"make\"),\n        pl.when(pl.col(\"comb08\") &lt; 15)\n        .then(\"bad\")\n        .when(pl.col(\"comb08\") &lt; 30)\n        .then(\"good\")\n        .otherwise(\"very good\")\n        .alias(\"fuel_economy\"),\n    )\n)\n\n\n\n\nshape: (41_144, 2)\n\n\n\nmake\nfuel_economy\n\n\ncat\nstr\n\n\n\n\n\"Alfa Romeo\"\n\"good\"\n\n\n…\n…\n\n\n\"Subaru\"\n\"good\"\n\n\n\n\n\nAlright, folks, that’s it for this introduction to Polars. I know it wasn’t exactly short, but I hope it was informative! I’ll be adding more content to this article in the future, so stay tuned. Until then, happy coding 😎💻🔎"
  },
  {
    "objectID": "posts/refugee/refugee.html",
    "href": "posts/refugee/refugee.html",
    "title": "Tidy Tuesday: Refugees",
    "section": "",
    "text": "Teowodros Hagos\n\n\nWelcome to our blog dedicated to Tidy Tuesday. This week, we venture into the sobering realm of refugee statistics using the {refugees} R package. This tool grants access to the United Nations High Commissioner for Refugees (UNHCR) Refugee Data Finder, providing critical insights into forcibly displaced populations spanning over seven decades. With data from UNHCR, UNRWA, and the Internal Displacement Monitoring Centre, we’ll explore a subset of this information, focusing on population statistics from 2010 to 2022.\nJoin us as we explore the landscape of refugee data.\n\nSetup\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nimport numpy as np\n\nfrom pathlib import Path\nimport json\nfrom urllib.request import urlopen\n\n\nfrom lets_plot import *\nfrom lets_plot.mapping import as_discrete\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\nLetsPlot.setup_html()\n\nimport plotly.io as pio\nimport plotly.express as px\n\npio.templates.default = \"presentation\"\n\n\n\n            \n            \n            \n\n\n\n\nCode\ntry:\n    df = pd.read_csv(\n        \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-08-22/population.csv\"\n    )\nexcept:\n    df = pd.read_csv(\n        Path.cwd().joinpath(\n            \"raw.githubusercontent.com_rfordatascience_tidytuesday_master_data_2023_2023-08-22_population.csv\"\n        )\n    )\n\ndf = (\n    df\n    # .assign(year=lambda df: pd.to_datetime(df.year, format='%Y'))\n)\n\ndf.sample(5)\n\n\n\n\n\n\n\n\n\nyear\ncoo_name\ncoo\ncoo_iso\ncoa_name\ncoa\ncoa_iso\nrefugees\nasylum_seekers\nreturned_refugees\nidps\nreturned_idps\nstateless\nooc\noip\nhst\n\n\n\n\n45147\n2019\nPakistan\nPAK\nPAK\nItaly\nITA\nITA\n20063\n7800\n0\n0\n0\n0\n0\nNaN\nNaN\n\n\n52502\n2020\nSomalia\nSOM\nSOM\nEswatini\nSWA\nSWZ\n101\n318\n0\n0\n0\n0\n0\nNaN\nNaN\n\n\n17234\n2013\nPoland\nPOL\nPOL\nUnited States of America\nUSA\nUSA\n46\n51\n0\n0\n0\n0\n0\nNaN\nNaN\n\n\n20539\n2014\nUzbekistan\nUZB\nUZB\nNetherlands (Kingdom of the)\nNET\nNLD\n170\n5\n0\n0\n0\n0\n0\nNaN\nNaN\n\n\n6894\n2011\nIraq\nIRQ\nIRQ\nNepal\nNEP\nNPL\n5\n0\n0\n0\n0\n0\n0\nNaN\nNaN\n\n\n\n\n\n\n\nHere is the data dictionary:\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nyear\ndouble\nThe year.\n\n\ncoo_name\ncharacter\nCountry of origin name.\n\n\ncoo\ncharacter\nCountry of origin UNHCR code.\n\n\ncoo_iso\ncharacter\nCountry of origin ISO code.\n\n\ncoa_name\ncharacter\nCountry of asylum name.\n\n\ncoa\ncharacter\nCountry of asylum UNHCR code.\n\n\ncoa_iso\ncharacter\nCountry of asylum ISO code.\n\n\nrefugees\ndouble\nThe number of refugees.\n\n\nasylum_seekers\ndouble\nThe number of asylum-seekers.\n\n\nreturned_refugees\ndouble\nThe number of returned refugees.\n\n\nidps\ndouble\nThe number of internally displaced persons.\n\n\nreturned_idps\ndouble\nThe number of returned internally displaced persons.\n\n\nstateless\ndouble\nThe number of stateless persons.\n\n\nooc\ndouble\nThe number of others of concern to UNHCR.\n\n\noip\ndouble\nThe number of other people in need of international protection.\n\n\nhst\ndouble\nThe number of host community members.\n\n\n\nLooking at the data dictionary, here are the questions we will be exploring in this blog post:\n\nHow has the overall refugee count evolved over the years?\nWhat countries experience the highest annual refugee outflows?\nWhat countries receive the most refugees annually?\nWhere do most refugees from the country with the highest numbers go?\nWhich countries have the highest numbers of internally displaced persons?\nWhat countries have the highest stateless populations?\n\nIn this blog, to visualize the topological data, we will be utilizing Plotly’s choropleth_mapbox function as well as scatter_geo. For a quick overview about Plotly check out this website. In addition, our GeoJSON file, which represents simple geographical features along with their non-spatial attributes, is sourced from GitHub curated by Rufus Pollock.\n\n\nCode\ntry:\n    with urlopen(\n        \"https://raw.githubusercontent.com/datasets/geo-boundaries-world-110m/master/countries.geojson\"\n    ) as response:\n        countries = json.load((response))\nexcept:\n    countries = json.load(open(\"countries.geojson\"))\n\n\n\n\nHow has the overall refugee count evolved over the years?\nAs you can see, the number of stateless persons has not changed significantly over the period studied; on the other hand, the number of internally displaced persons has dramatically increased from over 14 million in 2010 to just shy of 60 million by 2022\n\n\nCode\nfig = (\n    df.groupby(\"year\")[[\"refugees\", \"asylum_seekers\", \"stateless\", \"idps\"]]\n    .sum()\n    .reset_index()\n    .melt(id_vars=\"year\")\n    .pipe(\n        lambda df: px.line(\n            df,\n            x=df.year,\n            y=df.value,\n            color=df.variable,\n            width=750,\n            height=600,\n            # markers=True,\n            labels={\n                \"year\": \"Year\",\n                \"value\": \"Number of individuals\",\n            },\n        )\n    )\n)\nfig.update_layout(legend_title_text=\"Categories\")\n\n\n\n\n                                                \nFigure 1: Overall statistics\n\n\n\n\n\nWhat countries experience the highest annual refugee outflows?\nAmong all the countries under examination, the highlighted ones represent the top 5 nations with the highest registered refugee populations. It’s evident that the Syrian Arab Republic consistently maintained the highest number of registered refugees from 2013 to 2022, followed closely by Afghanistan\n\n\nCode\nfig = go.Figure()\n\ntop10_countries = (\n    df.groupby(\"coo_name\")\n    .refugees.sum()\n    .to_frame()\n    .reset_index()\n    .sort_values(by=\"refugees\", ascending=False)\n    .head(5)\n    .coo_name.values\n)\n\ntest = (\n    df.groupby([\"year\", \"coo_name\"])\n    .refugees.sum()\n    .to_frame()\n    .reset_index()\n    .assign(\n        color=lambda df: df.coo_name.mask(\n            df.coo_name == (top10_countries[0]), \"#6eb14a\"\n        )\n        .mask(df.coo_name == (top10_countries[1]), \"#2d4526\")\n        .mask(df.coo_name == (top10_countries[2]), \"#f6d19b\")\n        .mask(df.coo_name == (top10_countries[3]), \"#ca3780\")\n        .mask(df.coo_name == (top10_countries[4]), \"#e9b4cd\")\n        .mask(~df.coo_name.isin(top10_countries), \"#dadadc\")\n    )\n)\n\n# Loop over each unique 'coo_name'\nfor coo_name in test[\"coo_name\"].unique():\n    df_subset = test[test[\"coo_name\"] == coo_name]\n    if coo_name in top10_countries:\n        fig.add_trace(\n            go.Scatter(\n                x=df_subset[\"year\"],\n                y=df_subset[\"refugees\"],\n                mode=\"lines+markers\",\n                name=coo_name,\n                showlegend=True,\n                line=dict(\n                    color=df_subset[\"color\"].iloc[0], dash=\"dash\"\n                ),  # Use the first color from the subset\n            )\n        )\n    else:\n        fig.add_trace(\n            go.Scatter(\n                x=df_subset[\"year\"],\n                y=df_subset[\"refugees\"],\n                mode=\"lines\",\n                name=coo_name,\n                showlegend=False,\n                line=dict(\n                    color=df_subset[\"color\"].iloc[0], dash=\"dot\"\n                ),  # Use the first color from the subset\n            )\n        )\n\n\nfig.update_layout(\n    autosize=False, width=750, height=600, yaxis_title=\"Sum Annual Refugee Outflows\"\n)\nfig.show()\n\n\n\n\n                                                \nFigure 2: Top 5 countries with highest number of sum annual refugee outflows\n\n\n\nI have also prepared a short animation that depicts the year-by-year pattern of refugee outflows.\n\n\nCode\n(\n    df.assign(year=lambda df: df.year.astype(str).str.split(\"-\", expand=True)[0])\n    .groupby([\"year\", \"coo_iso\", \"coo_name\"])[\"refugees\"]\n    .sum()\n    .to_frame()\n    .reset_index()\n    .pipe(\n        lambda df: px.choropleth_mapbox(\n            data_frame=df,\n            locations=\"coo_iso\",\n            geojson=countries,\n            color=\"refugees\",\n            animation_frame=\"year\",\n            animation_group=\"coo_iso\",\n            hover_name=\"coo_name\",\n            hover_data={\n                \"coo_iso\": \"\",\n            },\n            range_color=(0, 8_000_000),\n            color_continuous_scale=px.colors.sequential.Oranges,\n            featureidkey=\"properties.iso_a3\",\n            zoom=1.2,\n            mapbox_style=\"carto-positron\",\n            opacity=0.6,\n            width=1200,\n            height=700\n            # **fig_dict\n        )\n    )\n)\n\n\n\n\n                                                \nFigure 3: Patterns of Annual Refugee Outflows\n\n\n\n\n\nWhat countries receive the most refugees annually?\nAfter examining which countries have the most refugees leaving, we will shift our focus to the countries with the highest number of refugees received.\n\n\nCode\nfig = go.Figure()\n\ntop5_countries_accept = (\n    df.groupby(\"coa_name\")[[\"refugees\"]]\n    .sum()\n    .sort_values(by=\"refugees\", ascending=False)\n    .head(5)\n    .index\n)\n\ntest = (\n    df.groupby([\"year\", \"coa_name\"])\n    .refugees.sum()\n    .to_frame()\n    .reset_index()\n    .assign(\n        color=lambda df: df.coa_name.mask(\n            df.coa_name == (top5_countries_accept[0]), \"#6eb14a\"\n        )\n        .mask(df.coa_name == (top5_countries_accept[1]), \"#2d4526\")\n        .mask(df.coa_name == (top5_countries_accept[2]), \"#f6d19b\")\n        .mask(df.coa_name == (top5_countries_accept[3]), \"#ca3780\")\n        .mask(df.coa_name == (top5_countries_accept[4]), \"#e9b4cd\")\n        .mask(~df.coa_name.isin(top5_countries_accept), \"#dadadc\")\n    )\n)\n\n# Loop over each unique 'coo_name'\nfor coa_name in test[\"coa_name\"].unique():\n    df_subset = test[test[\"coa_name\"] == coa_name]\n    if coa_name in top5_countries_accept:\n        fig.add_trace(\n            go.Scatter(\n                x=df_subset[\"year\"],\n                y=df_subset[\"refugees\"],\n                mode=\"lines+markers\",\n                name=coa_name,\n                showlegend=True,\n                line=dict(\n                    color=df_subset[\"color\"].iloc[0], dash=\"dash\"\n                ),  # Use the first color from the subset\n            )\n        )\n    else:\n        fig.add_trace(\n            go.Scatter(\n                x=df_subset[\"year\"],\n                y=df_subset[\"refugees\"],\n                mode=\"lines\",\n                name=coa_name,\n                showlegend=False,\n                line=dict(\n                    color=df_subset[\"color\"].iloc[0], dash=\"dot\"\n                ),  # Use the first color from the subset\n            )\n        )\n\n\nfig.update_layout(\n    autosize=False, width=750, height=600, yaxis_title=\"Total Annual Refugee Arrivals\"\n)\nfig.show()\n\n\n\n\n                                                \nFigure 4: Top 5 countries with highest number of refugees accepted\n\n\n\n\n\nWhere do most refugees from the country with the highest numbers go?\nNext, we wanted to find out where Syrian refugees have found welcomes for their new homes.\n\n\nCode\nfig = (\n    df.query(\"coo_name == 'Syrian Arab Rep.'\")\n    .groupby(\"coa_name\")\n    .refugees.sum()\n    .pipe(lambda x: x / x.sum())\n    .mul(100)\n    .sort_values(ascending=False)\n    .head(10)\n    .to_frame()\n    .reset_index()\n    .pipe(\n        lambda df: px.bar(\n            df,\n            y=\"coa_name\",\n            x=\"refugees\",\n            labels={\n                \"refugees\": \"Acceptance Rate of Syrian Refugees (%)\",\n                \"coa_name\": \"\",\n            },\n            height=500,\n            width=1000,\n        )\n    )\n)\nfig.update_yaxes(tickangle=0, automargin=True)\nfig\n\n\n\n\n                                                \nFigure 5: Welcoming Destinations for Syrian Refugees\n\n\n\n\n\nWhich countries have the highest numbers of internally displaced persons?\nNext, we’ll explore the count of internally displaced persons (IDPs). An IDP is someone who has had to leave their home or usual residence because of reasons like armed conflict, violence, human rights violations, natural disasters, or humanitarian emergencies. Unlike refugees, IDPs haven’t crossed international borders to find safety; they stay within their own country’s borders\n\n\nCode\nfig = (\n    df.groupby([\"coo_iso\", \"coo_name\"])\n    .idps.sum()\n    .sort_values(ascending=False)\n    .to_frame()\n    .reset_index()\n    .head(50)\n    .pipe(\n        lambda df: px.scatter_geo(\n            df,\n            locations=\"coo_iso\",\n            geojson=countries,\n            hover_name=\"coo_name\",\n            # color='coo_name',\n            featureidkey=\"properties.iso_a3\",\n            size=\"idps\",\n            # opacity = 0.6,\n            width=1200,\n            height=700,\n        )\n    )\n)\nfig\n\n\n\n\n                                                \nFigure 6: Top 50 nations hosting the largest internally displaced populations\n\n\n\n\n\nWhat countries have received the highest stateless populations?\nNext, we focus on countries that have provided refuge to a substantial number of stateless individuals. Stateless persons are individuals who don’t have the legal status of citizenship in any country. They lack the rights and protection typically granted to citizens. Stateless people often encounter significant difficulties in accessing education, healthcare, jobs, and the freedom to travel.\n\n\nCode\nfig = (\n    df.groupby([\"coa_iso\", \"coa_name\"])\n    .stateless.sum()\n    .pipe(lambda x: x / 49036122)\n    .sort_values(ascending=False)\n    .mul(100)\n    .to_frame()\n    .reset_index()\n    .head(10)\n    .pipe(\n        lambda df: px.scatter(\n            df,\n            y=\"coa_name\",\n            x=\"stateless\",\n            labels={\n                \"stateless\": \"Acceptance Rate of Stateless Populations (%)\",\n                \"coa_name\": \"\",\n            },\n            height=500,\n            width=800,\n        )\n    )\n)\nfig.update_yaxes(tickangle=0, automargin=True)\nfig.update_layout(xaxis_range=[0, 100])\n\nfig\n\n\n\n\n                                                \nFigure 7: Top 10 nations sheltering the largest stateless populations\n\n\n\nAnd there you have it, everyone. That wraps up this week’s data exploration. I’m cant’t wait for what Tidy Tuesday has in store for us next week. Until then, take good care of yourselves, and I’ll catch you soon!"
  },
  {
    "objectID": "posts/speed_up_py/speed_up_py.html",
    "href": "posts/speed_up_py/speed_up_py.html",
    "title": "Speed up your Python code 60x",
    "section": "",
    "text": "Photo by NASA on UnSplash\nEver felt the need to turbo boost your Python code? Tired of the seemingly endless wait for your iterations or simulations to complete? Allow me to present the rustimport library. In its most basic form, rustimport empowers you to call highly optimized Rust functions from within your code. This way, you can delegate the heavy-duty tasks to Rust, instead of relying on Python, a language known for its versatility but also for its slower speed. With rustimport, you can enjoy the best of both worlds - the simplicity of Python and the performance of Rust.\nLet me give you an example: imagine you’re tackling one of the problems from Project Euler, specifically problem #50. Here’s what the problem statement looks like:\nWhile there are many ways to solve this problem, one approach (though not the fastest) is as follows:\nLet’s dive in the code quickly:\nNote: To evaluate the execution time and identify which functions take longer to run, we use cProfile. The statistics generated are then formatted using the pstats module. The complete code for all the examples can be found here.\nAs you can see, this straightforward task required over 5.5 seconds to execute. The primary function, PE50_v1.py:36(main), was invoked once and took a total of 5.517 seconds. The majority of this time was spent calling the function PE50_v1.py:17(generate_primes), which itself spent most of its time calling PE50_v1.py:7(is_prime) over a million times. The function PE50_v1.py:21(longest_sum_of_consecutive_primes) also consumed a significant amount of time. Additionally, the built-in functions sum and len were called multiple times."
  },
  {
    "objectID": "posts/speed_up_py/speed_up_py.html#replacing-the-is_prime-function",
    "href": "posts/speed_up_py/speed_up_py.html#replacing-the-is_prime-function",
    "title": "Speed up your Python code 60x",
    "section": "Replacing the is_prime function",
    "text": "Replacing the is_prime function\nFrom the profiling results, it’s clear that is_prime is a prime candidate (pun intended) for replacement with a Rust equivalent. Let’s explore how we can achieve this.\nFirstly, we need to install the rustimport library. This can be done using pip: pip install rustimport. Following this, we can create a Rust file, which we’ll name rs_extension.rs for this example. Now, let’s write some Rust code.\n// rustimport:pyo3\n\nuse pyo3::prelude::*;\n\n#[pyfunction]\nfn is_prime(n: u32) -&gt; bool {\n    if n &lt;= 1 {\n        return false;\n    }\n    let mut i = 2;\n    while i * i &lt;= n {\n        if n % i == 0 {\n            return false;\n        }\n        i += 1;\n    }\n    true\n}\nUpon comparing the Python is_prime function with its Rust counterpart, you’ll notice some similarities. The key difference comes from Rust’s statically and strongly typed system, which requires us to specify the type of the input variables and return types. Not to worry, it’s simple; we anticipate an unsigned integer (positive integers only) and return a boolean value indicating whether or not the integer is prime. Let’s see what kind of speed up we get.\n\n\n\n\n\n\nWarning\n\n\n\nIf your Rust code isn’t performing as expected, it could be because it was compiled in debug mode, which doesn’t enable optimizations. However, by running the python -m rustimport build --release command, you can generate Rust code that is highly optimized and significantly faster. Give it a try!`.\n\n\n         1157601 function calls in 0.641 seconds\n\n   Ordered by: cumulative time\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n        1    0.000    0.000    0.641    0.641 PE50_v1.py:30(main)\n        1    0.197    0.197    0.391    0.391 PE50_v1.py:14(longest_sum_of_consecutive_primes)\n        1    0.000    0.000    0.249    0.249 PE50_v1.py:9(generate_primes)\n        1    0.089    0.089    0.249    0.249 PE50_v1.py:10(&lt;listcomp&gt;)\n    78526    0.191    0.000    0.191    0.000 {built-in method builtins.sum}\n  1000570    0.160    0.000    0.160    0.000 {built-in method rs_extension.is_prime}\n    78499    0.004    0.000    0.004    0.000 {built-in method builtins.len}\n        1    0.000    0.000    0.000    0.000 {built-in method builtins.print}\n        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n🤯🤯🤯🤯 Our execution time went from 5.517 seconds to 0.641 seconds 🚀. That’s a whopping 8.6 times speed up, achieved by replacing just one function. The is_prime function now takes a total of 0.16 seconds to execute. But why stop here? We can push the boundaries even further.\nConsider the longest_sum_of_consecutive_primes function. It performs a substantial amount of work, and if we could harness the power of Rust for this function, we could potentially slash the execution time even more. Let´s give it a try."
  },
  {
    "objectID": "posts/speed_up_py/speed_up_py.html#replacing-the-longest_sum_of_consecutive_primes-function",
    "href": "posts/speed_up_py/speed_up_py.html#replacing-the-longest_sum_of_consecutive_primes-function",
    "title": "Speed up your Python code 60x",
    "section": "Replacing the longest_sum_of_consecutive_primes function",
    "text": "Replacing the longest_sum_of_consecutive_primes function\n#[pyfunction]\nfn longest_sum_of_consecutive_primes(primes: Vec&lt;u32&gt;, target: u32) -&gt; (u32, usize) {\n    let mut max_length: usize = 0;\n    let mut max_prime: u32 = 0;\n\n    for i in 0..primes.len() {\n        for j in i + max_length..primes.len() {\n            let sum_of_primes: u32 = primes[i..j].iter().sum();\n            if sum_of_primes &gt; target {\n                break;\n            }\n            if is_prime(sum_of_primes) && (j - i) &gt; max_length {\n                max_length = j - i;\n                max_prime = sum_of_primes;\n            }\n        }\n    }\n    (max_prime, max_length)\n}\nNow this Rust function looks quite a bit different from our original Python function, but it’s still possible to understand what’s going on even if you haven’t seen Rust code before. In this case, we’re supplying the function with a vector of prime numbers, each represented as u32 values, and a target number, which is 1000 in our case. The function then returns the solution to our problem: the number (max_prime) that can be expressed as the sum of the greatest number of consecutive primes, and the actual count of these consecutive primes (max-length). This is the result from our second optimization:\n         1000005 function calls in 0.252 seconds\n\n   Ordered by: cumulative time\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n        1    0.000    0.000    0.252    0.252 PE50_v2.py:14(main)\n        1    0.000    0.000    0.248    0.248 PE50_v2.py:9(generate_primes)\n        1    0.088    0.088    0.248    0.248 PE50_v2.py:10(&lt;listcomp&gt;)\n   999999    0.160    0.000    0.160    0.000 {built-in method rs_extension.is_prime}\n        1    0.004    0.004    0.004    0.004 {built-in method rs_extension.longest_sum_of_consecutive_primes}\n        1    0.000    0.000    0.000    0.000 {built-in method builtins.print}\n        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n🤯🤯🤯🤯 Our execution time has now been reduced to just 0.252 seconds, a further improvement of 2.5 times. In total, we’ve achieved a staggering 21.5 times speedup compared to our original Python implementation. The longest_sum_of_consecutive_primes function, which previously took 0.391 seconds to execute, now completes in a mere 0.004 seconds. This is a significant achievement!\nAs you might have guessed, we’re not stopping here 😈. We still have one more Python function left to convert to Rust 🦀. Let’s do that and see how much more we can optimize our code!"
  },
  {
    "objectID": "posts/speed_up_py/speed_up_py.html#replacing-the-generate_primes-function",
    "href": "posts/speed_up_py/speed_up_py.html#replacing-the-generate_primes-function",
    "title": "Speed up your Python code 60x",
    "section": "Replacing the generate_primes function",
    "text": "Replacing the generate_primes function\n#[pyfunction]\nfn generate_primes(target: u32) -&gt; Vec&lt;u32&gt; {\n    (2..=target).filter(|&n| is_prime(n)).collect::&lt;Vec&lt;u32&gt;&gt;()\n}\nThis Rust code, while seemingly simple, is quite powerful. It generates, or more accurately, filters out prime numbers up to a specified range. Given that Rust is heavily inspired by OCaml and Haskell, you may notice the influence of functional programming paradigms throughout the language (and in this example).\n         5 function calls in 0.092 seconds\n\n   Ordered by: cumulative time\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n        1    0.000    0.000    0.092    0.092 PE50_v3.py:9(main)\n        1    0.088    0.088    0.088    0.088 {built-in method rs_extension.generate_primes}\n        1    0.004    0.004    0.004    0.004 {built-in method rs_extension.longest_sum_of_consecutive_primes}\n        1    0.000    0.000    0.000    0.000 {built-in method builtins.print}\n        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n\n😲😵‍💫🤯💥💥💥 And there you have it! 🎉 Rust did it again. The execution time has been slashed from 0.252 seconds to a mere 0.092 seconds, making it 2.7 times faster. The generate_primes function, which previously took 0.248 seconds, now completes in just 0.088 seconds. And thus our code went from taking 5.517 sec to just 0.092 sec…\nHere is our final iteration:\nimport cProfile\nimport pstats\nimport io\n\n# import rustimport.import_hook\nimport rs_extension\n\n\ndef main():\n    TARGET = 1_000_000\n    primes = rs_extension.generate_primes(TARGET)\n    max_prime, max_length = rs_extension.longest_sum_of_consecutive_primes(\n        primes, TARGET\n    )\n    print(f\"Prime: {max_prime}, Length: {max_length}\")\n\n\nCode\nimport pandas as pd\nimport plotly.express as px\n\ndf = pd.DataFrame(\n    {\n        \"step\": [\n            \"original\",\n            \"rewrite is_prime\",\n            \"rewrite longest_sum_of_consecutive_primes\",\n            \"rewrite generate_primes\",\n        ],\n        \"execution_time\": [5.517, 0.641, 0.252, 0.092],\n    }\n)\nfig = px.bar(\n    df,\n    x=\"execution_time\",\n    y=\"step\",\n    color=\"step\",\n    title=\"Turbocharging Performance: A 60x Speed Boost with Rust!\",\n    orientation=\"h\",\n    template=\"plotly_white\",\n)\nfig.update_yaxes(title=\"\", visible=True)\nfig.update_xaxes(title=\"Execution time (s)\")\nfig.update_layout(showlegend=False)\n\n\nfig.show()\n\n\n\n                                                \n\n\nAnd that’s a wrap, everyone! In this post, we’ve showed you how to use cProfile to profile your Python code, helping you pinpoint potential bottlenecks. We’ve also showcased the power of the rustimport library, which enables you to effortlessly invoke highly optimized Rust code straight from your Python script.\nHave you had the chance to use these tools in your projects? Do you believe your work could benefit from Rust? I’d love to hear your thoughts. Until next time, happy coding!"
  },
  {
    "objectID": "projects/CP_Seeker_Post_Processing_App/CP_Seeker_Post_Processing_App.html",
    "href": "projects/CP_Seeker_Post_Processing_App/CP_Seeker_Post_Processing_App.html",
    "title": "Streamlining Data Wrangling with the CP-Seeker Post-Processing App",
    "section": "",
    "text": "Photo by National Cancer Institute on UnSplash\n\n\nWelcome to the CP-Seeker Post-Processing App! This application is designed to streamline the organization of CP-Seeker outputs, simplifying additional quantitation steps required for further pipeline processes. CP-Seeker, a novel custom-built software developed by Ronan Cariou and team, is based on the R-programming language. Its purpose is to seamlessly conduct peak integration and quantification of chlorinated paraffins from high-resolution LC-MS data.\n\n\n\n\n\n\nNote\n\n\n\nYou can access the project’s app through its Streamlit website.\n\n\n\nIntroduction\nThe environmental analysis of chlorinated paraffins (CPs) poses significant challenges due to their intricate composition, varying congener patterns in the environment, and the absence of suitable reference standards. The presence of numerous isomers, enantiomers, and diastereomers within CP mixtures represents a considerable hurdle.\nIntroducing CP-Seeker, a custom-built, user-friendly, and automated software dedicated to peak integration of CPs and related chemical families. Developed at LABERCA, a joint research unit of ONIRIS and INRAE in Nantes, France. CP-Seeker is an open-source R software built using the Shiny framework.\nThe CP-Seeker post-processing app automates the data-wrangling once the data, including confidence levels and peak areas, is obtained from CP-Seeker. Manual processes often involve repetitive copy-pasting of data, introducing potential errors and tediousness. As depicted in Figure 1, CP-Seeker’s output poses challenges as injected samples are spread across separate excel sheets. Moreover, each sample encompasses two dataframes on the same tab: - one containing confidence scores assigned by CP-Seeker to detected congeners, - and the other holding the actual peak areas.\n\n\n\nFigure 1: Illustrative Application of CP-Seeker Post-Processing App\n\n\nThe CP-Seeker post-processing app swiftly imports all datasets from various tabs and restructures the dataframes, akin to the pandas melt function. This transformation organizes congeners neatly, stacking them one beneath the other, rather than grouping them separately by carbon number and chlorine content. Consequently, all samples are presented side by side, eliminating the need for different tabs. Additionally, the app offers the ability to filter congeners and peak areas based on confidence intervals. In other words, congeners assigned lower confidence scores can be automatically excluded from further processing. Once the automated data wrangling is complete, the data is ready for further analysis using software/workflow of your choice.\nWe believe the CP-Seeker Post-Processing App significantly cuts down data preparation time for subsequent analysis stages, thereby enhancing productivity and removing potential errors associated with manual data copying and pasting.\n\n\nHow to use the App\n\nUpload your CP-Seeker output files here.\nOptionally adjust the confidence level, which is set to 80% by default.\nDownload the filtered and merged dataset.\n\n\n\nGet in touch\nDid the app help with your research? Any ideas for making it better? Get in touch! I would love to hear from you."
  },
  {
    "objectID": "projects/Predicting Belgian Real Estate Prices_ Part 1_Feature Selection for Web Scraping/NB_1_ACs_Select_features_for_scraping.html",
    "href": "projects/Predicting Belgian Real Estate Prices_ Part 1_Feature Selection for Web Scraping/NB_1_ACs_Select_features_for_scraping.html",
    "title": "Predicting Belgian Real Estate Prices: Part 1: Feature Selection for Web Scraping",
    "section": "",
    "text": "Photo by Stephen Phillips - Hostreviews.co.uk on UnSplash\n\n\nWelcome to our project focusing on understanding the key factors that impact real estate property prices in Belgium. Our ultimate goal is to leverage data collected from immoweb.be, a prominent real estate platform in the country, to predict house prices in Belgium.\n\n\n\n\n\n\nNote\n\n\n\nYou can access the project’s app through its Streamlit website.\n\n\nThe app is divided into three sections:\n\nIntro: This section provides a basic overview of how the project is structured, how data is handled, and how the models are trained.\nExplore Data: In this section, you can explore the data interactively using boxplots and scatter plots. It allows you to visualize how specific variables impact property prices.\nMake Predictions: On the “Make Predictions” page, you can input certain variables and generate your own predictions based on the latest trained model.\n\nIt’s worth noting that we maintain the app’s accuracy by regularly updating the data through GitHub Actions, which scrapes and retrains the model every month. To test your skills against my base test RMSE score, you can download and use the dataset I uploaded to my Kaggle account through Kaggle Datasets.\nI’m excited to see what you can come up with using this tool. Feel free to explore and experiment with the app, and don’t hesitate to ask if you have any questions or need assistance with anything related to it.\nIn a series of blog posts, we will guide you through the thought process that led to the creation of the Streamlit application. Feel free to explore the topics that pique your interest or that you’d like to learn more about. We hope you’ll find this information valuable for your own projects. Let’s get started!\nNote: Although the data collection process is not described in detail here, you can find the complete workflow in the src/main.py file, specifically focusing on the relevant functions and methods in src/data/make_dataset.py. Feel free to explore it further. In summary, we utilized the request_html library to scrape all available data, which we will show you how to process in subsequent notebooks.\n\n\n\n\n\n\nHow to import your own module using a .pth file\n\n\n\nIn case you encounter difficulties importing your own modules, I found this Stack Overflow question to be quite helpful. To resolve this issue, you can follow these steps:\n\nCreate a .pth file that contains the path to the folder where your module is located. For example, prepare a .pth file with the content: C:\\Users\\myname\\house_price_prediction\\src.\nPlace this .pth file into the following folder: C:\\Users\\myname\\AppData\\Roaming\\Python\\Python310\\site-packages. This folder is already included in your PYTHONPATH, allowing Python to recognize your package directory.\nTo verify what folders are in your PYTHONPATH, you can check it using the import sys and sys.path commands.\n\nOnce you’ve completed these steps, you’ll be able to import the utils module with the following statement: `from data importach out.\n\n\n\nImport data\n\n\nCode\nimport time\nfrom pathlib import Path\n\nimport pandas as pd\nfrom data import utils\nfrom lets_plot import *\nfrom lets_plot.mapping import as_discrete\n\nLetsPlot.setup_html()\n\n\n\n            \n            \n            \n\n\n\n\nSelect Columns to Retain Based on the Quantity of Missing Values\nIn the realm of web scraping, managing the sheer volume of data is often the initial hurdle to conquer. It’s not so much about deciding what data to collect but rather what data to retain. As we delve into the data collected from the Imoweb website, we are met with a plethora of listings, each offering a unique set of information.\nFor many of these listings, there are commonalities – details like location and price tend to be constants. However, interspersed among them are those one-of-a-kind nuggets of information, such as the number of swimming pools available that obviously will be unique to certain listings. While these specific details can certainly be vital in assessing the value of certain listings, the downside is that they can lead to a sparse dataset.\nNow, let’s import our initial dataset to examine the features that are commonly shared among most ads, i.e., those that are filled in most frequently. After identifying these common attributes, we can optimize our data collection process by keeping these key characteristics and removing the less common ones.\n\n\nCode\ndf = pd.read_parquet(\n    utils.Configuration.RAW_DATA_PATH.joinpath(\n        \"complete_dataset_2023-09-27_for_NB2.parquet.gzip\"\n    )\n)\n\n\nAs depicted in Figure 1, the features ‘day of retrieval,’ ‘url,’ and ‘Energy Class’ demonstrate the highest completeness, with more than 90% of instances being present. In contrast, ‘dining room,’ ‘office,’ and ‘TV cable’ are among the least populated features, with roughly 10-20% of non-missing instances.\nThis information allows us to devise a strategy where we, for example, could retain features with a completeness of over 50%. We will delve deeper into this question in our subsequent notebooks.\n\n\nCode\n# Getting the column names with lowest missing values\nlowest_missing_value_columns = (\n    df.notna()\n    .sum()\n    .div(df.shape[0])\n    .mul(100)\n    .sort_values(ascending=False)\n    .head(50)\n    .round(1)\n)\nindexes_to_keep = lowest_missing_value_columns.index\n\n(\n    lowest_missing_value_columns.reset_index()\n    .rename(columns={\"index\": \"column\", 0: \"perc_values_present\"})\n    .assign(\n        Has_non_missing_values_above_50_pct=lambda df: df.perc_values_present.gt(50),\n        perc_values_present=lambda df: df.perc_values_present - 50,\n    )\n    .pipe(\n        lambda df: ggplot(\n            df,\n            aes(\n                \"perc_values_present\",\n                \"column\",\n                fill=\"Has_non_missing_values_above_50_pct\",\n            ),\n        )\n        + geom_bar(stat=\"identity\", orientation=\"y\", show_legend=False)\n        + ggsize(800, 1000)\n        + labs(\n            title=\"Top 50 Features with Non-Missing Values Above 50%\",\n            subtitle=\"\"\"The plot illustrates that the features 'day of retrieval,' 'url,' and 'Energy Class' exhibited the \n            highest completeness, with over 90% of instances present. Conversely, 'dining room','office,' and 'TV cable' \n            were among the least populated features, with approximately 10-20% of non-missing instances.\n            \"\"\",\n            x=\"Percentage of Instances Present with Reference Point at 50%\",\n            y=\"\",\n            caption=\"https://www.immoweb.be/\",\n        )\n        + theme(\n            plot_subtitle=element_text(\n                size=12, face=\"italic\"\n            ),  # Customize subtitle appearance\n            plot_title=element_text(size=15, face=\"bold\"),  # Customize title appearance\n        )\n    )\n)\n\n\n\n   \n   \nFigure 1: Top 50 Features with Non-Missing Values Above 50%\n\n\n\nThat’s all for now. In part 2, we will examine the downloaded raw data and investigate the error messages we encountered during the web scraping process with the goal of understanding how to overcome these challenges. See you in the next installment!"
  },
  {
    "objectID": "projects/Predicting Belgian Real Estate Prices_ Part 3_Characterizing_the_data/NB_3_ACs_Characterizing_the_data.html",
    "href": "projects/Predicting Belgian Real Estate Prices_ Part 3_Characterizing_the_data/NB_3_ACs_Characterizing_the_data.html",
    "title": "Predicting Belgian Real Estate Prices: Part 3: Characterizing Belgian Real Estate Data Post-Scraping",
    "section": "",
    "text": "Photo by Stephen Phillips - Hostreviews.co.uk on UnSplash\nIn the second part, we delved into the intricacies of data processing necessary after scraping. Our discussion covered the treatment of numerical data, handling of categorical variables, and management of boolean values. Furthermore, we evaluated the data quality by scrutinizing the error log produced by the Immowebscraper class. In the upcoming Part 3, our focus will shift to getting a fundamental overview of our data by characterizing the cleaned scraped data. Additionally, we aim to assess feature cardinality, scrutinize distributions, and explore potential correlations between features and our target variable—property price."
  },
  {
    "objectID": "projects/Predicting Belgian Real Estate Prices_ Part 3_Characterizing_the_data/NB_3_ACs_Characterizing_the_data.html#distribution-of-features",
    "href": "projects/Predicting Belgian Real Estate Prices_ Part 3_Characterizing_the_data/NB_3_ACs_Characterizing_the_data.html#distribution-of-features",
    "title": "Predicting Belgian Real Estate Prices: Part 3: Characterizing Belgian Real Estate Data Post-Scraping",
    "section": "Distribution of features",
    "text": "Distribution of features\nNext, our focus will be on identifying low and high cardinality features. Subsequently, we will investigate how house prices vary when grouped according to these variables, using boxplots. Please take into account that the price values have undergone log transformation to address skewness.\n\n\nCode\nlow_cardinality_features = (\n    pd.DataFrame(number_unique_entries)\n    .query(\"unique_values_pct &lt;= 5\")\n    .column_name.to_list()\n)\n\n\n\n\nCode\nhigh_cardinality_features = (\n    pd.DataFrame(number_unique_entries)\n    .query(\"(unique_values_pct &gt;= 5)\")\n    .loc[lambda df: (df.column_dtype == \"float32\") | (df.column_dtype == \"float64\"), :]\n    .column_name.to_list()\n)\n\n\n\n\nCode\nplots = []\n\nfor idx, feature in enumerate(low_cardinality_features):\n    plot = (\n        df.melt(id_vars=[\"ad_url\", \"price\"])\n        .loc[lambda df: df.variable == feature, :]\n        .assign(price=lambda df: np.log10(df.price))\n        .pipe(\n            lambda df: ggplot(\n                df,\n                aes(as_discrete(\"value\"), \"price\"),\n            )\n            + facet_wrap(\"variable\")\n            + geom_boxplot(\n                show_legend=False,\n            )\n        )\n    )\n    plots.append(plot)\ngggrid(plots, ncol=4) + ggsize(900, 1600)\n\n\n\n   \n   \nFigure 3: Exploring Price Variations Across Different Variables"
  },
  {
    "objectID": "projects/Predicting Belgian Real Estate Prices_ Part 3_Characterizing_the_data/NB_3_ACs_Characterizing_the_data.html#distribution-of-target",
    "href": "projects/Predicting Belgian Real Estate Prices_ Part 3_Characterizing_the_data/NB_3_ACs_Characterizing_the_data.html#distribution-of-target",
    "title": "Predicting Belgian Real Estate Prices: Part 3: Characterizing Belgian Real Estate Data Post-Scraping",
    "section": "Distribution of target",
    "text": "Distribution of target\nUpon examining the distribution of our target variable, which is the price, it becomes evident that there is a notable skew. Our median price stands at 379,000 EUR, with the lowest at 350,000 EUR and the highest reaching 10 million EUR. To increase the accuracy of our predictions, it is worth considering a transformation of our target variable before proceeding with modeling. This transformation serves several beneficial purposes:\n\nNormalization: It has the potential to render the distribution of the target variable more symmetrical, resembling a normal distribution. Such a transformation can significantly enhance the performance of various regression models.\nEqualizing Variance: By stabilizing the variance of the target variable across different price ranges, this transformation becomes particularly valuable for ensuring the effectiveness of certain regression algorithms.\nMitigating Outliers: It is effective at diminishing the impact of extreme outliers, bolstering the model’s robustness against data anomalies.\nInterpretability: Notably, when interpreting model predictions, this transformation allows for straightforward back-transformation to the original scale. This can be achieved using a base 10 exponentiation, ensuring that predictions are easily interpretable in their origination task.\n\n\n\nCode\nbefore_transformation = df.pipe(\n    lambda df: ggplot(df, aes(\"price\")) + geom_histogram()\n) + labs(\n    title=\"Before Transformation\",\n)\nafter_transformation = df.assign(price=lambda df: np.log10(df.price)).pipe(\n    lambda df: ggplot(df, aes(\"price\"))\n    + geom_histogram()\n    + labs(\n        title=\"After log10 Transformation\",\n    )\n)\ngggrid([before_transformation, after_transformation], ncol=2) + ggsize(800, 500)\n\n\n\n   \n   \nFigure 4: Target distribution before and after log10 transformation"
  },
  {
    "objectID": "projects/Predicting Belgian Real Estate Prices_ Part 5_Initial_feature_selection/NB_5_ACs_Initial_feature_selection.html",
    "href": "projects/Predicting Belgian Real Estate Prices_ Part 5_Initial_feature_selection/NB_5_ACs_Initial_feature_selection.html",
    "title": "Predicting Belgian Real Estate Prices: Part 5: Initial feature selection",
    "section": "",
    "text": "Photo by Stephen Phillips - Hostreviews.co.uk on UnSplash\nIn Part 4, we looked into some crucial sample pre-processing steps before modeling, establishing the required pipeline for data processing, evaluating various algorithms, and ultimately identifying an appropriate baseline model, that is CatBoost. As we proceed to Part 5, our focus will be on assessing the significance of features in the initial scraped dataset. We’ll achieve this by employing the feature_importances_ method of CatBoostRegressor and analyzing SHAP values. Additionally, we’ll systematically eliminate features showing lower importance or predictive capability. Excited to delve into this phase!"
  },
  {
    "objectID": "projects/Predicting Belgian Real Estate Prices_ Part 5_Initial_feature_selection/NB_5_ACs_Initial_feature_selection.html#read-in-dataframe",
    "href": "projects/Predicting Belgian Real Estate Prices_ Part 5_Initial_feature_selection/NB_5_ACs_Initial_feature_selection.html#read-in-dataframe",
    "title": "Predicting Belgian Real Estate Prices: Part 5: Initial feature selection",
    "section": "Read in dataframe",
    "text": "Read in dataframe\n\n\nCode\ndf = pd.read_parquet(\n    utils.Configuration.INTERIM_DATA_PATH.joinpath(\n        \"2023-10-01_Processed_dataset_for_NB_use.parquet.gzip\"\n    )\n)"
  },
  {
    "objectID": "projects/Predicting Belgian Real Estate Prices_ Part 5_Initial_feature_selection/NB_5_ACs_Initial_feature_selection.html#train-test-split",
    "href": "projects/Predicting Belgian Real Estate Prices_ Part 5_Initial_feature_selection/NB_5_ACs_Initial_feature_selection.html#train-test-split",
    "title": "Predicting Belgian Real Estate Prices: Part 5: Initial feature selection",
    "section": "Train-test split",
    "text": "Train-test split\nOur initial step involves executing a train-test split. This process divides the data into two distinct sets: a training set and a testing set. The training set is employed for model training, while the testing set is exclusively reserved for model evaluation. This methodology allows models to be trained on the training set and then assessed for accuracy using the unseen testing set. Ultimately, this approach enables an unbiased evaluation of our model’s performance, utilizing the test set that remained untouched during the model training phase.\n\n\nCode\ntrain, test = model_selection.train_test_split(\n    df, test_size=0.2, random_state=utils.Configuration.seed\n)\n\nprint(f\"Shape of train: {train.shape}\")\nprint(f\"Shape of test: {test.shape}\")\n\n\nShape of train: (2928, 56)\nShape of test: (732, 56)"
  },
  {
    "objectID": "projects/Predicting Belgian Real Estate Prices_ Part 5_Initial_feature_selection/NB_5_ACs_Initial_feature_selection.html#preprocess-dataframe-for-modelling",
    "href": "projects/Predicting Belgian Real Estate Prices_ Part 5_Initial_feature_selection/NB_5_ACs_Initial_feature_selection.html#preprocess-dataframe-for-modelling",
    "title": "Predicting Belgian Real Estate Prices: Part 5: Initial feature selection",
    "section": "Preprocess dataframe for modelling",
    "text": "Preprocess dataframe for modelling\nOur first step entails removing features that lack informative value for our model, including ‘external_reference’, ‘ad_url’, ‘day_of_retrieval’, ‘website’, ‘reference_number_of_the_epc_report’, and ‘housenumber’. After this, we’ll proceed to apply a log transformation to our target variable. Furthermore, we’ll address missing values in categorical features by replacing them with the label “missing value.” This step is crucial as CatBoost can handle missing values in numerical columns, but for categorical missing values, user intervention is needed.\n\n\nCode\nprocessed_train = (\n    train.reset_index(drop=True)\n    .assign(price=lambda df: np.log10(df.price))  # Log transformation of 'price' column\n    .drop(columns=utils.Configuration.features_to_drop)\n)\n\n# This step is needed since catboost cannot handle missing values when feature is categorical\nfor col in processed_train.columns:\n    if processed_train[col].dtype.name in (\"bool\", \"object\", \"category\"):\n        processed_train[col] = processed_train[col].fillna(\"missing value\")\n\nprocessed_train.shape\n\n\n(2928, 50)"
  },
  {
    "objectID": "projects/Predicting Belgian Real Estate Prices_ Part 7_Fine_tuning/NB_7_ACs_Fine_tuning.html",
    "href": "projects/Predicting Belgian Real Estate Prices_ Part 7_Fine_tuning/NB_7_ACs_Fine_tuning.html",
    "title": "Predicting Belgian Real Estate Prices: Part 7: Fine-tuning our model",
    "section": "",
    "text": "Photo by Stephen Phillips - Hostreviews.co.uk on UnSplash\nIn Part 6, we established a robust cross-validation strategy to consistently assess our model’s performance across multiple data folds. We also identified and managed potential outliers in our dataset. Additionally, we explored diverse feature engineering methods, creating and evaluating informative features to enhance our model’s predictive capabilities.\nIn this final segment, we’ll optimize our hyperparameters using Optuna and end by evaluating the final model’s performance based on the test portion. Let’s dive in!"
  },
  {
    "objectID": "projects/Predicting Belgian Real Estate Prices_ Part 7_Fine_tuning/NB_7_ACs_Fine_tuning.html#hyperparameter-tuning-using-optuna",
    "href": "projects/Predicting Belgian Real Estate Prices_ Part 7_Fine_tuning/NB_7_ACs_Fine_tuning.html#hyperparameter-tuning-using-optuna",
    "title": "Predicting Belgian Real Estate Prices: Part 7: Fine-tuning our model",
    "section": "Hyperparameter tuning using Optuna",
    "text": "Hyperparameter tuning using Optuna\nTo identify the most optimal settings, we’ll leverage the Optuna library. The key hyperparameters under consideration are iterations, depth, learning_rate, random_strength, bagging_temperature, and others.\n\n\nCode\ndef objective(trial: optuna.Trial) -&gt; float:\n    \"\"\"\n    Optuna objective function for tuning CatBoost hyperparameters.\n\n    This function takes an Optuna trial and explores hyperparameters for a CatBoost\n    model to minimize the Root Mean Squared Error (RMSE) using K-Fold cross-validation.\n\n    Parameters:\n    - trial (optuna.Trial): Optuna trial object for hyperparameter optimization.\n\n    Returns:\n    - float: Mean RMSE across K-Fold cross-validation iterations.\n\n    Example use case:\n    # Create an Optuna study and optimize hyperparameters\n    study = optuna.create_study(direction=\"minimize\")\n    study.optimize(objective, n_trials=100)\n\n    # Get the best hyperparameters\n    best_params = study.best_params\n    \"\"\"\n    catboost_params = {\n        \"iterations\": trial.suggest_int(\"iterations\", 10, 1000),\n        \"depth\": trial.suggest_int(\"depth\", 1, 8),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 1),\n        \"random_strength\": trial.suggest_float(\"random_strength\", 1e-9, 10),\n        \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 0, 1),\n        \"l2_leaf_reg\": trial.suggest_int(\"l2_leaf_reg\", 2, 30),\n        \"border_count\": trial.suggest_int(\"border_count\", 1, 255),\n        \"thread_count\": os.cpu_count(),\n    }\n\n    results = []\n    optuna.logging.set_verbosity(optuna.logging.WARNING)\n\n    # Extract feature names and data types\n    # features = X.columns[~X.columns.str.contains(\"price\")]\n    # numerical_features = X.select_dtypes(\"number\").columns.to_list()\n    categorical_features = X.select_dtypes(\"object\").columns.to_list()\n\n    # Create a K-Fold cross-validator\n    CV = model_selection.RepeatedKFold(\n        n_splits=10, n_repeats=1, random_state=utils.Configuration.seed\n    )\n\n    for train_fold_index, val_fold_index in CV.split(X):\n        X_train_fold, X_val_fold = X.loc[train_fold_index], X.loc[val_fold_index]\n        y_train_fold, y_val_fold = y.loc[train_fold_index], y.loc[val_fold_index]\n\n        # Create CatBoost datasets\n        catboost_train = catboost.Pool(\n            X_train_fold,\n            y_train_fold,\n            cat_features=categorical_features,\n        )\n        catboost_valid = catboost.Pool(\n            X_val_fold,\n            y_val_fold,\n            cat_features=categorical_features,\n        )\n\n        # Initialize and train the CatBoost model\n        model = catboost.CatBoostRegressor(**catboost_params)\n        model.fit(\n            catboost_train,\n            eval_set=[catboost_valid],\n            early_stopping_rounds=utils.Configuration.early_stopping_round,\n            verbose=utils.Configuration.verbose,\n            use_best_model=True,\n        )\n\n        # Calculate OOF validation predictions\n        valid_pred = model.predict(X_val_fold)\n\n        RMSE_score = metrics.mean_squared_error(y_val_fold, valid_pred, squared=False)\n\n        del (\n            X_train_fold,\n            y_train_fold,\n            X_val_fold,\n            y_val_fold,\n            catboost_train,\n            catboost_valid,\n            model,\n            valid_pred,\n        )\n        gc.collect()\n\n        results.append(RMSE_score)\n    return np.mean(results)\n\n\n\n\n\n\n\n\nNote\n\n\n\nSimilar to Part 6, the hyperparameter optimization step was pre-computed due to the significant computational time needed. The results were saved rather than executed during notebook rendering to save time. However, note that the outcomes should remain unchanged.\n\n\n\n\nCode\n%%script echo skipping\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(train_model.Optuna_Objective(X_train, y_train), n_trials=100, show_progress_bar=True)\n\ndumper(study.best_params, \"CatBoost_params\")\ndumper(study.best_value, \"CatBoost_value\")\n\n\nCouldn't find program: 'echo'\n\n\nAs shown below, Optuna found the best Out-Of-Fold (OOF) score using the selected parameters, which is 0.1060. Recall that in Part 6, our best OOF score was 0.1070, so this represents a modest improvement, albeit a slight one.\n\n\nCode\ncatboost_params_optuna = pd.read_pickle(\"CatBoost_params.pickle\")\n\nprint(\n    f'The best OOF RMSE score of the hyperparameter tuning is {pd.read_pickle(\"CatBoost_value.pickle\"):.4f}.'\n)\nprint(f\"The corresponding values: {catboost_params_optuna}\")\n\n\nThe best OOF RMSE score of the hyperparameter tuning is 0.1060.\nThe corresponding values: {'iterations': 956, 'depth': 7, 'learning_rate': 0.050050595110243595, 'random_strength': 7.110744896133362, 'bagging_temperature': 0.024119607385698107, 'l2_leaf_reg': 3, 'border_count': 205}"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Quantifyer: Simplifying Data Analysis for Targeted LC/GC-MS\n\n\n\n\n\n\n\nQuantifyer\n\n\n\n\n\n\n\n\n\n\n\nJan 20, 2024\n\n\nAdam Cseresznye\n\n\n\n\n\n\n  \n\n\n\n\nDeepLCMS: A framework that leverages transfer learning for the classification of pseudo images in mass spectrometry-based analysis\n\n\n\n\n\n\n\nDeepLCMS\n\n\n\n\n\n\n\n\n\n\n\nDec 23, 2023\n\n\nAdam Cseresznye\n\n\n\n\n\n\n  \n\n\n\n\nStreamlining Data Wrangling with the CP-Seeker Post-Processing App\n\n\n\n\n\n\n\nCP-Seeker Post-Processing App\n\n\n\n\n\n\n\n\n\n\n\nNov 13, 2023\n\n\nAdam Cseresznye\n\n\n\n\n\n\n  \n\n\n\n\nPredicting Belgian Real Estate Prices: Part 7: Fine-tuning our model\n\n\n\n\n\n\n\nPredicting Belgian Real Estate Prices\n\n\n\n\n\n\n\n\n\n\n\nNov 9, 2023\n\n\nAdam Cseresznye\n\n\n\n\n\n\n  \n\n\n\n\nPredicting Belgian Real Estate Prices: Part 6: Feature Engineering\n\n\n\n\n\n\n\nPredicting Belgian Real Estate Prices\n\n\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\nAdam Cseresznye\n\n\n\n\n\n\n  \n\n\n\n\nPredicting Belgian Real Estate Prices: Part 5: Initial feature selection\n\n\n\n\n\n\n\nPredicting Belgian Real Estate Prices\n\n\n\n\n\n\n\n\n\n\n\nNov 7, 2023\n\n\nAdam Cseresznye\n\n\n\n\n\n\n  \n\n\n\n\nPredicting Belgian Real Estate Prices: Part 4: Building a Baseline Model\n\n\n\n\n\n\n\nPredicting Belgian Real Estate Prices\n\n\n\n\n\n\n\n\n\n\n\nNov 6, 2023\n\n\nAdam Cseresznye\n\n\n\n\n\n\n  \n\n\n\n\nPredicting Belgian Real Estate Prices: Part 3: Characterizing Belgian Real Estate Data Post-Scraping\n\n\n\n\n\n\n\nPredicting Belgian Real Estate Prices\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2023\n\n\nAdam Cseresznye\n\n\n\n\n\n\n  \n\n\n\n\nPredicting Belgian Real Estate Prices: Part 2: Data Cleanup After Web Scraping\n\n\n\n\n\n\n\nPredicting Belgian Real Estate Prices\n\n\n\n\n\n\n\n\n\n\n\nNov 4, 2023\n\n\nAdam Cseresznye\n\n\n\n\n\n\n  \n\n\n\n\nPredicting Belgian Real Estate Prices: Part 1: Feature Selection for Web Scraping\n\n\n\n\n\n\n\nPredicting Belgian Real Estate Prices\n\n\n\n\n\n\n\n\n\n\n\nNov 3, 2023\n\n\nAdam Cseresznye\n\n\n\n\n\n\nNo matching items"
  }
]